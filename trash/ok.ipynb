{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# sys.path.append(\"/Users/shashanks./Downloads/Installations/ddn/\")\n",
    "sys.path.append(\"../Base/\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from bernstein import bernstein_coeff_order10_new\n",
    "from ddn.pytorch.node import AbstractDeclarativeNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_fin = 2.0\n",
    "num = 16\n",
    "\n",
    "tot_time = np.linspace(0.0, t_fin, num)\n",
    "tot_time_copy = tot_time.reshape(num, 1)\n",
    "P, Pdot, Pddot = bernstein_coeff_order10_new(10, tot_time_copy[0], tot_time_copy[-1], tot_time_copy)\n",
    "nvar = np.shape(P)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_eq_mat = np.vstack((P[0], P[-1]))\n",
    "A_eq_np = block_diag(A_eq_mat, A_eq_mat)\n",
    "Q_np = 10 * block_diag(np.dot(Pddot.T, Pddot), np.dot(Pddot.T, Pddot))\n",
    "q_np = np.zeros(2 * nvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QPNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPNode(AbstractDeclarativeNode):\n",
    "    def __init__(self, Q_np, q_np, A_eq_np, rho=1.0, nvar=22, maxiter=1000):\n",
    "        super().__init__()\n",
    "        self.rho = rho\n",
    "        self.nvar = nvar\n",
    "        self.maxiter = maxiter\n",
    "        self.Q = torch.tensor(Q_np, dtype=torch.double).to(device)\n",
    "        self.q = torch.tensor(q_np, dtype=torch.double).to(device)\n",
    "        self.A = torch.tensor(A_eq_np, dtype=torch.double).to(device)\n",
    "    \n",
    "    def objective(self, b, lamda, y):\n",
    "        \"\"\"\n",
    "        b: (B x 12)\n",
    "        lamda: (B x 22)\n",
    "        y: (B x 22)\n",
    "        \"\"\"\n",
    "        lamda = lamda.transpose(0, 1)\n",
    "        y = y.transpose(0, 1)\n",
    "        cost_mat = self.rho * torch.matmul(self.A.T, self.A) + self.Q\n",
    "        lincost_mat = -self.rho * torch.matmul(b, self.A).T + self.q.view(-1, 1) - lamda\n",
    "        f = 0.5 * torch.diag(torch.matmul(y.T, torch.matmul(cost_mat, y))) + torch.diag(torch.matmul(lincost_mat.T, y))\n",
    "        return f\n",
    "    \n",
    "    def compute_augmented_lagrangian(self, b, lamda):\n",
    "        \"\"\"\n",
    "        b: (12,)\n",
    "        lamda: (22,)\n",
    "        \"\"\"\n",
    "        cost_mat = self.rho * torch.matmul(self.A.T, self.A) + self.Q\n",
    "        lincost_mat = -self.rho * torch.matmul(b, self.A).T + self.q - lamda\n",
    "        lincost_mat = lincost_mat.view(-1, 1)\n",
    "        sol, _ = torch.solve(lincost_mat, -cost_mat)\n",
    "        sol = sol.view(-1)\n",
    "        res = torch.matmul(self.A, sol) - b\n",
    "        return sol, res\n",
    "    \n",
    "    def optimize(self, b, lamda):\n",
    "        sol, res = self.compute_augmented_lagrangian(b, lamda)\n",
    "        for i in range(0, self.maxiter):\n",
    "            sol, res = self.compute_augmented_lagrangian(b, lamda)\n",
    "            lamda -= self.rho * torch.matmul(self.A.T, res)\n",
    "        return sol\n",
    "    \n",
    "    def solve(self, b, lamda):\n",
    "        batch_size, _ = b.size()\n",
    "        y = torch.zeros(batch_size, 22, dtype=torch.double).to(device)\n",
    "        for i in range(batch_size):\n",
    "            b_cur = b[i]\n",
    "            lamda_cur = lamda[i]\n",
    "            sol = self.optimize(b_cur, lamda_cur)\n",
    "            y[i, :] = sol\n",
    "        return y, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Declarative Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPFunction(torch.autograd.Function):\n",
    "    \"\"\"Generic declarative autograd function.\n",
    "    Defines the forward and backward functions. Saves all inputs and outputs,\n",
    "    which may be memory-inefficient for the specific problem.\n",
    "    \n",
    "    Assumptions:\n",
    "    * All inputs are PyTorch tensors\n",
    "    * All inputs have a single batch dimension (b, ...)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, problem, *inputs):\n",
    "        output, solve_ctx = torch.no_grad()(problem.solve)(*inputs)\n",
    "        ctx.save_for_backward(output, *inputs)\n",
    "        ctx.problem = problem\n",
    "        ctx.solve_ctx = solve_ctx\n",
    "        return output.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output, *inputs = ctx.saved_tensors\n",
    "        problem = ctx.problem\n",
    "        solve_ctx = ctx.solve_ctx\n",
    "        output.requires_grad = True\n",
    "        inputs = tuple(inputs)\n",
    "        grad_inputs = problem.gradient(*inputs, y=output, v=grad_output,\n",
    "            ctx=solve_ctx)\n",
    "        return (None, *grad_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Declarative Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeclarativeLayer(torch.nn.Module):\n",
    "    \"\"\"Generic declarative layer.\n",
    "    \n",
    "    Assumptions:\n",
    "    * All inputs are PyTorch tensors\n",
    "    * All inputs have a single batch dimension (b, ...)\n",
    "    Usage:\n",
    "        problem = <derived class of *DeclarativeNode>\n",
    "        declarative_layer = DeclarativeLayer(problem)\n",
    "        y = declarative_layer(x1, x2, ...)\n",
    "    \"\"\"\n",
    "    def __init__(self, problem):\n",
    "        super(DeclarativeLayer, self).__init__()\n",
    "        self.problem = problem\n",
    "        \n",
    "    def forward(self, *inputs):\n",
    "        return QPFunction.apply(self.problem, *inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TrajNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajNet(nn.Module):\n",
    "    def __init__(self, opt_layer, P, Pdot, input_size=32, hidden_size=64, output_size=4, nvar=11, t_obs=8):\n",
    "        super(TrajNet, self).__init__()\n",
    "        self.nvar = nvar\n",
    "        self.t_obs = t_obs\n",
    "        self.P = torch.tensor(P, dtype=torch.double).to(device)\n",
    "        self.Pdot = torch.tensor(Pdot, dtype=torch.double).to(device)\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        self.opt_layer = opt_layer\n",
    "        self.activation = nn.ReLU()\n",
    "        self.mask = torch.tensor([[1.0, 0.0, 1.0, 0.0]], dtype=torch.double).to(device)\n",
    "#         self.mask = torch.tensor([[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]], dtype=torch.double).to(device)\n",
    "    \n",
    "    def forward(self, x, b):\n",
    "        batch_size, _ = x.size()\n",
    "        out = self.activation(self.linear1(x))\n",
    "        b_pred = self.linear2(out)\n",
    "        b_gen = self.mask * b + (1 - self.mask) * b_pred\n",
    "        \n",
    "        # Run optimization\n",
    "        lamda = torch.zeros(batch_size, 2 * self.nvar, dtype=torch.double).to(device)\n",
    "        sol = self.opt_layer(b_gen, lamda)\n",
    "        \n",
    "        # Compute final trajectory\n",
    "        x_pred = torch.matmul(self.P, sol[:, :self.nvar].transpose(0, 1))\n",
    "        y_pred = torch.matmul(self.P, sol[:, self.nvar:].transpose(0, 1))\n",
    "        vx_pred = torch.matmul(self.Pdot, sol[:, :self.nvar].transpose(0, 1))\n",
    "        vy_pred = torch.matmul(self.Pdot, sol[:, self.nvar:].transpose(0, 1))\n",
    "        x_pred = x_pred.transpose(0, 1)\n",
    "        y_pred = y_pred.transpose(0, 1)\n",
    "        vx_pred = vx_pred.transpose(0, 1)\n",
    "        vy_pred = vy_pred.transpose(0, 1)\n",
    "        out = torch.cat([x_pred, y_pred, vx_pred, vy_pred], dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trajectory Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, data_path, t_obs=16, dt=0.125):\n",
    "        self.data = np.load(data_path)\n",
    "        self.t_obs = t_obs\n",
    "        self.dt = dt\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        traj = self.data[idx]\n",
    "        x_traj = traj[:, 0]\n",
    "        y_traj = traj[:, 1]\n",
    "        vx_traj = traj[:, 2]\n",
    "        vy_traj = traj[:, 3]\n",
    "        \n",
    "        if np.random.choice([0, 1]):\n",
    "            y_traj *= -1\n",
    "            vy_traj *= -1\n",
    "        \n",
    "        x_traj = x_traj - x_traj[0]\n",
    "        y_traj = y_traj - y_traj[0]\n",
    "        \n",
    "        x_inp = x_traj[:self.t_obs]\n",
    "        y_inp = y_traj[:self.t_obs]\n",
    "        x_fut = x_traj[self.t_obs:]\n",
    "        y_fut = y_traj[self.t_obs:]\n",
    "        \n",
    "        vx_beg = vx_traj[self.t_obs-1]\n",
    "        vy_beg = vy_traj[self.t_obs-1]\n",
    "        \n",
    "        vx_beg_prev = vx_traj[self.t_obs-2]\n",
    "        vy_beg_prev = vy_traj[self.t_obs-2]\n",
    "        \n",
    "        ax_beg = (vx_beg - vx_beg_prev) / self.dt\n",
    "        ay_beg = (vy_beg - vy_beg_prev) / self.dt\n",
    "        \n",
    "        vx_fin = vx_traj[2*self.t_obs-1]\n",
    "        vy_fin = vy_traj[2*self.t_obs-1]\n",
    "        \n",
    "        vx_fin_prev = vx_traj[2*self.t_obs-2]\n",
    "        vy_fin_prev = vy_traj[2*self.t_obs-2]\n",
    "        \n",
    "        ax_fin = (vx_fin - vx_fin_prev) / self.dt\n",
    "        ay_fin = (vy_fin - vy_fin_prev) / self.dt\n",
    "        \n",
    "        traj_inp = np.dstack((x_inp, y_inp)).flatten()\n",
    "        vx_fut = vx_traj[self.t_obs:]\n",
    "        vy_fut = vy_traj[self.t_obs:]\n",
    "        traj_out = np.hstack((x_fut, y_fut, vx_fut, vy_fut)).flatten()\n",
    "        \n",
    "        \n",
    "        b_inp = np.array([x_inp[-1], 0, y_inp[-1], 0])\n",
    "        return torch.tensor(traj_inp), torch.tensor(traj_out), torch.tensor(b_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TrajectoryDataset(Dataset):\n",
    "#     def __init__(self, root_dir, t_obs=8, dt=0.4):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.t_obs = t_obs\n",
    "#         self.dt = dt\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(os.listdir(self.root_dir))\n",
    "    \n",
    "#     def get_vel(self, pos):\n",
    "#         return (pos[-1] - pos[-2]) / self.dt\n",
    "    \n",
    "#     def get_acc(self, vel):\n",
    "#         return (vel[-1] - vel[-2]) / self.dt\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         file_name = \"{}.npy\".format(idx)\n",
    "#         file_path = os.path.join(self.root_dir, file_name)\n",
    "        \n",
    "#         data = np.load(file_path, allow_pickle=True).item()\n",
    "#         x_traj = data['x_traj']\n",
    "#         y_traj = data['y_traj']\n",
    "        \n",
    "#         x_inp = x_traj[:self.t_obs]\n",
    "#         y_inp = y_traj[:self.t_obs]\n",
    "#         x_fut = x_traj[self.t_obs:]\n",
    "#         y_fut = y_traj[self.t_obs:]\n",
    "        \n",
    "#         vx_beg = (x_inp[-1] - x_inp[-2]) / self.dt\n",
    "#         vy_beg = (y_inp[-1] - y_inp[-2]) / self.dt\n",
    "        \n",
    "#         vx_beg_prev = (x_inp[-2] - x_inp[-3]) / self.dt\n",
    "#         vy_beg_prev = (y_inp[-2] - y_inp[-3]) / self.dt\n",
    "        \n",
    "#         ax_beg = (vx_beg - vx_beg_prev) / self.dt\n",
    "#         ay_beg = (vy_beg - vy_beg_prev) / self.dt\n",
    "\n",
    "#         traj_inp = np.dstack((x_inp, y_inp)).flatten()\n",
    "#         traj_out = np.hstack((x_fut, y_fut)).flatten()\n",
    "#         b_inp = np.array([x_inp[-1], vx_beg, ax_beg, 0, 0, 0, y_inp[-1], vy_beg, ay_beg, 0, 0, 0])\n",
    "#         return torch.tensor(traj_inp), torch.tensor(traj_out), torch.tensor(b_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrajectoryDataset(\"./train_data.npy\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TrajectoryDataset(\"./test_data.npy\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32]) torch.Size([20, 64])\n"
     ]
    }
   ],
   "source": [
    "for batch_num, data in enumerate(train_loader):\n",
    "    traj_inp, traj_out, b_inp = data\n",
    "    print(traj_inp.size(), traj_out.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32])\n"
     ]
    }
   ],
   "source": [
    "for batch_num, data in enumerate(test_loader):\n",
    "    traj_inp, traj_out, b_inp = data\n",
    "    print(traj_inp.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = QPNode(Q_np, q_np, A_eq_np)\n",
    "qp_layer = DeclarativeLayer(problem)\n",
    "\n",
    "model = TrajNet(qp_layer, P, Pdot)\n",
    "model = model.double()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(traj_inp, b_inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 277.7320111118003\n",
      "Epoch: 0, Batch: 10, Loss: 312.8331085395828\n",
      "Epoch: 0, Mean Loss: 262.01484225189677\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 1, Batch: 0, Loss: 179.51807400449405\n",
      "Epoch: 1, Batch: 10, Loss: 112.3209833924763\n",
      "Epoch: 1, Mean Loss: 165.6104336806438\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 2, Batch: 0, Loss: 99.00017752363837\n",
      "Epoch: 2, Batch: 10, Loss: 116.41864648538528\n",
      "Epoch: 2, Mean Loss: 100.63189386322897\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 3, Batch: 0, Loss: 57.775769431725436\n",
      "Epoch: 3, Batch: 10, Loss: 68.3514217419637\n",
      "Epoch: 3, Mean Loss: 67.86907743334798\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 4, Batch: 0, Loss: 65.15231324268466\n",
      "Epoch: 4, Batch: 10, Loss: 37.4168546897667\n",
      "Epoch: 4, Mean Loss: 49.94964613306898\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 5, Batch: 0, Loss: 47.66937674922618\n",
      "Epoch: 5, Batch: 10, Loss: 30.14575823167892\n",
      "Epoch: 5, Mean Loss: 40.282674496156645\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 6, Batch: 0, Loss: 43.29475990884858\n",
      "Epoch: 6, Batch: 10, Loss: 32.94170645592377\n",
      "Epoch: 6, Mean Loss: 35.464709078379556\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 7, Batch: 0, Loss: 39.14926940862124\n",
      "Epoch: 7, Batch: 10, Loss: 22.582242013507965\n",
      "Epoch: 7, Mean Loss: 32.3054265365952\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 8, Batch: 0, Loss: 50.38044876384779\n",
      "Epoch: 8, Batch: 10, Loss: 26.543413499076593\n",
      "Epoch: 8, Mean Loss: 28.62192858022131\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 9, Batch: 0, Loss: 29.68630464327474\n",
      "Epoch: 9, Batch: 10, Loss: 24.513702403687585\n",
      "Epoch: 9, Mean Loss: 26.82678570320396\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 10, Batch: 0, Loss: 17.134025614283107\n",
      "Epoch: 10, Batch: 10, Loss: 21.950968941842852\n",
      "Epoch: 10, Mean Loss: 23.651288392301055\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 11, Batch: 0, Loss: 15.719199458515467\n",
      "Epoch: 11, Batch: 10, Loss: 29.402583612120974\n",
      "Epoch: 11, Mean Loss: 22.28855527324246\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 12, Batch: 0, Loss: 21.031604981479962\n",
      "Epoch: 12, Batch: 10, Loss: 23.540416101658852\n",
      "Epoch: 12, Mean Loss: 21.145625761725476\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 13, Batch: 0, Loss: 15.534983030790235\n",
      "Epoch: 13, Batch: 10, Loss: 12.920923115168018\n",
      "Epoch: 13, Mean Loss: 19.44653364765155\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 14, Batch: 0, Loss: 23.19556789584815\n",
      "Epoch: 14, Batch: 10, Loss: 13.874132779099366\n",
      "Epoch: 14, Mean Loss: 17.96916735006616\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 15, Batch: 0, Loss: 15.320711783618844\n",
      "Epoch: 15, Batch: 10, Loss: 21.312128275716482\n",
      "Epoch: 15, Mean Loss: 17.520239481300088\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 16, Batch: 0, Loss: 15.867156597859047\n",
      "Epoch: 16, Batch: 10, Loss: 20.528015267975587\n",
      "Epoch: 16, Mean Loss: 16.769920398951964\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 17, Batch: 0, Loss: 16.59594119130158\n",
      "Epoch: 17, Batch: 10, Loss: 13.779821664632994\n",
      "Epoch: 17, Mean Loss: 16.11603793513381\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 18, Batch: 0, Loss: 21.277470958551696\n",
      "Epoch: 18, Batch: 10, Loss: 14.955412257055617\n",
      "Epoch: 18, Mean Loss: 15.725522255360184\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 19, Batch: 0, Loss: 16.041478189553185\n",
      "Epoch: 19, Batch: 10, Loss: 14.217039756559661\n",
      "Epoch: 19, Mean Loss: 15.513705820018473\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 20, Batch: 0, Loss: 14.855507636084303\n",
      "Epoch: 20, Batch: 10, Loss: 13.71755118633381\n",
      "Epoch: 20, Mean Loss: 15.464140924848442\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 21, Batch: 0, Loss: 13.54667170121606\n",
      "Epoch: 21, Batch: 10, Loss: 18.88819902614592\n",
      "Epoch: 21, Mean Loss: 15.073733635822816\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 22, Batch: 0, Loss: 17.90181155878265\n",
      "Epoch: 22, Batch: 10, Loss: 14.890751603739286\n",
      "Epoch: 22, Mean Loss: 14.729567309030978\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 23, Batch: 0, Loss: 13.394519880274988\n",
      "Epoch: 23, Batch: 10, Loss: 18.81091849055616\n",
      "Epoch: 23, Mean Loss: 14.883614702268863\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 24, Batch: 0, Loss: 11.983177289152673\n",
      "Epoch: 24, Batch: 10, Loss: 14.589951255673844\n",
      "Epoch: 24, Mean Loss: 14.835610935634744\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 25, Batch: 0, Loss: 13.56462013849844\n",
      "Epoch: 25, Batch: 10, Loss: 13.352257344064435\n",
      "Epoch: 25, Mean Loss: 14.477648813738563\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 26, Batch: 0, Loss: 18.983218152632624\n",
      "Epoch: 26, Batch: 10, Loss: 11.84934153153283\n",
      "Epoch: 26, Mean Loss: 14.487189400504771\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 27, Batch: 0, Loss: 16.550372628503474\n",
      "Epoch: 27, Batch: 10, Loss: 14.893109112023856\n",
      "Epoch: 27, Mean Loss: 14.34652381326391\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 28, Batch: 0, Loss: 16.86039992110296\n",
      "Epoch: 28, Batch: 10, Loss: 10.877162290375475\n",
      "Epoch: 28, Mean Loss: 14.40609433450847\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 29, Batch: 0, Loss: 12.21155571095139\n",
      "Epoch: 29, Batch: 10, Loss: 14.658234737518205\n",
      "Epoch: 29, Mean Loss: 14.456971338955867\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "epoch_train_loss = []\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for batch_num, data in enumerate(train_loader):\n",
    "        traj_inp, traj_out, b_inp = data\n",
    "        traj_inp = traj_inp.to(device)\n",
    "        traj_out = traj_out.to(device)\n",
    "        b_inp = b_inp.to(device)\n",
    "\n",
    "        out = model(traj_inp, b_inp)\n",
    "        loss = criterion(out, traj_out)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        if batch_num % 10 == 0:\n",
    "            print(\"Epoch: {}, Batch: {}, Loss: {}\".format(epoch, batch_num, loss.item()))\n",
    "    \n",
    "    mean_loss = np.mean(train_loss)\n",
    "    epoch_train_loss.append(mean_loss)\n",
    "    print(\"Epoch: {}, Mean Loss: {}\".format(epoch, mean_loss))\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traj(i, traj_inp, traj_out, traj_pred):\n",
    "    traj_inp = traj_inp.numpy()\n",
    "    traj_out = traj_out.numpy()\n",
    "    traj_pred = traj_pred.numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax.scatter(traj_inp[::2], traj_inp[1::2], label='Inp traj')\n",
    "    ax.scatter(traj_out[:16], traj_out[16:], label='GT')\n",
    "    ax.scatter(traj_pred[:16], traj_pred[16:], label='Pred')\n",
    "    ax.legend()\n",
    "    ax.set_xlim([-70, 70])\n",
    "    ax.set_ylim([-70, 70])\n",
    "    plt.savefig('./onlygoal-randomflip-velocity-loss-test-plots/{}.png'.format(i))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Loss: 11.757869306921917\n",
      "Batch: 1, Loss: 14.018490186038429\n",
      "Batch: 2, Loss: 16.13911064033244\n",
      "Batch: 3, Loss: 16.222504305044232\n",
      "Batch: 4, Loss: 17.676572300192657\n",
      "Batch: 5, Loss: 13.728760598617862\n",
      "Batch: 6, Loss: 11.059128365194534\n",
      "Batch: 7, Loss: 10.864685245177856\n",
      "Batch: 8, Loss: 11.842039016891212\n",
      "Batch: 9, Loss: 19.87885029894585\n",
      "Epoch Mean Test Loss: 14.3188010263357\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    cnt = 0\n",
    "    test_loss = []\n",
    "    for batch_num, data in enumerate(test_loader):\n",
    "        traj_inp, traj_out, b_inp = data\n",
    "        traj_inp = traj_inp.to(device)\n",
    "        traj_out = traj_out.to(device)\n",
    "        b_inp = b_inp.to(device)\n",
    "\n",
    "        out = model(traj_inp, b_inp)\n",
    "        loss = criterion(out, traj_out)\n",
    "        \n",
    "        test_loss.append(loss.item())\n",
    "        print(\"Batch: {}, Loss: {}\".format(batch_num, loss.item()))\n",
    "        \n",
    "        for i in range(traj_inp.size()[0]):\n",
    "            plot_traj(cnt, traj_inp[i], traj_out[i, :32], out[i, :32])\n",
    "            cnt += 1\n",
    "\n",
    "mean_loss = np.mean(test_loss)\n",
    "print(\"Epoch Mean Test Loss: {}\".format(mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_num, data in enumerate(test_loader):\n",
    "    traj_inp, traj_out, b_inp = data\n",
    "    print(torch.min(traj_out[:, :16]), torch.max(traj_out[:, :16]))\n",
    "    print(torch.min(traj_out[:, 16:]), torch.max(traj_out[:, 16:]))\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
