{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2daa20f-69c5-45d1-886b-70024a51d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/ims/Documents/ddn\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from bernstein import bernstein_coeff_order10_new\n",
    "from ddn.pytorch.node import AbstractDeclarativeNode\n",
    "\n",
    "#from OPTNode import OPTNode\n",
    "from dataloader import ArgoverseDataset\n",
    "\n",
    "#from models import TrajNet, TrajNetLSTM, TrajNetLSTMSimple\n",
    "from bernstein import bernstein_coeff_order10_new\n",
    "from viz_helpers import plot_traj, plot_trajj\n",
    "from metrics import get_ade, get_fde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5adf67-9ca7-4234-9631-26ac4a974163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "680626bd-6744-4cf2-b32d-8684b7d21671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPTNode(AbstractDeclarativeNode):\n",
    "    def __init__(self, rho_eq=1.0, rho_goal=1.0, rho_nonhol=1.0, rho_psi=1.0, maxiter=500, weight_smoothness=1.0, weight_smoothness_psi=1.0, t_fin=2.0, num=30, device=device):\n",
    "        super().__init__()\n",
    "        self.rho_eq = rho_eq\n",
    "        self.rho_goal = rho_goal\n",
    "        self.rho_nonhol = rho_nonhol\n",
    "        self.rho_psi = rho_psi\n",
    "        self.maxiter = maxiter\n",
    "        self.weight_smoothness = weight_smoothness\n",
    "        self.weight_smoothness_psi = weight_smoothness_psi\n",
    "        self.device = device\n",
    "        \n",
    "        self.t_fin = t_fin\n",
    "        self.num = num\n",
    "        self.t = self.t_fin / self.num\n",
    "\n",
    "        #self.num_batch = 10\n",
    "        \n",
    "        tot_time = np.linspace(0.0, self.t_fin, self.num)\n",
    "        tot_time_copy = tot_time.reshape(self.num, 1)\n",
    "        self.P, self.Pdot, self.Pddot = bernstein_coeff_order10_new(10, tot_time_copy[0], tot_time_copy[-1], tot_time_copy)\n",
    "        self.nvar = np.shape(self.P)[1]\n",
    "        \n",
    "        self.cost_smoothness = self.weight_smoothness * np.dot(self.Pddot.T, self.Pddot)\n",
    "        self.cost_smoothness_psi = self.weight_smoothness_psi * np.dot(self.Pddot.T, self.Pddot)\n",
    "        self.lincost_smoothness_psi = np.zeros(self.nvar)\n",
    "\n",
    "        self.A_eq = np.vstack((self.P[0], self.P[-1]))\n",
    "        self.A_eq_psi = np.vstack((self.P[0], self.Pdot[0], self.P[-1]))\n",
    "        \n",
    "        self.P = torch.tensor(self.P, dtype=torch.double).to(device)\n",
    "        self.Pdot = torch.tensor(self.Pdot, dtype=torch.double).to(device)\n",
    "        self.Pddot = torch.tensor(self.Pddot, dtype=torch.double).to(device)\n",
    "        self.A_eq = torch.tensor(self.A_eq, dtype=torch.double).to(device)        \n",
    "        self.A_eq_psi = torch.tensor(self.A_eq_psi, dtype=torch.double).to(device)\n",
    "        self.cost_smoothness = torch.tensor(self.cost_smoothness, dtype=torch.double).to(device)\n",
    "        self.cost_smoothness_psi = torch.tensor(self.cost_smoothness_psi, dtype=torch.double).to(device)\n",
    "        self.lincost_smoothness_psi = torch.tensor(self.lincost_smoothness_psi, dtype=torch.double).to(device)\n",
    "    \n",
    "        self.A_nonhol = self.Pdot\n",
    "        self.A_psi = self.P\n",
    "        \n",
    "        self.lamda_x = None\n",
    "        self.lamda_y = None\n",
    "        self.lamda_psi = None\n",
    "        \n",
    "    def compute_x(self, v, psi, b_eq_x, b_eq_y):\n",
    "        b_nonhol_x = v * torch.cos(psi)\n",
    "        b_nonhol_y = v * torch.sin(psi)\n",
    "    \n",
    "        cost = self.cost_smoothness + self.rho_nonhol * torch.matmul(self.A_nonhol.T, self.A_nonhol) + self.rho_eq * torch.matmul(self.A_eq.T, self.A_eq)\n",
    "        lincost_x = -self.lamda_x - self.rho_nonhol * torch.matmul(self.A_nonhol.T, b_nonhol_x.T).T - self.rho_eq * torch.matmul(self.A_eq.T, b_eq_x.T).T\n",
    "        lincost_y = -self.lamda_y - self.rho_nonhol * torch.matmul(self.A_nonhol.T, b_nonhol_y.T).T - self.rho_eq * torch.matmul(self.A_eq.T, b_eq_y.T).T\n",
    "\n",
    "        cost_inv = torch.linalg.inv(cost)\n",
    "\n",
    "        sol_x = torch.matmul(-cost_inv, lincost_x.T).T\n",
    "        sol_y = torch.matmul(-cost_inv, lincost_y.T).T\n",
    "\n",
    "        x = torch.matmul(self.P, sol_x.T).T\n",
    "        xdot = torch.matmul(self.Pdot, sol_x.T).T\n",
    "\n",
    "        y = torch.matmul(self.P, sol_y.T).T\n",
    "        ydot = torch.matmul(self.Pdot, sol_y.T).T\n",
    "         \n",
    "        return sol_x, sol_y, x, xdot, y, ydot\n",
    "    \n",
    "    def compute_psi(self, psi, lamda_psi, psi_temp, b_eq_psi):\n",
    "        cost = self.cost_smoothness_psi + self.rho_psi * torch.matmul(self.A_psi.T, self.A_psi) + self.rho_eq * torch.matmul(self.A_eq_psi.T, self.A_eq_psi)\n",
    "        lincost_psi = -self.lamda_psi - self.rho_psi * torch.matmul(self.A_psi.T, psi_temp.T).T - self.rho_eq * torch.matmul(self.A_eq_psi.T, b_eq_psi.T).T\n",
    "\n",
    "        cost_inv = torch.linalg.inv(cost)\n",
    "\n",
    "        sol_psi = torch.matmul(-cost_inv, lincost_psi.T).T\n",
    "\n",
    "        psi = torch.matmul(self.P, sol_psi.T).T\n",
    "\n",
    "        res_psi = torch.matmul(self.A_psi, sol_psi.T).T - psi_temp\n",
    "        res_eq_psi = torch.matmul(self.A_eq_psi, sol_psi.T).T - b_eq_psi\n",
    "\n",
    "        self.lamda_psi = self.lamda_psi - self.rho_psi * torch.matmul(self.A_psi.T, res_psi.T).T - self.rho_eq * torch.matmul(self.A_eq_psi.T, res_eq_psi.T).T\n",
    "\n",
    "        return sol_psi, torch.linalg.norm(res_psi), torch.linalg.norm(res_eq_psi), psi\n",
    "\n",
    "    \n",
    "    def solve(self, fixed_params, variable_params):\n",
    "        batch_size, _ = fixed_params.size()\n",
    "        x_init, y_init, v_init, psi_init, psidot_init = torch.chunk(fixed_params, 5, dim=1)\n",
    "        x_fin, y_fin, psi_fin = torch.chunk(variable_params, 3, dim=1)\n",
    "        \n",
    "        b_eq_x = torch.cat((x_init, x_fin), dim=1)\n",
    "        b_eq_y = torch.cat((y_init, y_fin), dim=1)\n",
    "        b_eq_psi = torch.cat((psi_init, psidot_init, psi_fin), dim=1)\n",
    "        \n",
    "        v = torch.ones(batch_size, self.num, dtype=torch.double).to(self.device) * v_init\n",
    "        psi = torch.ones(batch_size, self.num, dtype=torch.double).to(self.device) * psi_init\n",
    "        xdot = v * torch.cos(psi)\n",
    "        ydot = v * torch.sin(psi)\n",
    "        \n",
    "        self.lamda_x = torch.zeros(batch_size, self.nvar, dtype=torch.double).to(self.device)\n",
    "        self.lamda_y = torch.zeros(batch_size, self.nvar, dtype=torch.double).to(self.device)\n",
    "        self.lamda_psi = torch.zeros(batch_size, self.nvar, dtype=torch.double).to(self.device)\n",
    "        \n",
    "        res_psi_arr = []\n",
    "        res_eq_psi_arr = []\n",
    "        res_eq_arr = []\n",
    "        res_nonhol_arr = []\n",
    "        for i in range(0, self.maxiter):\n",
    "            psi_temp = torch.atan2(ydot, xdot)\n",
    "            c_psi, res_psi, res_eq_psi, psi = self.compute_psi(psi, self.lamda_psi, psi_temp, b_eq_psi)\n",
    "            c_x, c_y, x, xdot, y, ydot = self.compute_x(v, psi, b_eq_x, b_eq_y)\n",
    "            \n",
    "            res_eq_psi_arr.append(res_eq_psi)\n",
    "            res_psi_arr.append(res_psi)\n",
    "            v = torch.sqrt(xdot ** 2 + ydot ** 2)\n",
    "            #v[:, 0] = v_init[:, 0]\n",
    "\n",
    "            res_eq_x = torch.matmul(self.A_eq, c_x.T).T - b_eq_x\n",
    "            res_nonhol_x = xdot - v * torch.cos(psi)\n",
    "\n",
    "            res_eq_y = torch.matmul(self.A_eq, c_y.T).T - b_eq_y\n",
    "            res_nonhol_y = ydot - v * torch.sin(psi)\n",
    "\n",
    "            res_eq_arr.append(torch.linalg.norm(torch.sqrt(res_eq_x**2 + res_eq_y**2)))\n",
    "            res_nonhol_arr.append(torch.linalg.norm(torch.sqrt(res_nonhol_x**2 + res_nonhol_y**2)))\n",
    "            \n",
    "            self.lamda_x = self.lamda_x - self.rho_eq * torch.matmul(self.A_eq.T, res_eq_x.T).T - self.rho_nonhol * torch.matmul(self.A_nonhol.T, res_nonhol_x.T).T\n",
    "            self.lamda_y = self.lamda_y - self.rho_eq * torch.matmul(self.A_eq.T, res_eq_y.T).T - self.rho_nonhol * torch.matmul(self.A_nonhol.T, res_nonhol_y.T).T\n",
    "        \n",
    "        primal_sol = torch.hstack((c_x, c_y, c_psi, v))\n",
    "        return primal_sol, None\n",
    "    \n",
    "    def objective(self, fixed_params, variable_params, y):\n",
    "        c_x = y[:, :self.nvar]\n",
    "        c_y = y[:, self.nvar:2*self.nvar]\n",
    "        c_psi = y[:, 2*self.nvar:3*self.nvar]\n",
    "        v = y[:, 3*self.nvar:]\n",
    "        \n",
    "        x_init, y_init, v_init, psi_init, psidot_init = torch.chunk(fixed_params, 5, dim=1)\n",
    "        x_fin, y_fin, psi_fin = torch.chunk(variable_params, 3, dim=1)\n",
    "        \n",
    "        x = torch.matmul(self.P, c_x.T).T\n",
    "        y = torch.matmul(self.P, c_y.T).T\n",
    "        psi = torch.matmul(self.P, c_psi.T).T\n",
    "        xdot = torch.matmul(self.Pdot, c_x.T).T\n",
    "        ydot = torch.matmul(self.Pdot, c_y.T).T\n",
    "        psidot = torch.matmul(self.Pdot, c_psi.T).T\n",
    "        xddot = torch.matmul(self.Pddot, c_x.T).T\n",
    "        yddot = torch.matmul(self.Pddot, c_y.T).T\n",
    "        psiddot = torch.matmul(self.Pddot, c_psi.T).T\n",
    "        \n",
    "        cost_nonhol = 0.5*self.rho_nonhol*torch.sum((xdot - v*torch.cos(psi)) ** 2, 1) + 0.5*self.rho_nonhol*torch.sum((ydot - v*torch.sin(psi)) ** 2, 1)\n",
    "        cost_pos = 0.5*self.rho_eq*(torch.sum((x[:, -1] - x_fin) ** 2, 1) + torch.sum((y[:, -1] - y_fin) ** 2, 1) + torch.sum((x[:, 0] - x_init) ** 2, 1) + torch.sum((y[:, 0] - y_init) ** 2, 1))\n",
    "        cost_psi = 0.5*self.rho_eq*(torch.sum((psi[:, -1] - psi_fin) ** 2, 1) + torch.sum((psi[:, 0] - psi_init) ** 2, 1)\n",
    "                                    + torch.sum((psidot[:, 0] - psidot_init) ** 2, 1))\n",
    "        #cost_v = 0.5*self.rho_eq*torch.sum((v[:, 0] - v_init) ** 2, 1)\n",
    "        cost_cancel = torch.diagonal(torch.matmul(-self.lamda_x, c_x.T) + torch.matmul(-self.lamda_y, c_y.T) + torch.matmul(-self.lamda_psi, c_psi.T))\n",
    "        \n",
    "        cost_smoothness = 0.5*self.weight_smoothness*(torch.sum(xddot**2, 1) + torch.sum(yddot**2, 1)) + 0.5*self.weight_smoothness_psi*torch.sum(psiddot**2, 1)\n",
    "        return cost_nonhol + cost_pos + cost_psi + cost_smoothness + cost_cancel #+ cost_v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eeb4f99-364e-4917-a14a-cb150a94984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeclarativeFunction(torch.autograd.Function):\n",
    "    \"\"\"Generic declarative autograd function.\n",
    "    Defines the forward and backward functions. Saves all inputs and outputs,\n",
    "    which may be memory-inefficient for the specific problem.\n",
    "    \n",
    "    Assumptions:\n",
    "    * All inputs are PyTorch tensors\n",
    "    * All inputs have a single batch dimension (b, ...)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, problem, *inputs):\n",
    "        output, solve_ctx = torch.no_grad()(problem.solve)(*inputs)\n",
    "        ctx.save_for_backward(output, *inputs)\n",
    "        ctx.problem = problem\n",
    "        ctx.solve_ctx = solve_ctx\n",
    "        return output.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output, *inputs = ctx.saved_tensors\n",
    "        problem = ctx.problem\n",
    "        solve_ctx = ctx.solve_ctx\n",
    "        output.requires_grad = True\n",
    "        inputs = tuple(inputs)\n",
    "        grad_inputs = problem.gradient(*inputs, y=output, v=grad_output,\n",
    "            ctx=solve_ctx)\n",
    "        return (None, *grad_inputs)\n",
    "\n",
    "\n",
    "class DeclarativeLayer(torch.nn.Module):\n",
    "    \"\"\"Generic declarative layer.\n",
    "    \n",
    "    Assumptions:\n",
    "    * All inputs are PyTorch tensors\n",
    "    * All inputs have a single batch dimension (b, ...)\n",
    "    Usage:\n",
    "        problem = <derived class of *DeclarativeNode>\n",
    "        declarative_layer = DeclarativeLayer(problem)\n",
    "        y = declarative_layer(x1, x2, ...)\n",
    "    \"\"\"\n",
    "    def __init__(self, problem):\n",
    "        super(DeclarativeLayer, self).__init__()\n",
    "        self.problem = problem\n",
    "        \n",
    "    def forward(self, *inputs):\n",
    "        return DeclarativeFunction.apply(self.problem, *inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f18d9a-f3bb-48c1-88c9-b09d9d03275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajNetLSTM2(nn.Module):\n",
    "    def __init__(self, opt_layer, P, Pdot, input_size=40, hidden_size=64, output_size=3, nvar=11, t_obs=8):\n",
    "        super(TrajNetLSTM2, self).__init__()\n",
    "        self.nvar = nvar\n",
    "        self.t_obs = t_obs\n",
    "        self.P = torch.tensor(P, dtype=torch.double).to(device)\n",
    "        self.Pdot = torch.tensor(Pdot, dtype=torch.double).to(device) \n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "        self.opt_layer = opt_layer\n",
    "        self.activation = nn.PReLU()\n",
    "        self.mask = torch.tensor([[0.0, 0.0, 1.0]], dtype=torch.double).to(device)\n",
    "    \n",
    "    def forward(self, x, fixed_params, var_inp):\n",
    "        batch_size = x.shape[0]\n",
    "        out = self.activation(self.linear1(x))\n",
    "        _, (hn, cn) = self.lstm(out.view(batch_size, 1, -1))\n",
    "        out = self.activation(self.linear2(hn[0]))\n",
    "        variable_params = self.linear3(out)\n",
    "        variable_params = self.mask * var_inp + (1-self.mask) * variable_params\n",
    "        \n",
    "        # Run optimization\n",
    "        sol = self.opt_layer(fixed_params, variable_params)\n",
    "         \n",
    "        # Compute final trajectory\n",
    "        x_pred = torch.matmul(self.P, sol[:, :self.nvar].transpose(0, 1))\n",
    "        y_pred = torch.matmul(self.P, sol[:, self.nvar:2*self.nvar].transpose(0, 1))\n",
    "        x_pred = x_pred.transpose(0, 1)\n",
    "        y_pred = y_pred.transpose(0, 1)\n",
    "        out = torch.cat([x_pred, y_pred], dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e3653ef-e6a8-4bff-a021-41ba7ec774be",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 30\n",
    "t_obs = 20\n",
    "num_elems = 15\n",
    "include_centerline = False\n",
    "name = \"final_without\" if include_centerline else \"final_with\"\n",
    "lr = 0.0005\n",
    "\n",
    "train_dataset = ArgoverseDataset(\"val_data.npy\", centerline_dir=\"val_centerlines.npy\", t_obs=20, dt=0.3, include_centerline = include_centerline)\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=False, num_workers=0)\n",
    "\n",
    "test_dataset = ArgoverseDataset(\"val_test_data.npy\", centerline_dir=\"val_test_centerlines.npy\", t_obs=20, dt=0.3, include_centerline = include_centerline)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False, num_workers=0)\n",
    "\n",
    "offsets_train = np.load(\"val_offsets.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f519a56e-adca-4f80-86cb-94f7c7c57f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 40]) torch.Size([20, 60]) torch.Size([20, 5]) torch.Size([20, 3])\n"
     ]
    }
   ],
   "source": [
    "for batch_num, data in enumerate(train_loader):\n",
    "    traj_inp, traj_out, fixed_params, var_inp = data\n",
    "    print(traj_inp.size(), traj_out.shape, fixed_params.shape, var_inp.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab604e7-f8fa-45f2-87f6-4dd99f9ba69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = OPTNode(rho_eq=10, t_fin=9.0, num=num)\n",
    "opt_layer = DeclarativeLayer(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "850d6dcc-474e-4c76-8f61-c571eb6382a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"LSTM\"\n",
    "\n",
    "if model_type == \"MLP\":\n",
    "    Model = TrajNet\n",
    "    model = Model(opt_layer, problem.P, problem.Pdot, input_size=t_obs * 2 + include_centerline * num_elems * 2)\n",
    "else:\n",
    "    Model = TrajNetLSTM2\n",
    "    model = Model(opt_layer, problem.P, problem.Pdot)\n",
    "\n",
    "# if flatten:\n",
    "    \n",
    "#     model = Model(opt_layer, problem.P, problem.Pdot, input_size=t_obs * 2 + include_centerline * num_elems * 2)\n",
    "# else:\n",
    "#     model = Model(opt_layer, problem.P, problem.Pdot)\n",
    "    \n",
    "model = model.double()\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb151f6e-d401-40dc-ace7-6bd06af00348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 419.96191817966394\n",
      "Epoch: 0, Batch: 20, Loss: 550.628209900511\n",
      "Epoch: 0, Batch: 40, Loss: 517.47136254722\n",
      "Epoch: 0, Mean Loss: 439.65039240608314\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 1, Batch: 0, Loss: 292.9328040311456\n",
      "Epoch: 1, Batch: 20, Loss: 347.9573279131005\n",
      "Epoch: 1, Batch: 40, Loss: 246.27161353742846\n",
      "Epoch: 1, Mean Loss: 236.0039269427845\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 2, Batch: 0, Loss: 122.98134488368956\n",
      "Epoch: 2, Batch: 20, Loss: 104.71803930392562\n",
      "Epoch: 2, Batch: 40, Loss: 81.2318825026222\n",
      "Epoch: 2, Mean Loss: 87.11101650837414\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 3, Batch: 0, Loss: 58.636994686604595\n",
      "Epoch: 3, Batch: 20, Loss: 43.733717686800496\n",
      "Epoch: 3, Batch: 40, Loss: 32.21471800616709\n",
      "Epoch: 3, Mean Loss: 38.10407485613135\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 4, Batch: 0, Loss: 30.43630649104231\n",
      "Epoch: 4, Batch: 20, Loss: 23.716698100920667\n",
      "Epoch: 4, Batch: 40, Loss: 16.162589664166408\n",
      "Epoch: 4, Mean Loss: 21.725698920360202\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 5, Batch: 0, Loss: 20.189698671844592\n",
      "Epoch: 5, Batch: 20, Loss: 10.832177894089977\n",
      "Epoch: 5, Batch: 40, Loss: 12.673364799361375\n",
      "Epoch: 5, Mean Loss: 14.868769415599933\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 6, Batch: 0, Loss: 16.345849307072005\n",
      "Epoch: 6, Batch: 20, Loss: 8.31981559685702\n",
      "Epoch: 6, Batch: 40, Loss: 10.736218475070544\n",
      "Epoch: 6, Mean Loss: 12.127031451498805\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 7, Batch: 0, Loss: 13.739611108460208\n",
      "Epoch: 7, Batch: 20, Loss: 6.426173076408024\n",
      "Epoch: 7, Batch: 40, Loss: 8.284222220925418\n",
      "Epoch: 7, Mean Loss: 10.295926150816447\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 8, Batch: 0, Loss: 12.635018403165887\n",
      "Epoch: 8, Batch: 20, Loss: 5.463755152341058\n",
      "Epoch: 8, Batch: 40, Loss: 6.503916705377671\n",
      "Epoch: 8, Mean Loss: 9.160374314693698\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 9, Batch: 0, Loss: 11.886844109842183\n",
      "Epoch: 9, Batch: 20, Loss: 5.7540186440499275\n",
      "Epoch: 9, Batch: 40, Loss: 4.461792254047115\n",
      "Epoch: 9, Mean Loss: 8.636473203775862\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 10, Batch: 0, Loss: 10.038308727892455\n",
      "Epoch: 10, Batch: 20, Loss: 6.450234667049519\n",
      "Epoch: 10, Batch: 40, Loss: 3.7232107668013352\n",
      "Epoch: 10, Mean Loss: 8.76386015389035\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 11, Batch: 0, Loss: 11.01616717871838\n",
      "Epoch: 11, Batch: 20, Loss: 6.269368482665723\n",
      "Epoch: 11, Batch: 40, Loss: 3.5531584951631356\n",
      "Epoch: 11, Mean Loss: 8.09444082768371\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 12, Batch: 0, Loss: 10.415999502940684\n",
      "Epoch: 12, Batch: 20, Loss: 5.821074247444887\n",
      "Epoch: 12, Batch: 40, Loss: 3.402311214187607\n",
      "Epoch: 12, Mean Loss: 7.7954497858873255\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 13, Batch: 0, Loss: 10.297138457749634\n",
      "Epoch: 13, Batch: 20, Loss: 5.439400692058968\n",
      "Epoch: 13, Batch: 40, Loss: 3.323240973844098\n",
      "Epoch: 13, Mean Loss: 7.5581120011258704\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 14, Batch: 0, Loss: 10.122683257978316\n",
      "Epoch: 14, Batch: 20, Loss: 5.289962835949686\n",
      "Epoch: 14, Batch: 40, Loss: 3.3003375676069533\n",
      "Epoch: 14, Mean Loss: 7.38104968347609\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 15, Batch: 0, Loss: 10.088949365783844\n",
      "Epoch: 15, Batch: 20, Loss: 4.95784646559232\n",
      "Epoch: 15, Batch: 40, Loss: 3.2741680199991285\n",
      "Epoch: 15, Mean Loss: 7.201334442924856\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 16, Batch: 0, Loss: 9.94749953151338\n",
      "Epoch: 16, Batch: 20, Loss: 4.787776793445356\n",
      "Epoch: 16, Batch: 40, Loss: 3.217704923169523\n",
      "Epoch: 16, Mean Loss: 7.059744316758612\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 17, Batch: 0, Loss: 9.869857911622075\n",
      "Epoch: 17, Batch: 20, Loss: 4.64388653207815\n",
      "Epoch: 17, Batch: 40, Loss: 3.170073379780061\n",
      "Epoch: 17, Mean Loss: 6.961300203782994\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 18, Batch: 0, Loss: 9.759157515231816\n",
      "Epoch: 18, Batch: 20, Loss: 4.467667500792016\n",
      "Epoch: 18, Batch: 40, Loss: 3.229604504203651\n",
      "Epoch: 18, Mean Loss: 6.857860895943199\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 19, Batch: 0, Loss: 9.675768235479\n",
      "Epoch: 19, Batch: 20, Loss: 4.304319326681015\n",
      "Epoch: 19, Batch: 40, Loss: 3.1066210889765724\n",
      "Epoch: 19, Mean Loss: 6.740892153914452\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 20, Batch: 0, Loss: 9.515220255345156\n",
      "Epoch: 20, Batch: 20, Loss: 4.3561092659392715\n",
      "Epoch: 20, Batch: 40, Loss: 3.086292082310593\n",
      "Epoch: 20, Mean Loss: 6.716091629876055\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 21, Batch: 0, Loss: 9.258297928507831\n",
      "Epoch: 21, Batch: 20, Loss: 4.285898272867636\n",
      "Epoch: 21, Batch: 40, Loss: 3.017824189565184\n",
      "Epoch: 21, Mean Loss: 6.657438893046555\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 22, Batch: 0, Loss: 9.272466252932313\n",
      "Epoch: 22, Batch: 20, Loss: 4.2254584385747\n",
      "Epoch: 22, Batch: 40, Loss: 3.0334082027566596\n",
      "Epoch: 22, Mean Loss: 6.607585803129111\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 23, Batch: 0, Loss: 9.271463098214811\n",
      "Epoch: 23, Batch: 20, Loss: 3.9862290563600573\n",
      "Epoch: 23, Batch: 40, Loss: 3.206058574629947\n",
      "Epoch: 23, Mean Loss: 6.5036276413768634\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 24, Batch: 0, Loss: 9.195472055552747\n",
      "Epoch: 24, Batch: 20, Loss: 3.876480543668879\n",
      "Epoch: 24, Batch: 40, Loss: 2.9998000305969024\n",
      "Epoch: 24, Mean Loss: 6.404894624176837\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 25, Batch: 0, Loss: 9.12924476438357\n",
      "Epoch: 25, Batch: 20, Loss: 3.970323568876591\n",
      "Epoch: 25, Batch: 40, Loss: 3.161126111684668\n",
      "Epoch: 25, Mean Loss: 6.452321126334859\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 26, Batch: 0, Loss: 9.006739945573452\n",
      "Epoch: 26, Batch: 20, Loss: 3.7655155310616975\n",
      "Epoch: 26, Batch: 40, Loss: 3.0333970398357932\n",
      "Epoch: 26, Mean Loss: 6.3423793136301745\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 27, Batch: 0, Loss: 9.074732049628238\n",
      "Epoch: 27, Batch: 20, Loss: 3.7994469860694458\n",
      "Epoch: 27, Batch: 40, Loss: 2.9297423241615412\n",
      "Epoch: 27, Mean Loss: 6.351077157215253\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 28, Batch: 0, Loss: 9.11730029632444\n",
      "Epoch: 28, Batch: 20, Loss: 3.851289297590703\n",
      "Epoch: 28, Batch: 40, Loss: 2.923607866554722\n",
      "Epoch: 28, Mean Loss: 6.340239230775502\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 29, Batch: 0, Loss: 9.054851026885501\n",
      "Epoch: 29, Batch: 20, Loss: 3.627852947946534\n",
      "Epoch: 29, Batch: 40, Loss: 3.0025492540625005\n",
      "Epoch: 29, Mean Loss: 6.256143795812675\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 30, Batch: 0, Loss: 8.966079283345808\n",
      "Epoch: 30, Batch: 20, Loss: 3.7143701140850336\n",
      "Epoch: 30, Batch: 40, Loss: 2.994478820105601\n",
      "Epoch: 30, Mean Loss: 6.219694581227629\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 31, Batch: 0, Loss: 8.781400400154824\n",
      "Epoch: 31, Batch: 20, Loss: 3.844486203369265\n",
      "Epoch: 31, Batch: 40, Loss: 2.651527294468484\n",
      "Epoch: 31, Mean Loss: 6.171252101877725\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 32, Batch: 0, Loss: 8.85957440842348\n",
      "Epoch: 32, Batch: 20, Loss: 3.78810721548599\n",
      "Epoch: 32, Batch: 40, Loss: 2.8053329727145924\n",
      "Epoch: 32, Mean Loss: 6.190672943074005\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 33, Batch: 0, Loss: 8.74376787238109\n",
      "Epoch: 33, Batch: 20, Loss: 3.4205140748158307\n",
      "Epoch: 33, Batch: 40, Loss: 3.2240456318983544\n",
      "Epoch: 33, Mean Loss: 6.0231216254699\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 34, Batch: 0, Loss: 8.493806388948846\n",
      "Epoch: 34, Batch: 20, Loss: 3.6198440997182586\n",
      "Epoch: 34, Batch: 40, Loss: 2.8034964308908945\n",
      "Epoch: 34, Mean Loss: 6.0374236314408725\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 35, Batch: 0, Loss: 8.725972802635358\n",
      "Epoch: 35, Batch: 20, Loss: 3.722165444136043\n",
      "Epoch: 35, Batch: 40, Loss: 2.948831213667499\n",
      "Epoch: 35, Mean Loss: 6.071435422191062\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 36, Batch: 0, Loss: 8.5498468949195\n",
      "Epoch: 36, Batch: 20, Loss: 3.652772703587433\n",
      "Epoch: 36, Batch: 40, Loss: 2.995182471337778\n",
      "Epoch: 36, Mean Loss: 6.057393629691389\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 37, Batch: 0, Loss: 8.519301779926552\n",
      "Epoch: 37, Batch: 20, Loss: 3.597116913380369\n",
      "Epoch: 37, Batch: 40, Loss: 2.8552430936782374\n",
      "Epoch: 37, Mean Loss: 6.0031115034867595\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 38, Batch: 0, Loss: 8.627032713206368\n",
      "Epoch: 38, Batch: 20, Loss: 3.9509542048317705\n",
      "Epoch: 38, Batch: 40, Loss: 2.8527106239147204\n",
      "Epoch: 38, Mean Loss: 6.149036390544314\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 39, Batch: 0, Loss: 8.585498927807782\n",
      "Epoch: 39, Batch: 20, Loss: 5.143323972488735\n",
      "Epoch: 39, Batch: 40, Loss: 2.2489241965901354\n",
      "Epoch: 39, Mean Loss: 6.445398235312487\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 40, Batch: 0, Loss: 9.145494621557893\n",
      "Epoch: 40, Batch: 20, Loss: 3.2229539134064225\n",
      "Epoch: 40, Batch: 40, Loss: 4.371094816459368\n",
      "Epoch: 40, Mean Loss: 6.124355175146922\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 41, Batch: 0, Loss: 7.603060604309841\n",
      "Epoch: 41, Batch: 20, Loss: 3.2068593986418508\n",
      "Epoch: 41, Batch: 40, Loss: 3.63036993271014\n",
      "Epoch: 41, Mean Loss: 5.785147454879299\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 42, Batch: 0, Loss: 7.9235569581708\n",
      "Epoch: 42, Batch: 20, Loss: 3.0858344844356567\n",
      "Epoch: 42, Batch: 40, Loss: 3.6979492255407984\n",
      "Epoch: 42, Mean Loss: 5.7975220735097945\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 43, Batch: 0, Loss: 7.96810432429782\n",
      "Epoch: 43, Batch: 20, Loss: 3.1309973588503404\n",
      "Epoch: 43, Batch: 40, Loss: 3.3648202061330474\n",
      "Epoch: 43, Mean Loss: 5.788287576902494\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 44, Batch: 0, Loss: 8.01206956253163\n",
      "Epoch: 44, Batch: 20, Loss: 3.0582720920258497\n",
      "Epoch: 44, Batch: 40, Loss: 3.5656612318100915\n",
      "Epoch: 44, Mean Loss: 5.740863502054124\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 45, Batch: 0, Loss: 7.716150165733601\n",
      "Epoch: 45, Batch: 20, Loss: 3.0669097086618824\n",
      "Epoch: 45, Batch: 40, Loss: 3.08098572657586\n",
      "Epoch: 45, Mean Loss: 5.631778097465578\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 46, Batch: 0, Loss: 7.80784027139931\n",
      "Epoch: 46, Batch: 20, Loss: 3.201167643783966\n",
      "Epoch: 46, Batch: 40, Loss: 2.8771001777107044\n",
      "Epoch: 46, Mean Loss: 5.629464692949923\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 47, Batch: 0, Loss: 7.966852728526877\n",
      "Epoch: 47, Batch: 20, Loss: 2.981653044100992\n",
      "Epoch: 47, Batch: 40, Loss: 2.9717455122658434\n",
      "Epoch: 47, Mean Loss: 5.537940675213787\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 48, Batch: 0, Loss: 7.954736853017306\n",
      "Epoch: 48, Batch: 20, Loss: 2.992023281581345\n",
      "Epoch: 48, Batch: 40, Loss: 2.855693479305673\n",
      "Epoch: 48, Mean Loss: 5.537188157815863\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 49, Batch: 0, Loss: 7.904751627719706\n",
      "Epoch: 49, Batch: 20, Loss: 2.9845279080211\n",
      "Epoch: 49, Batch: 40, Loss: 2.8483441684172237\n",
      "Epoch: 49, Mean Loss: 5.512754135010868\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 50, Batch: 0, Loss: 7.885267703908188\n",
      "Epoch: 50, Batch: 20, Loss: 2.996274785632738\n",
      "Epoch: 50, Batch: 40, Loss: 2.793403940578502\n",
      "Epoch: 50, Mean Loss: 5.467509380981386\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 51, Batch: 0, Loss: 7.921516460637092\n",
      "Epoch: 51, Batch: 20, Loss: 2.9840320550960704\n",
      "Epoch: 51, Batch: 40, Loss: 2.8600439819084236\n",
      "Epoch: 51, Mean Loss: 5.434902925137075\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 52, Batch: 0, Loss: 7.84641139217437\n",
      "Epoch: 52, Batch: 20, Loss: 2.9632233442210065\n",
      "Epoch: 52, Batch: 40, Loss: 2.825494564006822\n",
      "Epoch: 52, Mean Loss: 5.387532130427352\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 53, Batch: 0, Loss: 7.777686291912536\n",
      "Epoch: 53, Batch: 20, Loss: 2.9880702818043976\n",
      "Epoch: 53, Batch: 40, Loss: 2.845632798457703\n",
      "Epoch: 53, Mean Loss: 5.427083289658688\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 54, Batch: 0, Loss: 7.83365149305984\n",
      "Epoch: 54, Batch: 20, Loss: 3.259235077606114\n",
      "Epoch: 54, Batch: 40, Loss: 2.6397743290829387\n",
      "Epoch: 54, Mean Loss: 5.521877052792876\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 55, Batch: 0, Loss: 7.79509216118182\n",
      "Epoch: 55, Batch: 20, Loss: 3.095552229825497\n",
      "Epoch: 55, Batch: 40, Loss: 2.9280342261619667\n",
      "Epoch: 55, Mean Loss: 5.4779283359600495\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 56, Batch: 0, Loss: 7.718592199554489\n",
      "Epoch: 56, Batch: 20, Loss: 3.0989574542624223\n",
      "Epoch: 56, Batch: 40, Loss: 3.255016637894055\n",
      "Epoch: 56, Mean Loss: 5.431867955152225\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 57, Batch: 0, Loss: 7.438851732312839\n",
      "Epoch: 57, Batch: 20, Loss: 3.246387441579866\n",
      "Epoch: 57, Batch: 40, Loss: 2.529289926966806\n",
      "Epoch: 57, Mean Loss: 5.38446240224001\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 58, Batch: 0, Loss: 7.674061541647576\n",
      "Epoch: 58, Batch: 20, Loss: 3.430092028795541\n",
      "Epoch: 58, Batch: 40, Loss: 2.9871606842958496\n",
      "Epoch: 58, Mean Loss: 5.537285838346437\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 59, Batch: 0, Loss: 7.921190355920796\n",
      "Epoch: 59, Batch: 20, Loss: 3.267881278586571\n",
      "Epoch: 59, Batch: 40, Loss: 3.1660370093947883\n",
      "Epoch: 59, Mean Loss: 5.446040935495533\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 60, Batch: 0, Loss: 7.8047712975263845\n",
      "Epoch: 60, Batch: 20, Loss: 3.005293977211283\n",
      "Epoch: 60, Batch: 40, Loss: 3.257260684809276\n",
      "Epoch: 60, Mean Loss: 5.47392247786762\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 61, Batch: 0, Loss: 7.625925343308501\n",
      "Epoch: 61, Batch: 20, Loss: 4.225943659683015\n",
      "Epoch: 61, Batch: 40, Loss: 2.4567532758616832\n",
      "Epoch: 61, Mean Loss: 5.593377445645676\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 62, Batch: 0, Loss: 7.820735608835402\n",
      "Epoch: 62, Batch: 20, Loss: 2.971424507398203\n",
      "Epoch: 62, Batch: 40, Loss: 2.7503386474981126\n",
      "Epoch: 62, Mean Loss: 5.4180549553583885\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 63, Batch: 0, Loss: 7.6402644122060845\n",
      "Epoch: 63, Batch: 20, Loss: 3.1990999748264617\n",
      "Epoch: 63, Batch: 40, Loss: 2.780207256595414\n",
      "Epoch: 63, Mean Loss: 5.343270308093516\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 64, Batch: 0, Loss: 7.603342546187957\n",
      "Epoch: 64, Batch: 20, Loss: 2.980097009945368\n",
      "Epoch: 64, Batch: 40, Loss: 2.8808114162634624\n",
      "Epoch: 64, Mean Loss: 5.24831488548553\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 65, Batch: 0, Loss: 7.160322319093536\n",
      "Epoch: 65, Batch: 20, Loss: 3.1600597322311277\n",
      "Epoch: 65, Batch: 40, Loss: 2.7123946988271466\n",
      "Epoch: 65, Mean Loss: 5.267431739283738\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 66, Batch: 0, Loss: 7.502830178376447\n",
      "Epoch: 66, Batch: 20, Loss: 3.1205832277121264\n",
      "Epoch: 66, Batch: 40, Loss: 2.6720814840364895\n",
      "Epoch: 66, Mean Loss: 5.28744835821714\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 67, Batch: 0, Loss: 7.530779386538934\n",
      "Epoch: 67, Batch: 20, Loss: 2.9765861204887014\n",
      "Epoch: 67, Batch: 40, Loss: 2.8792026467959895\n",
      "Epoch: 67, Mean Loss: 5.216019034478168\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 68, Batch: 0, Loss: 7.440996277013117\n",
      "Epoch: 68, Batch: 20, Loss: 2.9741336369525793\n",
      "Epoch: 68, Batch: 40, Loss: 2.816952232127107\n",
      "Epoch: 68, Mean Loss: 5.198523564265514\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 69, Batch: 0, Loss: 7.398645579409752\n",
      "Epoch: 69, Batch: 20, Loss: 3.026001245821587\n",
      "Epoch: 69, Batch: 40, Loss: 2.714457811059213\n",
      "Epoch: 69, Mean Loss: 5.153116276672456\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 70, Batch: 0, Loss: 7.372443929927559\n",
      "Epoch: 70, Batch: 20, Loss: 3.0950307750658372\n",
      "Epoch: 70, Batch: 40, Loss: 2.6930805826038458\n",
      "Epoch: 70, Mean Loss: 5.195260600079536\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 71, Batch: 0, Loss: 7.462605451899883\n",
      "Epoch: 71, Batch: 20, Loss: 3.2141084010283563\n",
      "Epoch: 71, Batch: 40, Loss: 2.658867093219271\n",
      "Epoch: 71, Mean Loss: 5.194145983215948\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 72, Batch: 0, Loss: 7.381522713731429\n",
      "Epoch: 72, Batch: 20, Loss: 3.0550215654469577\n",
      "Epoch: 72, Batch: 40, Loss: 2.732002618955687\n",
      "Epoch: 72, Mean Loss: 5.191411450450483\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 73, Batch: 0, Loss: 7.41748787763002\n",
      "Epoch: 73, Batch: 20, Loss: 2.851842488101426\n",
      "Epoch: 73, Batch: 40, Loss: 3.0322530514068275\n",
      "Epoch: 73, Mean Loss: 5.147878014967063\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 74, Batch: 0, Loss: 7.342834756741905\n",
      "Epoch: 74, Batch: 20, Loss: 2.903056358167889\n",
      "Epoch: 74, Batch: 40, Loss: 2.855070225638725\n",
      "Epoch: 74, Mean Loss: 5.153281130502897\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 75, Batch: 0, Loss: 7.261596174683255\n",
      "Epoch: 75, Batch: 20, Loss: 3.0504164807577414\n",
      "Epoch: 75, Batch: 40, Loss: 2.7979429113222003\n",
      "Epoch: 75, Mean Loss: 5.127963111335479\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 76, Batch: 0, Loss: 7.18709800008005\n",
      "Epoch: 76, Batch: 20, Loss: 3.1711970205051765\n",
      "Epoch: 76, Batch: 40, Loss: 2.5964054311463736\n",
      "Epoch: 76, Mean Loss: 5.2836278031211625\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 77, Batch: 0, Loss: 7.192795107158423\n",
      "Epoch: 77, Batch: 20, Loss: 3.1060918189155284\n",
      "Epoch: 77, Batch: 40, Loss: 2.720848508374199\n",
      "Epoch: 77, Mean Loss: 5.22180472490661\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 78, Batch: 0, Loss: 7.385152714230261\n",
      "Epoch: 78, Batch: 20, Loss: 3.0845091270844645\n",
      "Epoch: 78, Batch: 40, Loss: 2.6321575793241307\n",
      "Epoch: 78, Mean Loss: 5.147243828953583\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 79, Batch: 0, Loss: 7.215923172522985\n",
      "Epoch: 79, Batch: 20, Loss: 3.1609331132874474\n",
      "Epoch: 79, Batch: 40, Loss: 2.7788549520656534\n",
      "Epoch: 79, Mean Loss: 5.211658416471019\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 80, Batch: 0, Loss: 7.467511060886217\n",
      "Epoch: 80, Batch: 20, Loss: 2.9977526925355167\n",
      "Epoch: 80, Batch: 40, Loss: 2.8841825285894016\n",
      "Epoch: 80, Mean Loss: 5.292864397145912\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 81, Batch: 0, Loss: 7.260534338684496\n",
      "Epoch: 81, Batch: 20, Loss: 3.8772317441424247\n",
      "Epoch: 81, Batch: 40, Loss: 2.19038201374483\n",
      "Epoch: 81, Mean Loss: 5.267671786390903\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 82, Batch: 0, Loss: 7.0713126654553635\n",
      "Epoch: 82, Batch: 20, Loss: 3.313930980139635\n",
      "Epoch: 82, Batch: 40, Loss: 2.51456060148823\n",
      "Epoch: 82, Mean Loss: 5.3564337105077415\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 83, Batch: 0, Loss: 7.467773370215661\n",
      "Epoch: 83, Batch: 20, Loss: 3.2517877988617268\n",
      "Epoch: 83, Batch: 40, Loss: 2.575138817180388\n",
      "Epoch: 83, Mean Loss: 5.1908411915465775\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 84, Batch: 0, Loss: 7.352917576650448\n",
      "Epoch: 84, Batch: 20, Loss: 3.1000525255821003\n",
      "Epoch: 84, Batch: 40, Loss: 2.9080216226760216\n",
      "Epoch: 84, Mean Loss: 5.161453970358201\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 85, Batch: 0, Loss: 7.075308849613497\n",
      "Epoch: 85, Batch: 20, Loss: 2.765860552743518\n",
      "Epoch: 85, Batch: 40, Loss: 2.9898613416195006\n",
      "Epoch: 85, Mean Loss: 4.994652659516111\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 86, Batch: 0, Loss: 6.9998873305544755\n",
      "Epoch: 86, Batch: 20, Loss: 3.1003732249246605\n",
      "Epoch: 86, Batch: 40, Loss: 3.1261525908645535\n",
      "Epoch: 86, Mean Loss: 5.0862338313469815\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 87, Batch: 0, Loss: 7.029225300454596\n",
      "Epoch: 87, Batch: 20, Loss: 4.017234248001412\n",
      "Epoch: 87, Batch: 40, Loss: 2.358273122114276\n",
      "Epoch: 87, Mean Loss: 5.614798787593398\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 88, Batch: 0, Loss: 7.6029062911812835\n",
      "Epoch: 88, Batch: 20, Loss: 2.9472210703551083\n",
      "Epoch: 88, Batch: 40, Loss: 3.4136509965056305\n",
      "Epoch: 88, Mean Loss: 5.124827835226722\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 89, Batch: 0, Loss: 6.686396399141817\n",
      "Epoch: 89, Batch: 20, Loss: 2.8330186494528142\n",
      "Epoch: 89, Batch: 40, Loss: 3.1345371916537537\n",
      "Epoch: 89, Mean Loss: 4.937079018176343\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 90, Batch: 0, Loss: 6.638101192923338\n",
      "Epoch: 90, Batch: 20, Loss: 2.827704342379941\n",
      "Epoch: 90, Batch: 40, Loss: 3.213061501797632\n",
      "Epoch: 90, Mean Loss: 4.918122586719599\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 91, Batch: 0, Loss: 6.679140325926845\n",
      "Epoch: 91, Batch: 20, Loss: 2.8109008391063104\n",
      "Epoch: 91, Batch: 40, Loss: 3.0698029081145712\n",
      "Epoch: 91, Mean Loss: 4.8594187222556045\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 92, Batch: 0, Loss: 6.716561817565032\n",
      "Epoch: 92, Batch: 20, Loss: 2.7640239953383654\n",
      "Epoch: 92, Batch: 40, Loss: 3.119902904818188\n",
      "Epoch: 92, Mean Loss: 4.875044467465651\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 93, Batch: 0, Loss: 6.6619053374219535\n",
      "Epoch: 93, Batch: 20, Loss: 2.811027186857952\n",
      "Epoch: 93, Batch: 40, Loss: 2.9112153707491264\n",
      "Epoch: 93, Mean Loss: 4.855579166016772\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 94, Batch: 0, Loss: 6.687221051637503\n",
      "Epoch: 94, Batch: 20, Loss: 2.7202167558334183\n",
      "Epoch: 94, Batch: 40, Loss: 2.9732230731556775\n",
      "Epoch: 94, Mean Loss: 4.84033334735151\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 95, Batch: 0, Loss: 6.652431811079002\n",
      "Epoch: 95, Batch: 20, Loss: 2.794012115030398\n",
      "Epoch: 95, Batch: 40, Loss: 2.937898964905969\n",
      "Epoch: 95, Mean Loss: 4.8330084890035\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 96, Batch: 0, Loss: 6.731313690004723\n",
      "Epoch: 96, Batch: 20, Loss: 2.7291193360775594\n",
      "Epoch: 96, Batch: 40, Loss: 2.7243380465391143\n",
      "Epoch: 96, Mean Loss: 4.9122811338629955\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 97, Batch: 0, Loss: 6.639465074072713\n",
      "Epoch: 97, Batch: 20, Loss: 2.840075689956144\n",
      "Epoch: 97, Batch: 40, Loss: 2.892832320821552\n",
      "Epoch: 97, Mean Loss: 4.970264943406347\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 98, Batch: 0, Loss: 7.0102652880863\n",
      "Epoch: 98, Batch: 20, Loss: 3.038032673229513\n",
      "Epoch: 98, Batch: 40, Loss: 2.6559809179489866\n",
      "Epoch: 98, Mean Loss: 4.976056443682437\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 99, Batch: 0, Loss: 6.8713767077670935\n",
      "Epoch: 99, Batch: 20, Loss: 2.7804639915664926\n",
      "Epoch: 99, Batch: 40, Loss: 2.8103249015834018\n",
      "Epoch: 99, Mean Loss: 4.857046333044777\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 100, Batch: 0, Loss: 6.596354218676493\n",
      "Epoch: 100, Batch: 20, Loss: 2.7760401346383734\n",
      "Epoch: 100, Batch: 40, Loss: 2.762603059426615\n",
      "Epoch: 100, Mean Loss: 4.811761480211464\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 101, Batch: 0, Loss: 6.596626450345992\n",
      "Epoch: 101, Batch: 20, Loss: 2.700663142037927\n",
      "Epoch: 101, Batch: 40, Loss: 2.806875259999504\n",
      "Epoch: 101, Mean Loss: 4.79078897882552\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 102, Batch: 0, Loss: 6.4030518682390865\n",
      "Epoch: 102, Batch: 20, Loss: 2.7053652903393512\n",
      "Epoch: 102, Batch: 40, Loss: 2.7929576703182883\n",
      "Epoch: 102, Mean Loss: 4.803774844256879\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 103, Batch: 0, Loss: 6.426126351941342\n",
      "Epoch: 103, Batch: 20, Loss: 2.9101403158028636\n",
      "Epoch: 103, Batch: 40, Loss: 2.626566521499065\n",
      "Epoch: 103, Mean Loss: 4.8718135158354565\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 104, Batch: 0, Loss: 6.698665461000346\n",
      "Epoch: 104, Batch: 20, Loss: 2.929560145790229\n",
      "Epoch: 104, Batch: 40, Loss: 2.657098826025885\n",
      "Epoch: 104, Mean Loss: 4.86722067914535\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 105, Batch: 0, Loss: 6.653545419456503\n",
      "Epoch: 105, Batch: 20, Loss: 2.6899293230867496\n",
      "Epoch: 105, Batch: 40, Loss: 2.682982756239471\n",
      "Epoch: 105, Mean Loss: 4.790205439716583\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 106, Batch: 0, Loss: 6.501834889963131\n",
      "Epoch: 106, Batch: 20, Loss: 2.7243247942493296\n",
      "Epoch: 106, Batch: 40, Loss: 2.7809422897681944\n",
      "Epoch: 106, Mean Loss: 4.729273203473587\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 107, Batch: 0, Loss: 6.524756490443421\n",
      "Epoch: 107, Batch: 20, Loss: 2.8770013381957953\n",
      "Epoch: 107, Batch: 40, Loss: 2.53657681128892\n",
      "Epoch: 107, Mean Loss: 4.784194980647735\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 108, Batch: 0, Loss: 6.312183976481503\n",
      "Epoch: 108, Batch: 20, Loss: 2.964870215286024\n",
      "Epoch: 108, Batch: 40, Loss: 2.6995225920935204\n",
      "Epoch: 108, Mean Loss: 4.859344422091219\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 109, Batch: 0, Loss: 6.423508047474112\n",
      "Epoch: 109, Batch: 20, Loss: 3.200433604843696\n",
      "Epoch: 109, Batch: 40, Loss: 2.329787123135559\n",
      "Epoch: 109, Mean Loss: 4.877775302554818\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 110, Batch: 0, Loss: 6.261376761817551\n",
      "Epoch: 110, Batch: 20, Loss: 3.492892246978156\n",
      "Epoch: 110, Batch: 40, Loss: 2.357021122356072\n",
      "Epoch: 110, Mean Loss: 4.9700670271650464\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 111, Batch: 0, Loss: 6.750509566322362\n",
      "Epoch: 111, Batch: 20, Loss: 2.7727385074133744\n",
      "Epoch: 111, Batch: 40, Loss: 2.715774195464128\n",
      "Epoch: 111, Mean Loss: 4.801823931592894\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 112, Batch: 0, Loss: 6.515435292122502\n",
      "Epoch: 112, Batch: 20, Loss: 2.803409243912332\n",
      "Epoch: 112, Batch: 40, Loss: 2.5669450294485032\n",
      "Epoch: 112, Mean Loss: 4.7014041986678725\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 113, Batch: 0, Loss: 6.544924047407527\n",
      "Epoch: 113, Batch: 20, Loss: 3.023126871393857\n",
      "Epoch: 113, Batch: 40, Loss: 2.4791205436849744\n",
      "Epoch: 113, Mean Loss: 4.763562246532673\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 114, Batch: 0, Loss: 6.3069696906444275\n",
      "Epoch: 114, Batch: 20, Loss: 2.8744371887766396\n",
      "Epoch: 114, Batch: 40, Loss: 2.5858845384658284\n",
      "Epoch: 114, Mean Loss: 4.699739247149888\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 115, Batch: 0, Loss: 6.435192633799811\n",
      "Epoch: 115, Batch: 20, Loss: 2.892314871705553\n",
      "Epoch: 115, Batch: 40, Loss: 2.5334805338730435\n",
      "Epoch: 115, Mean Loss: 4.696581053448759\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 116, Batch: 0, Loss: 6.233338907226347\n",
      "Epoch: 116, Batch: 20, Loss: 2.9940555770619044\n",
      "Epoch: 116, Batch: 40, Loss: 2.577163078345052\n",
      "Epoch: 116, Mean Loss: 4.6928287909005\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 117, Batch: 0, Loss: 6.472912126663434\n",
      "Epoch: 117, Batch: 20, Loss: 3.043379636833244\n",
      "Epoch: 117, Batch: 40, Loss: 2.7037542674890265\n",
      "Epoch: 117, Mean Loss: 4.71323413766427\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 118, Batch: 0, Loss: 6.245041279353091\n",
      "Epoch: 118, Batch: 20, Loss: 2.9733264003883906\n",
      "Epoch: 118, Batch: 40, Loss: 2.5533391584022276\n",
      "Epoch: 118, Mean Loss: 4.649237201504702\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 119, Batch: 0, Loss: 6.132246246090697\n",
      "Epoch: 119, Batch: 20, Loss: 2.8899490646649557\n",
      "Epoch: 119, Batch: 40, Loss: 2.5962563836641444\n",
      "Epoch: 119, Mean Loss: 4.684276613098065\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 120, Batch: 0, Loss: 6.274081460338742\n",
      "Epoch: 120, Batch: 20, Loss: 2.994828240364339\n",
      "Epoch: 120, Batch: 40, Loss: 2.556322283861285\n",
      "Epoch: 120, Mean Loss: 4.641621025714178\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 121, Batch: 0, Loss: 6.2526732807455225\n",
      "Epoch: 121, Batch: 20, Loss: 3.069989702570949\n",
      "Epoch: 121, Batch: 40, Loss: 2.5792791545973004\n",
      "Epoch: 121, Mean Loss: 4.725537390803981\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 122, Batch: 0, Loss: 6.236230796431218\n",
      "Epoch: 122, Batch: 20, Loss: 2.926373665772421\n",
      "Epoch: 122, Batch: 40, Loss: 2.8030684705330846\n",
      "Epoch: 122, Mean Loss: 4.6453552930701765\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 123, Batch: 0, Loss: 6.4084532158297165\n",
      "Epoch: 123, Batch: 20, Loss: 3.009837847086942\n",
      "Epoch: 123, Batch: 40, Loss: 2.558044657817418\n",
      "Epoch: 123, Mean Loss: 4.784323597384545\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 124, Batch: 0, Loss: 6.135240519624095\n",
      "Epoch: 124, Batch: 20, Loss: 3.2066517749301133\n",
      "Epoch: 124, Batch: 40, Loss: 2.593850222201833\n",
      "Epoch: 124, Mean Loss: 4.703925845508917\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 125, Batch: 0, Loss: 6.569256398015625\n",
      "Epoch: 125, Batch: 20, Loss: 3.1612820228141483\n",
      "Epoch: 125, Batch: 40, Loss: 2.6487156197505133\n",
      "Epoch: 125, Mean Loss: 4.681227020559914\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 126, Batch: 0, Loss: 6.339449211576867\n",
      "Epoch: 126, Batch: 20, Loss: 3.105604016847514\n",
      "Epoch: 126, Batch: 40, Loss: 2.6783104457740228\n",
      "Epoch: 126, Mean Loss: 4.646513164800531\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 127, Batch: 0, Loss: 6.063607351842384\n",
      "Epoch: 127, Batch: 20, Loss: 3.083675371455057\n",
      "Epoch: 127, Batch: 40, Loss: 2.672004412145239\n",
      "Epoch: 127, Mean Loss: 4.715529716818601\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 128, Batch: 0, Loss: 6.436884594461779\n",
      "Epoch: 128, Batch: 20, Loss: 2.9261493106084098\n",
      "Epoch: 128, Batch: 40, Loss: 2.7871979752080014\n",
      "Epoch: 128, Mean Loss: 4.624231908322243\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 129, Batch: 0, Loss: 6.153088170466554\n",
      "Epoch: 129, Batch: 20, Loss: 2.8884936372485974\n",
      "Epoch: 129, Batch: 40, Loss: 2.7363213440577114\n",
      "Epoch: 129, Mean Loss: 4.671593388275351\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 130, Batch: 0, Loss: 6.477548150727304\n",
      "Epoch: 130, Batch: 20, Loss: 2.8862825809351573\n",
      "Epoch: 130, Batch: 40, Loss: 2.8509216858926254\n",
      "Epoch: 130, Mean Loss: 4.632225749541446\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 131, Batch: 0, Loss: 6.241542338238279\n",
      "Epoch: 131, Batch: 20, Loss: 2.798751171782026\n",
      "Epoch: 131, Batch: 40, Loss: 2.6727540947341533\n",
      "Epoch: 131, Mean Loss: 4.609987745329474\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 132, Batch: 0, Loss: 6.123827340839844\n",
      "Epoch: 132, Batch: 20, Loss: 3.0083425461670235\n",
      "Epoch: 132, Batch: 40, Loss: 2.5270608407631276\n",
      "Epoch: 132, Mean Loss: 4.624308109362852\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 133, Batch: 0, Loss: 6.09493027296186\n",
      "Epoch: 133, Batch: 20, Loss: 2.9927448497482225\n",
      "Epoch: 133, Batch: 40, Loss: 2.800558278167299\n",
      "Epoch: 133, Mean Loss: 4.6035860593852815\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 134, Batch: 0, Loss: 6.131242815861935\n",
      "Epoch: 134, Batch: 20, Loss: 3.0237323611395093\n",
      "Epoch: 134, Batch: 40, Loss: 3.0667654050373323\n",
      "Epoch: 134, Mean Loss: 4.605606231342246\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 135, Batch: 0, Loss: 5.874053219855786\n",
      "Epoch: 135, Batch: 20, Loss: 2.9002540178844924\n",
      "Epoch: 135, Batch: 40, Loss: 2.8735809577678055\n",
      "Epoch: 135, Mean Loss: 4.642747804825665\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 136, Batch: 0, Loss: 5.964049455242653\n",
      "Epoch: 136, Batch: 20, Loss: 3.2052583375625217\n",
      "Epoch: 136, Batch: 40, Loss: 2.63514736029272\n",
      "Epoch: 136, Mean Loss: 4.681123087889864\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 137, Batch: 0, Loss: 6.028630356312626\n",
      "Epoch: 137, Batch: 20, Loss: 2.8295285443241545\n",
      "Epoch: 137, Batch: 40, Loss: 2.8944919330026257\n",
      "Epoch: 137, Mean Loss: 4.614314114141345\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 138, Batch: 0, Loss: 6.121736929789891\n",
      "Epoch: 138, Batch: 20, Loss: 2.9805108701822514\n",
      "Epoch: 138, Batch: 40, Loss: 2.929614300982885\n",
      "Epoch: 138, Mean Loss: 4.567711951848164\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 139, Batch: 0, Loss: 6.095015142751436\n",
      "Epoch: 139, Batch: 20, Loss: 2.7224261528576217\n",
      "Epoch: 139, Batch: 40, Loss: 3.010186570645442\n",
      "Epoch: 139, Mean Loss: 4.553095306815743\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 140, Batch: 0, Loss: 6.121615345165425\n",
      "Epoch: 140, Batch: 20, Loss: 2.811309668452124\n",
      "Epoch: 140, Batch: 40, Loss: 3.070430980167434\n",
      "Epoch: 140, Mean Loss: 4.559514434920506\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 141, Batch: 0, Loss: 6.15125392989405\n",
      "Epoch: 141, Batch: 20, Loss: 2.8102832878262674\n",
      "Epoch: 141, Batch: 40, Loss: 3.089165893477573\n",
      "Epoch: 141, Mean Loss: 4.636121215065043\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 142, Batch: 0, Loss: 5.675965183879942\n",
      "Epoch: 142, Batch: 20, Loss: 3.0683758910630408\n",
      "Epoch: 142, Batch: 40, Loss: 2.8130182039230434\n",
      "Epoch: 142, Mean Loss: 4.612896328503801\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 143, Batch: 0, Loss: 5.964601479271323\n",
      "Epoch: 143, Batch: 20, Loss: 3.861120237378737\n",
      "Epoch: 143, Batch: 40, Loss: 2.575212084439777\n",
      "Epoch: 143, Mean Loss: 4.788502191449303\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 144, Batch: 0, Loss: 6.041270045727449\n",
      "Epoch: 144, Batch: 20, Loss: 2.9534720349067003\n",
      "Epoch: 144, Batch: 40, Loss: 2.7470763344230176\n",
      "Epoch: 144, Mean Loss: 4.649840577239664\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 145, Batch: 0, Loss: 5.583048524208686\n",
      "Epoch: 145, Batch: 20, Loss: 3.536732686988617\n",
      "Epoch: 145, Batch: 40, Loss: 2.522182249866537\n",
      "Epoch: 145, Mean Loss: 4.832643077284991\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 146, Batch: 0, Loss: 6.049404298665471\n",
      "Epoch: 146, Batch: 20, Loss: 2.878370229748121\n",
      "Epoch: 146, Batch: 40, Loss: 3.2745985306830843\n",
      "Epoch: 146, Mean Loss: 4.713727669823674\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 147, Batch: 0, Loss: 6.01791905865413\n",
      "Epoch: 147, Batch: 20, Loss: 2.6188138830164815\n",
      "Epoch: 147, Batch: 40, Loss: 2.883809855793074\n",
      "Epoch: 147, Mean Loss: 4.47049134126508\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 148, Batch: 0, Loss: 5.944689941994529\n",
      "Epoch: 148, Batch: 20, Loss: 2.7622533091940737\n",
      "Epoch: 148, Batch: 40, Loss: 2.8975708696782365\n",
      "Epoch: 148, Mean Loss: 4.5339045782548615\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 149, Batch: 0, Loss: 6.01548032043399\n",
      "Epoch: 149, Batch: 20, Loss: 2.7813216357184376\n",
      "Epoch: 149, Batch: 40, Loss: 2.8779831292591562\n",
      "Epoch: 149, Mean Loss: 4.525167196637888\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 150, Batch: 0, Loss: 5.753357441800921\n",
      "Epoch: 150, Batch: 20, Loss: 2.888495852193695\n",
      "Epoch: 150, Batch: 40, Loss: 2.857810319376124\n",
      "Epoch: 150, Mean Loss: 4.523077350386586\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 151, Batch: 0, Loss: 5.77265376470579\n",
      "Epoch: 151, Batch: 20, Loss: 2.7443110212321744\n",
      "Epoch: 151, Batch: 40, Loss: 3.1023327975084714\n",
      "Epoch: 151, Mean Loss: 4.479502647296642\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 152, Batch: 0, Loss: 5.975183191717025\n",
      "Epoch: 152, Batch: 20, Loss: 2.7276313433240507\n",
      "Epoch: 152, Batch: 40, Loss: 2.9302426802375976\n",
      "Epoch: 152, Mean Loss: 4.418625530311988\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 153, Batch: 0, Loss: 5.7121265828858885\n",
      "Epoch: 153, Batch: 20, Loss: 2.6617913422768473\n",
      "Epoch: 153, Batch: 40, Loss: 3.0204829722080695\n",
      "Epoch: 153, Mean Loss: 4.464973586832983\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 154, Batch: 0, Loss: 5.74033865647302\n",
      "Epoch: 154, Batch: 20, Loss: 2.7953158345968627\n",
      "Epoch: 154, Batch: 40, Loss: 2.9864517905305408\n",
      "Epoch: 154, Mean Loss: 4.410348685502242\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 155, Batch: 0, Loss: 5.759308029598765\n",
      "Epoch: 155, Batch: 20, Loss: 2.786356784999649\n",
      "Epoch: 155, Batch: 40, Loss: 2.9045661807409275\n",
      "Epoch: 155, Mean Loss: 4.45032815494681\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 156, Batch: 0, Loss: 5.755627789444017\n",
      "Epoch: 156, Batch: 20, Loss: 2.7999453694867436\n",
      "Epoch: 156, Batch: 40, Loss: 2.8969990892023065\n",
      "Epoch: 156, Mean Loss: 4.416269819016716\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 157, Batch: 0, Loss: 5.662256420931253\n",
      "Epoch: 157, Batch: 20, Loss: 2.654843797900143\n",
      "Epoch: 157, Batch: 40, Loss: 2.946128817547781\n",
      "Epoch: 157, Mean Loss: 4.438220729002721\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 158, Batch: 0, Loss: 5.694015889733334\n",
      "Epoch: 158, Batch: 20, Loss: 2.7710999203593127\n",
      "Epoch: 158, Batch: 40, Loss: 2.9030079274564833\n",
      "Epoch: 158, Mean Loss: 4.405561308887375\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 159, Batch: 0, Loss: 5.746577920671758\n",
      "Epoch: 159, Batch: 20, Loss: 2.7491986284041343\n",
      "Epoch: 159, Batch: 40, Loss: 2.6264454685780345\n",
      "Epoch: 159, Mean Loss: 4.4592223890546485\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 160, Batch: 0, Loss: 5.70126430128665\n",
      "Epoch: 160, Batch: 20, Loss: 2.997560068926774\n",
      "Epoch: 160, Batch: 40, Loss: 2.804957812876954\n",
      "Epoch: 160, Mean Loss: 4.465717782572059\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 161, Batch: 0, Loss: 5.50663893990939\n",
      "Epoch: 161, Batch: 20, Loss: 2.9853147514382865\n",
      "Epoch: 161, Batch: 40, Loss: 2.7905969799382735\n",
      "Epoch: 161, Mean Loss: 4.497187828284825\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 162, Batch: 0, Loss: 5.658719522981087\n",
      "Epoch: 162, Batch: 20, Loss: 2.8427802245464195\n",
      "Epoch: 162, Batch: 40, Loss: 2.872743725539483\n",
      "Epoch: 162, Mean Loss: 4.41446561281134\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 163, Batch: 0, Loss: 5.689131869473505\n",
      "Epoch: 163, Batch: 20, Loss: 2.7520934269785045\n",
      "Epoch: 163, Batch: 40, Loss: 2.8033092914371514\n",
      "Epoch: 163, Mean Loss: 4.386007683221171\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 164, Batch: 0, Loss: 5.7712232621697925\n",
      "Epoch: 164, Batch: 20, Loss: 2.8392204783921353\n",
      "Epoch: 164, Batch: 40, Loss: 2.7919321573611717\n",
      "Epoch: 164, Mean Loss: 4.378325991553346\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 165, Batch: 0, Loss: 5.531899516907546\n",
      "Epoch: 165, Batch: 20, Loss: 2.866663204306459\n",
      "Epoch: 165, Batch: 40, Loss: 2.8051910573451124\n",
      "Epoch: 165, Mean Loss: 4.338697933514385\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 166, Batch: 0, Loss: 5.5954593108582795\n",
      "Epoch: 166, Batch: 20, Loss: 2.7518758535079684\n",
      "Epoch: 166, Batch: 40, Loss: 2.909902426222274\n",
      "Epoch: 166, Mean Loss: 4.34336890089533\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 167, Batch: 0, Loss: 5.6146369959741245\n",
      "Epoch: 167, Batch: 20, Loss: 2.739954646614105\n",
      "Epoch: 167, Batch: 40, Loss: 2.720998059375101\n",
      "Epoch: 167, Mean Loss: 4.330425891740736\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 168, Batch: 0, Loss: 5.584388056582561\n",
      "Epoch: 168, Batch: 20, Loss: 2.8832162715391583\n",
      "Epoch: 168, Batch: 40, Loss: 2.799769147147161\n",
      "Epoch: 168, Mean Loss: 4.37681924111436\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 169, Batch: 0, Loss: 5.484640162241456\n",
      "Epoch: 169, Batch: 20, Loss: 2.998169126978277\n",
      "Epoch: 169, Batch: 40, Loss: 2.64676011437508\n",
      "Epoch: 169, Mean Loss: 4.384282104545115\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 170, Batch: 0, Loss: 5.652884973079162\n",
      "Epoch: 170, Batch: 20, Loss: 2.9771169827755166\n",
      "Epoch: 170, Batch: 40, Loss: 2.8287579649584673\n",
      "Epoch: 170, Mean Loss: 4.373841317659333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 171, Batch: 0, Loss: 5.574927126951157\n",
      "Epoch: 171, Batch: 20, Loss: 2.8325486281588756\n",
      "Epoch: 171, Batch: 40, Loss: 2.858676579343177\n",
      "Epoch: 171, Mean Loss: 4.333778687060447\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 172, Batch: 0, Loss: 5.648388987558657\n",
      "Epoch: 172, Batch: 20, Loss: 2.945251818177463\n",
      "Epoch: 172, Batch: 40, Loss: 2.715845679708609\n",
      "Epoch: 172, Mean Loss: 4.378704061273754\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 173, Batch: 0, Loss: 5.662209718275676\n",
      "Epoch: 173, Batch: 20, Loss: 2.808860141061888\n",
      "Epoch: 173, Batch: 40, Loss: 2.8256586603959826\n",
      "Epoch: 173, Mean Loss: 4.324714730527766\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 174, Batch: 0, Loss: 5.607930893920588\n",
      "Epoch: 174, Batch: 20, Loss: 2.7834948144765352\n",
      "Epoch: 174, Batch: 40, Loss: 2.9337998197797037\n",
      "Epoch: 174, Mean Loss: 4.343972174801505\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 175, Batch: 0, Loss: 5.566114211534428\n",
      "Epoch: 175, Batch: 20, Loss: 2.793080115489078\n",
      "Epoch: 175, Batch: 40, Loss: 2.8795090241758627\n",
      "Epoch: 175, Mean Loss: 4.315507188675532\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 176, Batch: 0, Loss: 5.680292447894597\n",
      "Epoch: 176, Batch: 20, Loss: 2.7872299685396533\n",
      "Epoch: 176, Batch: 40, Loss: 2.866189196608272\n",
      "Epoch: 176, Mean Loss: 4.32536727885877\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 177, Batch: 0, Loss: 5.543035617768896\n",
      "Epoch: 177, Batch: 20, Loss: 2.8856127111969823\n",
      "Epoch: 177, Batch: 40, Loss: 2.8314549145887753\n",
      "Epoch: 177, Mean Loss: 4.336861224172049\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 178, Batch: 0, Loss: 5.543032943545583\n",
      "Epoch: 178, Batch: 20, Loss: 2.9048639936777265\n",
      "Epoch: 178, Batch: 40, Loss: 2.989102347872961\n",
      "Epoch: 178, Mean Loss: 4.381749825059108\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 179, Batch: 0, Loss: 5.473408993444008\n",
      "Epoch: 179, Batch: 20, Loss: 2.8258060449269458\n",
      "Epoch: 179, Batch: 40, Loss: 2.867826539469901\n",
      "Epoch: 179, Mean Loss: 4.292794284244276\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 180, Batch: 0, Loss: 5.540019788939529\n",
      "Epoch: 180, Batch: 20, Loss: 2.892174824118859\n",
      "Epoch: 180, Batch: 40, Loss: 2.915217132325244\n",
      "Epoch: 180, Mean Loss: 4.3141551259602755\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 181, Batch: 0, Loss: 5.467602656219329\n",
      "Epoch: 181, Batch: 20, Loss: 2.8630694083740846\n",
      "Epoch: 181, Batch: 40, Loss: 2.9704054519628293\n",
      "Epoch: 181, Mean Loss: 4.284550119821746\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 182, Batch: 0, Loss: 5.4597488759564765\n",
      "Epoch: 182, Batch: 20, Loss: 2.872740075465106\n",
      "Epoch: 182, Batch: 40, Loss: 2.99421686726149\n",
      "Epoch: 182, Mean Loss: 4.264990037020492\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 183, Batch: 0, Loss: 5.3226021055557196\n",
      "Epoch: 183, Batch: 20, Loss: 2.962082389423478\n",
      "Epoch: 183, Batch: 40, Loss: 2.929370241987373\n",
      "Epoch: 183, Mean Loss: 4.267690761843843\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 184, Batch: 0, Loss: 5.339925414404002\n",
      "Epoch: 184, Batch: 20, Loss: 2.985206183485484\n",
      "Epoch: 184, Batch: 40, Loss: 2.9258028653682855\n",
      "Epoch: 184, Mean Loss: 4.302933455915278\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 185, Batch: 0, Loss: 5.183295559846545\n",
      "Epoch: 185, Batch: 20, Loss: 3.3666530518005007\n",
      "Epoch: 185, Batch: 40, Loss: 2.7577174121950114\n",
      "Epoch: 185, Mean Loss: 4.3415134746645085\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 186, Batch: 0, Loss: 5.16889402724158\n",
      "Epoch: 186, Batch: 20, Loss: 3.2695758045746315\n",
      "Epoch: 186, Batch: 40, Loss: 2.8355097262858884\n",
      "Epoch: 186, Mean Loss: 4.309397044380961\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 187, Batch: 0, Loss: 5.204832778407992\n",
      "Epoch: 187, Batch: 20, Loss: 2.95690125807168\n",
      "Epoch: 187, Batch: 40, Loss: 3.027251802059403\n",
      "Epoch: 187, Mean Loss: 4.319990178609697\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 188, Batch: 0, Loss: 5.4735647625833765\n",
      "Epoch: 188, Batch: 20, Loss: 3.110438829405501\n",
      "Epoch: 188, Batch: 40, Loss: 2.785711360355463\n",
      "Epoch: 188, Mean Loss: 4.333871776131106\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 189, Batch: 0, Loss: 5.317721298733624\n",
      "Epoch: 189, Batch: 20, Loss: 3.412025057924314\n",
      "Epoch: 189, Batch: 40, Loss: 2.7988223903442218\n",
      "Epoch: 189, Mean Loss: 4.348559004972981\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 190, Batch: 0, Loss: 5.348261602891235\n",
      "Epoch: 190, Batch: 20, Loss: 3.1575737588170356\n",
      "Epoch: 190, Batch: 40, Loss: 2.929879651571254\n",
      "Epoch: 190, Mean Loss: 4.2867729961690335\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 191, Batch: 0, Loss: 5.518983090983573\n",
      "Epoch: 191, Batch: 20, Loss: 3.098176889154739\n",
      "Epoch: 191, Batch: 40, Loss: 3.086206331511915\n",
      "Epoch: 191, Mean Loss: 4.345394899406195\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 192, Batch: 0, Loss: 5.163744995262185\n",
      "Epoch: 192, Batch: 20, Loss: 2.753260598308514\n",
      "Epoch: 192, Batch: 40, Loss: 2.932929751518548\n",
      "Epoch: 192, Mean Loss: 4.255591696974695\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 193, Batch: 0, Loss: 5.423297898417106\n",
      "Epoch: 193, Batch: 20, Loss: 2.798043149791303\n",
      "Epoch: 193, Batch: 40, Loss: 2.8696706408050368\n",
      "Epoch: 193, Mean Loss: 4.249491208736867\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 194, Batch: 0, Loss: 5.207303925543361\n",
      "Epoch: 194, Batch: 20, Loss: 2.852093867582655\n",
      "Epoch: 194, Batch: 40, Loss: 2.955363262803869\n",
      "Epoch: 194, Mean Loss: 4.220866802640948\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 195, Batch: 0, Loss: 5.22561685151934\n",
      "Epoch: 195, Batch: 20, Loss: 2.9505417474058513\n",
      "Epoch: 195, Batch: 40, Loss: 2.9552936464290416\n",
      "Epoch: 195, Mean Loss: 4.225225164615653\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 196, Batch: 0, Loss: 5.225387296263907\n",
      "Epoch: 196, Batch: 20, Loss: 2.7766929566955234\n",
      "Epoch: 196, Batch: 40, Loss: 3.0021389981984887\n",
      "Epoch: 196, Mean Loss: 4.191179678882883\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 197, Batch: 0, Loss: 5.211638269305093\n",
      "Epoch: 197, Batch: 20, Loss: 2.828068971435656\n",
      "Epoch: 197, Batch: 40, Loss: 3.018069107715023\n",
      "Epoch: 197, Mean Loss: 4.194150773611035\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 198, Batch: 0, Loss: 5.1706753165800174\n",
      "Epoch: 198, Batch: 20, Loss: 2.7353632226010594\n",
      "Epoch: 198, Batch: 40, Loss: 2.9046684134224563\n",
      "Epoch: 198, Mean Loss: 4.20360399345106\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 199, Batch: 0, Loss: 5.086239055226404\n",
      "Epoch: 199, Batch: 20, Loss: 2.943860455893323\n",
      "Epoch: 199, Batch: 40, Loss: 2.8122485166746958\n",
      "Epoch: 199, Mean Loss: 4.270337504524502\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 200, Batch: 0, Loss: 5.158069899568093\n",
      "Epoch: 200, Batch: 20, Loss: 2.9736640258554345\n",
      "Epoch: 200, Batch: 40, Loss: 3.116595181450611\n",
      "Epoch: 200, Mean Loss: 4.2183120941131085\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 201, Batch: 0, Loss: 4.949087697158539\n",
      "Epoch: 201, Batch: 20, Loss: 3.2283246936906873\n",
      "Epoch: 201, Batch: 40, Loss: 2.7838011099059456\n",
      "Epoch: 201, Mean Loss: 4.258719092945366\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 202, Batch: 0, Loss: 5.036339456654623\n",
      "Epoch: 202, Batch: 20, Loss: 2.7849619302954216\n",
      "Epoch: 202, Batch: 40, Loss: 3.007331642724818\n",
      "Epoch: 202, Mean Loss: 4.185690077451008\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 203, Batch: 0, Loss: 5.2663904292390065\n",
      "Epoch: 203, Batch: 20, Loss: 3.0474363812294025\n",
      "Epoch: 203, Batch: 40, Loss: 2.857517285701727\n",
      "Epoch: 203, Mean Loss: 4.240640389300234\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 204, Batch: 0, Loss: 4.99585147011357\n",
      "Epoch: 204, Batch: 20, Loss: 2.846746675149047\n",
      "Epoch: 204, Batch: 40, Loss: 3.1092102272329845\n",
      "Epoch: 204, Mean Loss: 4.280416278340729\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 205, Batch: 0, Loss: 5.2379069056780025\n",
      "Epoch: 205, Batch: 20, Loss: 3.0225577652433837\n",
      "Epoch: 205, Batch: 40, Loss: 2.920495251963667\n",
      "Epoch: 205, Mean Loss: 4.262800442310981\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 206, Batch: 0, Loss: 5.301271322780941\n",
      "Epoch: 206, Batch: 20, Loss: 2.952059042877814\n",
      "Epoch: 206, Batch: 40, Loss: 2.8068331748666533\n",
      "Epoch: 206, Mean Loss: 4.188602004699822\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 207, Batch: 0, Loss: 5.202484437740873\n",
      "Epoch: 207, Batch: 20, Loss: 3.1591282355981223\n",
      "Epoch: 207, Batch: 40, Loss: 2.945198674043562\n",
      "Epoch: 207, Mean Loss: 4.199498973128155\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 208, Batch: 0, Loss: 5.244057394983597\n",
      "Epoch: 208, Batch: 20, Loss: 2.9539795803807207\n",
      "Epoch: 208, Batch: 40, Loss: 3.0539423775493018\n",
      "Epoch: 208, Mean Loss: 4.159275190723667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 209, Batch: 0, Loss: 5.149261389553916\n",
      "Epoch: 209, Batch: 20, Loss: 2.657878798467702\n",
      "Epoch: 209, Batch: 40, Loss: 2.996322852904945\n",
      "Epoch: 209, Mean Loss: 4.104168160920976\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 210, Batch: 0, Loss: 5.099952193191646\n",
      "Epoch: 210, Batch: 20, Loss: 2.8281814463141504\n",
      "Epoch: 210, Batch: 40, Loss: 2.972518514412108\n",
      "Epoch: 210, Mean Loss: 4.07631345388457\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 211, Batch: 0, Loss: 5.0926677549956585\n",
      "Epoch: 211, Batch: 20, Loss: 2.748737731143531\n",
      "Epoch: 211, Batch: 40, Loss: 2.960379713376033\n",
      "Epoch: 211, Mean Loss: 4.088894048684148\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 212, Batch: 0, Loss: 5.126988578986021\n",
      "Epoch: 212, Batch: 20, Loss: 3.088860314074378\n",
      "Epoch: 212, Batch: 40, Loss: 2.96541472721385\n",
      "Epoch: 212, Mean Loss: 4.137173106437223\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 213, Batch: 0, Loss: 5.060048347955094\n",
      "Epoch: 213, Batch: 20, Loss: 2.824824609344346\n",
      "Epoch: 213, Batch: 40, Loss: 3.0924358784363215\n",
      "Epoch: 213, Mean Loss: 4.098934450139695\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 214, Batch: 0, Loss: 5.0543585849578045\n",
      "Epoch: 214, Batch: 20, Loss: 2.782115554977697\n",
      "Epoch: 214, Batch: 40, Loss: 3.0329826804094924\n",
      "Epoch: 214, Mean Loss: 4.0775241108054425\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 215, Batch: 0, Loss: 5.035769463677594\n",
      "Epoch: 215, Batch: 20, Loss: 2.8417934422962428\n",
      "Epoch: 215, Batch: 40, Loss: 2.9090199647145387\n",
      "Epoch: 215, Mean Loss: 4.13723063386645\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 216, Batch: 0, Loss: 4.855697713315888\n",
      "Epoch: 216, Batch: 20, Loss: 2.8979250498468327\n",
      "Epoch: 216, Batch: 40, Loss: 3.6945020098209893\n",
      "Epoch: 216, Mean Loss: 4.264360466537005\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 217, Batch: 0, Loss: 5.429633684163969\n",
      "Epoch: 217, Batch: 20, Loss: 3.0136492369723116\n",
      "Epoch: 217, Batch: 40, Loss: 2.9327854795401036\n",
      "Epoch: 217, Mean Loss: 4.270599568237039\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 218, Batch: 0, Loss: 5.196070242174373\n",
      "Epoch: 218, Batch: 20, Loss: 3.1895797185285764\n",
      "Epoch: 218, Batch: 40, Loss: 2.9666134617623907\n",
      "Epoch: 218, Mean Loss: 4.223864078795704\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 219, Batch: 0, Loss: 4.789655706078333\n",
      "Epoch: 219, Batch: 20, Loss: 2.7925501081336668\n",
      "Epoch: 219, Batch: 40, Loss: 2.98647747930665\n",
      "Epoch: 219, Mean Loss: 4.132483617802441\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 220, Batch: 0, Loss: 4.97407335044099\n",
      "Epoch: 220, Batch: 20, Loss: 2.9197662417078787\n",
      "Epoch: 220, Batch: 40, Loss: 2.9328508020553152\n",
      "Epoch: 220, Mean Loss: 4.1245916629498245\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 221, Batch: 0, Loss: 4.93500506487337\n",
      "Epoch: 221, Batch: 20, Loss: 2.8672028919704373\n",
      "Epoch: 221, Batch: 40, Loss: 2.9065115449417394\n",
      "Epoch: 221, Mean Loss: 4.080278377174777\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 222, Batch: 0, Loss: 5.049016543612321\n",
      "Epoch: 222, Batch: 20, Loss: 3.0973975357333807\n",
      "Epoch: 222, Batch: 40, Loss: 2.8791978018686573\n",
      "Epoch: 222, Mean Loss: 4.15013167970681\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 223, Batch: 0, Loss: 5.051023006424592\n",
      "Epoch: 223, Batch: 20, Loss: 2.7875396137194066\n",
      "Epoch: 223, Batch: 40, Loss: 3.064959917092405\n",
      "Epoch: 223, Mean Loss: 4.111057970860227\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 224, Batch: 0, Loss: 5.007469323945884\n",
      "Epoch: 224, Batch: 20, Loss: 2.664867210491732\n",
      "Epoch: 224, Batch: 40, Loss: 3.2092514647402965\n",
      "Epoch: 224, Mean Loss: 4.24566072007907\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 225, Batch: 0, Loss: 4.832011575771714\n",
      "Epoch: 225, Batch: 20, Loss: 2.8377022029105934\n",
      "Epoch: 225, Batch: 40, Loss: 3.1310096838263046\n",
      "Epoch: 225, Mean Loss: 4.073109793232753\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 226, Batch: 0, Loss: 4.9328080185246\n",
      "Epoch: 226, Batch: 20, Loss: 2.7133998225695946\n",
      "Epoch: 226, Batch: 40, Loss: 3.1280427470037724\n",
      "Epoch: 226, Mean Loss: 4.128258997745513\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 227, Batch: 0, Loss: 4.871990769158863\n",
      "Epoch: 227, Batch: 20, Loss: 2.9308411484831725\n",
      "Epoch: 227, Batch: 40, Loss: 2.969185214214495\n",
      "Epoch: 227, Mean Loss: 4.102420177715805\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 228, Batch: 0, Loss: 4.720039035259484\n",
      "Epoch: 228, Batch: 20, Loss: 2.958636798647263\n",
      "Epoch: 228, Batch: 40, Loss: 2.7296942922078595\n",
      "Epoch: 228, Mean Loss: 4.109318272118032\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 229, Batch: 0, Loss: 5.081606792247225\n",
      "Epoch: 229, Batch: 20, Loss: 2.9484852287718657\n",
      "Epoch: 229, Batch: 40, Loss: 2.91410272185683\n",
      "Epoch: 229, Mean Loss: 4.099476026352213\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 230, Batch: 0, Loss: 5.162999473827529\n",
      "Epoch: 230, Batch: 20, Loss: 2.9086141781845485\n",
      "Epoch: 230, Batch: 40, Loss: 2.7522242401606047\n",
      "Epoch: 230, Mean Loss: 4.104172240738495\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 231, Batch: 0, Loss: 4.929986632789166\n",
      "Epoch: 231, Batch: 20, Loss: 2.6718408426837534\n",
      "Epoch: 231, Batch: 40, Loss: 3.2779569623184828\n",
      "Epoch: 231, Mean Loss: 4.187898378527507\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 232, Batch: 0, Loss: 5.169106170539819\n",
      "Epoch: 232, Batch: 20, Loss: 3.3308237872841433\n",
      "Epoch: 232, Batch: 40, Loss: 2.813833593553136\n",
      "Epoch: 232, Mean Loss: 4.245895967225587\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 233, Batch: 0, Loss: 4.9744869385930865\n",
      "Epoch: 233, Batch: 20, Loss: 3.018472873739685\n",
      "Epoch: 233, Batch: 40, Loss: 3.2182505720503354\n",
      "Epoch: 233, Mean Loss: 4.185113287456327\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 234, Batch: 0, Loss: 5.143460848200213\n",
      "Epoch: 234, Batch: 20, Loss: 2.6010553648034005\n",
      "Epoch: 234, Batch: 40, Loss: 3.625134779498507\n",
      "Epoch: 234, Mean Loss: 4.2462866728457245\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 235, Batch: 0, Loss: 4.281716015604172\n",
      "Epoch: 235, Batch: 20, Loss: 2.780640680180533\n",
      "Epoch: 235, Batch: 40, Loss: 3.0244451283275486\n",
      "Epoch: 235, Mean Loss: 4.108096591486247\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 236, Batch: 0, Loss: 4.6848661359354855\n",
      "Epoch: 236, Batch: 20, Loss: 2.980761186568704\n",
      "Epoch: 236, Batch: 40, Loss: 2.9186328999951394\n",
      "Epoch: 236, Mean Loss: 4.025340852150394\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 237, Batch: 0, Loss: 4.543046493177408\n",
      "Epoch: 237, Batch: 20, Loss: 2.7921576108758335\n",
      "Epoch: 237, Batch: 40, Loss: 2.955846284007889\n",
      "Epoch: 237, Mean Loss: 4.045545074174344\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 238, Batch: 0, Loss: 4.791490095637691\n",
      "Epoch: 238, Batch: 20, Loss: 2.811272333349809\n",
      "Epoch: 238, Batch: 40, Loss: 2.9739441798928574\n",
      "Epoch: 238, Mean Loss: 3.9834490893970047\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 239, Batch: 0, Loss: 4.806802442874584\n",
      "Epoch: 239, Batch: 20, Loss: 2.7043792670188664\n",
      "Epoch: 239, Batch: 40, Loss: 2.9437301213137803\n",
      "Epoch: 239, Mean Loss: 3.9895643560279335\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 240, Batch: 0, Loss: 4.643359323778081\n",
      "Epoch: 240, Batch: 20, Loss: 2.6659259165655507\n",
      "Epoch: 240, Batch: 40, Loss: 3.0190051225538923\n",
      "Epoch: 240, Mean Loss: 3.9103895921069505\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 241, Batch: 0, Loss: 4.718433467830612\n",
      "Epoch: 241, Batch: 20, Loss: 2.7209116437705863\n",
      "Epoch: 241, Batch: 40, Loss: 2.9700218094469406\n",
      "Epoch: 241, Mean Loss: 3.9452125801994367\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 242, Batch: 0, Loss: 4.679627260039039\n",
      "Epoch: 242, Batch: 20, Loss: 2.7068998542450875\n",
      "Epoch: 242, Batch: 40, Loss: 3.135216150523669\n",
      "Epoch: 242, Mean Loss: 3.978250982768032\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 243, Batch: 0, Loss: 4.617871392507006\n",
      "Epoch: 243, Batch: 20, Loss: 2.6213512224983644\n",
      "Epoch: 243, Batch: 40, Loss: 2.959227769060953\n",
      "Epoch: 243, Mean Loss: 3.952751662788677\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 244, Batch: 0, Loss: 4.6747626260624235\n",
      "Epoch: 244, Batch: 20, Loss: 2.9128188226228815\n",
      "Epoch: 244, Batch: 40, Loss: 2.9894985093521997\n",
      "Epoch: 244, Mean Loss: 4.060209468754296\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 245, Batch: 0, Loss: 4.475462882533462\n",
      "Epoch: 245, Batch: 20, Loss: 2.7822817387935213\n",
      "Epoch: 245, Batch: 40, Loss: 3.0910851857078936\n",
      "Epoch: 245, Mean Loss: 4.0295705119121115\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 246, Batch: 0, Loss: 4.8300511903495105\n",
      "Epoch: 246, Batch: 20, Loss: 3.060736557871263\n",
      "Epoch: 246, Batch: 40, Loss: 2.81255782613611\n",
      "Epoch: 246, Mean Loss: 4.052480948179069\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 247, Batch: 0, Loss: 4.489628838965085\n",
      "Epoch: 247, Batch: 20, Loss: 3.0332782586544393\n",
      "Epoch: 247, Batch: 40, Loss: 2.84992335531355\n",
      "Epoch: 247, Mean Loss: 4.0041548796584445\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 248, Batch: 0, Loss: 4.702184720820147\n",
      "Epoch: 248, Batch: 20, Loss: 2.8187259392006445\n",
      "Epoch: 248, Batch: 40, Loss: 2.9321040751666585\n",
      "Epoch: 248, Mean Loss: 3.9034152835142493\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 249, Batch: 0, Loss: 4.693291199163716\n",
      "Epoch: 249, Batch: 20, Loss: 3.1179748603089767\n",
      "Epoch: 249, Batch: 40, Loss: 2.8016418758984987\n",
      "Epoch: 249, Mean Loss: 3.9801089530421065\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 250, Batch: 0, Loss: 4.611232324301182\n",
      "Epoch: 250, Batch: 20, Loss: 2.9245072839739494\n",
      "Epoch: 250, Batch: 40, Loss: 2.8123681771420483\n",
      "Epoch: 250, Mean Loss: 3.903347075889007\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 251, Batch: 0, Loss: 4.582300434329548\n",
      "Epoch: 251, Batch: 20, Loss: 2.9628910608427623\n",
      "Epoch: 251, Batch: 40, Loss: 2.9087244968693877\n",
      "Epoch: 251, Mean Loss: 3.9458669139958595\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 252, Batch: 0, Loss: 4.4875882474408915\n",
      "Epoch: 252, Batch: 20, Loss: 2.8119042854974383\n",
      "Epoch: 252, Batch: 40, Loss: 2.8732894914612643\n",
      "Epoch: 252, Mean Loss: 3.894289184268442\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 253, Batch: 0, Loss: 4.62667983354659\n",
      "Epoch: 253, Batch: 20, Loss: 2.684707186231468\n",
      "Epoch: 253, Batch: 40, Loss: 2.7870900966850187\n",
      "Epoch: 253, Mean Loss: 3.913233796708574\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 254, Batch: 0, Loss: 4.513692346592715\n",
      "Epoch: 254, Batch: 20, Loss: 2.9232940229055906\n",
      "Epoch: 254, Batch: 40, Loss: 2.774614295037396\n",
      "Epoch: 254, Mean Loss: 3.8917085005269128\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 255, Batch: 0, Loss: 4.67243816305043\n",
      "Epoch: 255, Batch: 20, Loss: 2.8603071734515493\n",
      "Epoch: 255, Batch: 40, Loss: 2.8666138753304433\n",
      "Epoch: 255, Mean Loss: 3.8959292350142603\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 256, Batch: 0, Loss: 4.54471131667145\n",
      "Epoch: 256, Batch: 20, Loss: 2.566713967524464\n",
      "Epoch: 256, Batch: 40, Loss: 3.1008520601102987\n",
      "Epoch: 256, Mean Loss: 3.9160661945334754\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 257, Batch: 0, Loss: 4.473396678199388\n",
      "Epoch: 257, Batch: 20, Loss: 2.4716517437261984\n",
      "Epoch: 257, Batch: 40, Loss: 3.367394414222774\n",
      "Epoch: 257, Mean Loss: 3.928885086340348\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 258, Batch: 0, Loss: 4.516480296924341\n",
      "Epoch: 258, Batch: 20, Loss: 2.7785601437188423\n",
      "Epoch: 258, Batch: 40, Loss: 2.86533011739341\n",
      "Epoch: 258, Mean Loss: 3.902117981432078\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 259, Batch: 0, Loss: 4.501006487383423\n",
      "Epoch: 259, Batch: 20, Loss: 3.3091993667359616\n",
      "Epoch: 259, Batch: 40, Loss: 2.8807067947387814\n",
      "Epoch: 259, Mean Loss: 4.075334399391284\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 260, Batch: 0, Loss: 4.712490222855724\n",
      "Epoch: 260, Batch: 20, Loss: 2.945455724791155\n",
      "Epoch: 260, Batch: 40, Loss: 3.1063656001518436\n",
      "Epoch: 260, Mean Loss: 4.072992277789401\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 261, Batch: 0, Loss: 4.8325814703640395\n",
      "Epoch: 261, Batch: 20, Loss: 2.8858710911537546\n",
      "Epoch: 261, Batch: 40, Loss: 2.6637523830124223\n",
      "Epoch: 261, Mean Loss: 4.0996945928969835\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 262, Batch: 0, Loss: 4.617348973525312\n",
      "Epoch: 262, Batch: 20, Loss: 2.7841158815628084\n",
      "Epoch: 262, Batch: 40, Loss: 2.660050647326453\n",
      "Epoch: 262, Mean Loss: 4.235980171311505\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 263, Batch: 0, Loss: 4.767367468771712\n",
      "Epoch: 263, Batch: 20, Loss: 3.885744956336924\n",
      "Epoch: 263, Batch: 40, Loss: 2.6093245163020797\n",
      "Epoch: 263, Mean Loss: 4.270916116233898\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 264, Batch: 0, Loss: 5.362820175260181\n",
      "Epoch: 264, Batch: 20, Loss: 2.9601659246560805\n",
      "Epoch: 264, Batch: 40, Loss: 3.5503863870457812\n",
      "Epoch: 264, Mean Loss: 4.866543278368131\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 265, Batch: 0, Loss: 3.764765792234715\n",
      "Epoch: 265, Batch: 20, Loss: 2.994191978098017\n",
      "Epoch: 265, Batch: 40, Loss: 3.3237813563806125\n",
      "Epoch: 265, Mean Loss: 4.989415240264812\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 266, Batch: 0, Loss: 4.057027860114634\n",
      "Epoch: 266, Batch: 20, Loss: 2.784469543182135\n",
      "Epoch: 266, Batch: 40, Loss: 2.6860815053186933\n",
      "Epoch: 266, Mean Loss: 4.49305577154353\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 267, Batch: 0, Loss: 4.486743719385458\n",
      "Epoch: 267, Batch: 20, Loss: 2.8170889047903342\n",
      "Epoch: 267, Batch: 40, Loss: 3.2611312115846363\n",
      "Epoch: 267, Mean Loss: 4.230766513006489\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 268, Batch: 0, Loss: 4.616336838640673\n",
      "Epoch: 268, Batch: 20, Loss: 2.6049495605236856\n",
      "Epoch: 268, Batch: 40, Loss: 2.98060082554871\n",
      "Epoch: 268, Mean Loss: 3.9860001902642574\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 269, Batch: 0, Loss: 4.6765620464833475\n",
      "Epoch: 269, Batch: 20, Loss: 2.6619011990289274\n",
      "Epoch: 269, Batch: 40, Loss: 3.1422615595471037\n",
      "Epoch: 269, Mean Loss: 3.8558251914853106\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 270, Batch: 0, Loss: 4.400020431743472\n",
      "Epoch: 270, Batch: 20, Loss: 2.6645011101186644\n",
      "Epoch: 270, Batch: 40, Loss: 2.9966020102965887\n",
      "Epoch: 270, Mean Loss: 3.8066620154977175\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 271, Batch: 0, Loss: 4.549943123095627\n",
      "Epoch: 271, Batch: 20, Loss: 2.421378535364038\n",
      "Epoch: 271, Batch: 40, Loss: 3.0163812232610008\n",
      "Epoch: 271, Mean Loss: 3.7949241757262326\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 272, Batch: 0, Loss: 4.425252608138758\n",
      "Epoch: 272, Batch: 20, Loss: 2.805882125100922\n",
      "Epoch: 272, Batch: 40, Loss: 2.745905080654487\n",
      "Epoch: 272, Mean Loss: 3.825188928233912\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 273, Batch: 0, Loss: 4.691456797714479\n",
      "Epoch: 273, Batch: 20, Loss: 2.5826624628687314\n",
      "Epoch: 273, Batch: 40, Loss: 2.8117452927286584\n",
      "Epoch: 273, Mean Loss: 3.7984942572572935\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 274, Batch: 0, Loss: 4.52655029046152\n",
      "Epoch: 274, Batch: 20, Loss: 2.5822692371199674\n",
      "Epoch: 274, Batch: 40, Loss: 2.789794914472325\n",
      "Epoch: 274, Mean Loss: 3.7608577933728635\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 275, Batch: 0, Loss: 4.604472425613098\n",
      "Epoch: 275, Batch: 20, Loss: 2.4762588426724568\n",
      "Epoch: 275, Batch: 40, Loss: 2.895157944248376\n",
      "Epoch: 275, Mean Loss: 3.771499837683807\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 276, Batch: 0, Loss: 4.334482780785653\n",
      "Epoch: 276, Batch: 20, Loss: 2.4243113653578368\n",
      "Epoch: 276, Batch: 40, Loss: 2.927498967045978\n",
      "Epoch: 276, Mean Loss: 3.7563415997787786\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 277, Batch: 0, Loss: 4.333155675449054\n",
      "Epoch: 277, Batch: 20, Loss: 2.564497905917408\n",
      "Epoch: 277, Batch: 40, Loss: 2.8237771897536015\n",
      "Epoch: 277, Mean Loss: 3.7571336852764596\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 278, Batch: 0, Loss: 4.345498962684338\n",
      "Epoch: 278, Batch: 20, Loss: 2.5362381226429656\n",
      "Epoch: 278, Batch: 40, Loss: 2.913704106679427\n",
      "Epoch: 278, Mean Loss: 3.777882258500629\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 279, Batch: 0, Loss: 4.333533728053788\n",
      "Epoch: 279, Batch: 20, Loss: 2.824158455917554\n",
      "Epoch: 279, Batch: 40, Loss: 2.7596857768382606\n",
      "Epoch: 279, Mean Loss: 3.8006834467607797\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 280, Batch: 0, Loss: 4.429199378020282\n",
      "Epoch: 280, Batch: 20, Loss: 2.7374798874637216\n",
      "Epoch: 280, Batch: 40, Loss: 2.770748397016382\n",
      "Epoch: 280, Mean Loss: 3.783285178463309\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 281, Batch: 0, Loss: 4.52957532007912\n",
      "Epoch: 281, Batch: 20, Loss: 2.654141306926656\n",
      "Epoch: 281, Batch: 40, Loss: 2.799639909493248\n",
      "Epoch: 281, Mean Loss: 3.7549989239016304\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 282, Batch: 0, Loss: 4.484724465356511\n",
      "Epoch: 282, Batch: 20, Loss: 2.629757551025595\n",
      "Epoch: 282, Batch: 40, Loss: 2.670652566767456\n",
      "Epoch: 282, Mean Loss: 3.7786428949155497\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 283, Batch: 0, Loss: 4.259670127432381\n",
      "Epoch: 283, Batch: 20, Loss: 2.67477784464395\n",
      "Epoch: 283, Batch: 40, Loss: 2.71528095619536\n",
      "Epoch: 283, Mean Loss: 3.7674581119056993\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 284, Batch: 0, Loss: 4.3114364205722815\n",
      "Epoch: 284, Batch: 20, Loss: 2.880268521209328\n",
      "Epoch: 284, Batch: 40, Loss: 2.5964466021718526\n",
      "Epoch: 284, Mean Loss: 3.8101111670913252\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 285, Batch: 0, Loss: 4.510054796465744\n",
      "Epoch: 285, Batch: 20, Loss: 2.838104299507051\n",
      "Epoch: 285, Batch: 40, Loss: 2.647680410692178\n",
      "Epoch: 285, Mean Loss: 3.8031651729576232\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 286, Batch: 0, Loss: 4.475046289074172\n",
      "Epoch: 286, Batch: 20, Loss: 2.6494454584330946\n",
      "Epoch: 286, Batch: 40, Loss: 2.7580348474757495\n",
      "Epoch: 286, Mean Loss: 3.7570236208230936\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 287, Batch: 0, Loss: 4.3877090919554655\n",
      "Epoch: 287, Batch: 20, Loss: 2.7406437836814943\n",
      "Epoch: 287, Batch: 40, Loss: 2.846037649230538\n",
      "Epoch: 287, Mean Loss: 3.772068919010106\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 288, Batch: 0, Loss: 4.412020343395418\n",
      "Epoch: 288, Batch: 20, Loss: 2.464114647267923\n",
      "Epoch: 288, Batch: 40, Loss: 2.8662390449891078\n",
      "Epoch: 288, Mean Loss: 3.7474550373691566\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 289, Batch: 0, Loss: 4.239486931974107\n",
      "Epoch: 289, Batch: 20, Loss: 2.5683567032467893\n",
      "Epoch: 289, Batch: 40, Loss: 2.75420547174646\n",
      "Epoch: 289, Mean Loss: 3.7323915193603336\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 290, Batch: 0, Loss: 4.177004709355959\n",
      "Epoch: 290, Batch: 20, Loss: 2.632280586853431\n",
      "Epoch: 290, Batch: 40, Loss: 2.766729437315633\n",
      "Epoch: 290, Mean Loss: 3.770889586711205\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 291, Batch: 0, Loss: 4.30409723090146\n",
      "Epoch: 291, Batch: 20, Loss: 2.6080435553413825\n",
      "Epoch: 291, Batch: 40, Loss: 2.9230338139300747\n",
      "Epoch: 291, Mean Loss: 3.75203327990965\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 292, Batch: 0, Loss: 4.266954757022883\n",
      "Epoch: 292, Batch: 20, Loss: 2.5331135510543885\n",
      "Epoch: 292, Batch: 40, Loss: 2.8734018049674535\n",
      "Epoch: 292, Mean Loss: 3.7389098475830065\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 293, Batch: 0, Loss: 4.145137127594846\n",
      "Epoch: 293, Batch: 20, Loss: 2.4835056126245854\n",
      "Epoch: 293, Batch: 40, Loss: 2.9908612940723014\n",
      "Epoch: 293, Mean Loss: 3.7641553528901084\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 294, Batch: 0, Loss: 4.301108551699654\n",
      "Epoch: 294, Batch: 20, Loss: 2.827347065100779\n",
      "Epoch: 294, Batch: 40, Loss: 2.6950220150610407\n",
      "Epoch: 294, Mean Loss: 3.7758462164599655\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 295, Batch: 0, Loss: 4.289278305946378\n",
      "Epoch: 295, Batch: 20, Loss: 2.5449284814129065\n",
      "Epoch: 295, Batch: 40, Loss: 2.845646944120053\n",
      "Epoch: 295, Mean Loss: 3.7460453848964255\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 296, Batch: 0, Loss: 4.197716830262845\n",
      "Epoch: 296, Batch: 20, Loss: 2.5498164243722896\n",
      "Epoch: 296, Batch: 40, Loss: 2.9361202192928686\n",
      "Epoch: 296, Mean Loss: 3.7288390467999415\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 297, Batch: 0, Loss: 4.065979520745206\n",
      "Epoch: 297, Batch: 20, Loss: 2.601548118103918\n",
      "Epoch: 297, Batch: 40, Loss: 2.712443580060285\n",
      "Epoch: 297, Mean Loss: 3.7189007114038675\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 298, Batch: 0, Loss: 3.909463882215543\n",
      "Epoch: 298, Batch: 20, Loss: 2.5071737268666014\n",
      "Epoch: 298, Batch: 40, Loss: 2.914655365319853\n",
      "Epoch: 298, Mean Loss: 3.718591750266969\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 299, Batch: 0, Loss: 3.9858683736441565\n",
      "Epoch: 299, Batch: 20, Loss: 2.705248891287774\n",
      "Epoch: 299, Batch: 40, Loss: 2.8215948086336615\n",
      "Epoch: 299, Mean Loss: 3.7425139620163295\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 300, Batch: 0, Loss: 3.8717156836872593\n",
      "Epoch: 300, Batch: 20, Loss: 2.7174775264659026\n",
      "Epoch: 300, Batch: 40, Loss: 2.8135805326721237\n",
      "Epoch: 300, Mean Loss: 3.6874735944994375\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 301, Batch: 0, Loss: 3.993431858829545\n",
      "Epoch: 301, Batch: 20, Loss: 2.6112433888866735\n",
      "Epoch: 301, Batch: 40, Loss: 2.8458603956475805\n",
      "Epoch: 301, Mean Loss: 3.6578869287598814\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 302, Batch: 0, Loss: 3.925133171345235\n",
      "Epoch: 302, Batch: 20, Loss: 2.5534075990736333\n",
      "Epoch: 302, Batch: 40, Loss: 2.854870224794086\n",
      "Epoch: 302, Mean Loss: 3.659882133477223\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 303, Batch: 0, Loss: 3.8617588422446416\n",
      "Epoch: 303, Batch: 20, Loss: 2.5351780195058526\n",
      "Epoch: 303, Batch: 40, Loss: 2.7490281816915068\n",
      "Epoch: 303, Mean Loss: 3.6524487047017713\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 304, Batch: 0, Loss: 4.031340360762504\n",
      "Epoch: 304, Batch: 20, Loss: 2.8281992482942413\n",
      "Epoch: 304, Batch: 40, Loss: 2.7714967857173582\n",
      "Epoch: 304, Mean Loss: 3.7973355411235574\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 305, Batch: 0, Loss: 3.8025268164418655\n",
      "Epoch: 305, Batch: 20, Loss: 2.979447593697692\n",
      "Epoch: 305, Batch: 40, Loss: 2.7588897334743208\n",
      "Epoch: 305, Mean Loss: 3.7945021153754386\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 306, Batch: 0, Loss: 3.7108280713466866\n",
      "Epoch: 306, Batch: 20, Loss: 3.406761198778873\n",
      "Epoch: 306, Batch: 40, Loss: 2.7159621529196087\n",
      "Epoch: 306, Mean Loss: 3.962901026921333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 307, Batch: 0, Loss: 4.135281344379126\n",
      "Epoch: 307, Batch: 20, Loss: 3.0200361613962787\n",
      "Epoch: 307, Batch: 40, Loss: 3.114596766629483\n",
      "Epoch: 307, Mean Loss: 3.9850133121060596\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 308, Batch: 0, Loss: 4.046961863334117\n",
      "Epoch: 308, Batch: 20, Loss: 2.7348987512055336\n",
      "Epoch: 308, Batch: 40, Loss: 2.707967300976981\n",
      "Epoch: 308, Mean Loss: 3.9884989690815824\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 309, Batch: 0, Loss: 4.217950308255918\n",
      "Epoch: 309, Batch: 20, Loss: 3.6140374982136003\n",
      "Epoch: 309, Batch: 40, Loss: 2.7271576584160893\n",
      "Epoch: 309, Mean Loss: 4.121761886987899\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 310, Batch: 0, Loss: 3.8781149626927762\n",
      "Epoch: 310, Batch: 20, Loss: 2.6253124446165264\n",
      "Epoch: 310, Batch: 40, Loss: 3.706973946488812\n",
      "Epoch: 310, Mean Loss: 4.060205633411241\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 311, Batch: 0, Loss: 4.209796755865746\n",
      "Epoch: 311, Batch: 20, Loss: 2.900350932254686\n",
      "Epoch: 311, Batch: 40, Loss: 2.703833771035761\n",
      "Epoch: 311, Mean Loss: 3.8746246487853226\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 312, Batch: 0, Loss: 4.163379172518783\n",
      "Epoch: 312, Batch: 20, Loss: 2.674386826313516\n",
      "Epoch: 312, Batch: 40, Loss: 2.80673817941214\n",
      "Epoch: 312, Mean Loss: 3.905850218216521\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 313, Batch: 0, Loss: 3.787318881438699\n",
      "Epoch: 313, Batch: 20, Loss: 2.976169264917432\n",
      "Epoch: 313, Batch: 40, Loss: 2.726128185762819\n",
      "Epoch: 313, Mean Loss: 3.776272822022028\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 314, Batch: 0, Loss: 4.300990987376869\n",
      "Epoch: 314, Batch: 20, Loss: 2.7886260111654604\n",
      "Epoch: 314, Batch: 40, Loss: 2.97297278772282\n",
      "Epoch: 314, Mean Loss: 3.776958126657827\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 315, Batch: 0, Loss: 3.9472930897141354\n",
      "Epoch: 315, Batch: 20, Loss: 2.3210070851773303\n",
      "Epoch: 315, Batch: 40, Loss: 2.985158706718709\n",
      "Epoch: 315, Mean Loss: 3.7424488372258224\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 316, Batch: 0, Loss: 3.770308785682839\n",
      "Epoch: 316, Batch: 20, Loss: 2.4641761390269146\n",
      "Epoch: 316, Batch: 40, Loss: 2.6875622192374298\n",
      "Epoch: 316, Mean Loss: 3.667313479216558\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 317, Batch: 0, Loss: 3.682899428447169\n",
      "Epoch: 317, Batch: 20, Loss: 2.6647838075101618\n",
      "Epoch: 317, Batch: 40, Loss: 2.460058528961342\n",
      "Epoch: 317, Mean Loss: 3.6401652063235956\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 318, Batch: 0, Loss: 4.0126614722502385\n",
      "Epoch: 318, Batch: 20, Loss: 2.772076643326636\n",
      "Epoch: 318, Batch: 40, Loss: 2.666796742818165\n",
      "Epoch: 318, Mean Loss: 3.6535757096767463\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 319, Batch: 0, Loss: 4.009693445658925\n",
      "Epoch: 319, Batch: 20, Loss: 2.7072428425059845\n",
      "Epoch: 319, Batch: 40, Loss: 2.5619304939190757\n",
      "Epoch: 319, Mean Loss: 3.6780216188815755\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 320, Batch: 0, Loss: 4.083932757415183\n",
      "Epoch: 320, Batch: 20, Loss: 2.5611602683540817\n",
      "Epoch: 320, Batch: 40, Loss: 2.743288701686367\n",
      "Epoch: 320, Mean Loss: 3.649077294441293\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 321, Batch: 0, Loss: 3.622484296136455\n",
      "Epoch: 321, Batch: 20, Loss: 2.4139298767912223\n",
      "Epoch: 321, Batch: 40, Loss: 2.670084411878644\n",
      "Epoch: 321, Mean Loss: 3.5568735350619187\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 322, Batch: 0, Loss: 3.578411336329141\n",
      "Epoch: 322, Batch: 20, Loss: 2.467250389632284\n",
      "Epoch: 322, Batch: 40, Loss: 2.6206810921252157\n",
      "Epoch: 322, Mean Loss: 3.5264401958940574\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 323, Batch: 0, Loss: 3.626586322599224\n",
      "Epoch: 323, Batch: 20, Loss: 2.5012851258958104\n",
      "Epoch: 323, Batch: 40, Loss: 2.596032447392715\n",
      "Epoch: 323, Mean Loss: 3.54720710017523\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 324, Batch: 0, Loss: 3.6755656398737075\n",
      "Epoch: 324, Batch: 20, Loss: 2.277629507719366\n",
      "Epoch: 324, Batch: 40, Loss: 2.8290012095808517\n",
      "Epoch: 324, Mean Loss: 3.560794700535416\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 325, Batch: 0, Loss: 3.493265495289809\n",
      "Epoch: 325, Batch: 20, Loss: 2.344287776856062\n",
      "Epoch: 325, Batch: 40, Loss: 2.682505787830576\n",
      "Epoch: 325, Mean Loss: 3.5045981199949283\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 326, Batch: 0, Loss: 3.3516066228460595\n",
      "Epoch: 326, Batch: 20, Loss: 2.2914705439531486\n",
      "Epoch: 326, Batch: 40, Loss: 2.672204244833548\n",
      "Epoch: 326, Mean Loss: 3.517922266283188\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 327, Batch: 0, Loss: 3.479259395568238\n",
      "Epoch: 327, Batch: 20, Loss: 2.3598366462713285\n",
      "Epoch: 327, Batch: 40, Loss: 2.748632059093784\n",
      "Epoch: 327, Mean Loss: 3.5200059602090654\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 328, Batch: 0, Loss: 3.551182807819513\n",
      "Epoch: 328, Batch: 20, Loss: 2.2337011724815867\n",
      "Epoch: 328, Batch: 40, Loss: 2.758341557052409\n",
      "Epoch: 328, Mean Loss: 3.5344300247337994\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 329, Batch: 0, Loss: 3.206289106676911\n",
      "Epoch: 329, Batch: 20, Loss: 2.2317343590489713\n",
      "Epoch: 329, Batch: 40, Loss: 2.7835283179976047\n",
      "Epoch: 329, Mean Loss: 3.5092912368121914\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 330, Batch: 0, Loss: 3.3390033877334058\n",
      "Epoch: 330, Batch: 20, Loss: 2.209513696650644\n",
      "Epoch: 330, Batch: 40, Loss: 2.951661412062004\n",
      "Epoch: 330, Mean Loss: 3.5016581846334076\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 331, Batch: 0, Loss: 3.307294979410145\n",
      "Epoch: 331, Batch: 20, Loss: 2.2162905921796794\n",
      "Epoch: 331, Batch: 40, Loss: 2.850638209039268\n",
      "Epoch: 331, Mean Loss: 3.5957919331222707\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 332, Batch: 0, Loss: 3.1956108570017334\n",
      "Epoch: 332, Batch: 20, Loss: 2.3426257290715613\n",
      "Epoch: 332, Batch: 40, Loss: 2.6215380985270063\n",
      "Epoch: 332, Mean Loss: 3.549921328831759\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 333, Batch: 0, Loss: 3.3920171107959685\n",
      "Epoch: 333, Batch: 20, Loss: 2.3059759149825148\n",
      "Epoch: 333, Batch: 40, Loss: 2.7422499623163756\n",
      "Epoch: 333, Mean Loss: 3.5808847512035915\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 334, Batch: 0, Loss: 3.3994332400332716\n",
      "Epoch: 334, Batch: 20, Loss: 2.3867743337030776\n",
      "Epoch: 334, Batch: 40, Loss: 2.6791576438840883\n",
      "Epoch: 334, Mean Loss: 3.626464420609629\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 335, Batch: 0, Loss: 3.452930245820015\n",
      "Epoch: 335, Batch: 20, Loss: 2.782539163053995\n",
      "Epoch: 335, Batch: 40, Loss: 2.372919584108061\n",
      "Epoch: 335, Mean Loss: 3.665554678646503\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 336, Batch: 0, Loss: 3.6772461604299704\n",
      "Epoch: 336, Batch: 20, Loss: 3.3228246699080883\n",
      "Epoch: 336, Batch: 40, Loss: 2.522946018409261\n",
      "Epoch: 336, Mean Loss: 3.801476176703848\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 337, Batch: 0, Loss: 3.854234927556431\n",
      "Epoch: 337, Batch: 20, Loss: 2.640228844493534\n",
      "Epoch: 337, Batch: 40, Loss: 2.6243586500358522\n",
      "Epoch: 337, Mean Loss: 3.758471444305913\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 338, Batch: 0, Loss: 4.02644981000618\n",
      "Epoch: 338, Batch: 20, Loss: 3.1140522155918138\n",
      "Epoch: 338, Batch: 40, Loss: 2.4817672099484733\n",
      "Epoch: 338, Mean Loss: 3.808956869596048\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 339, Batch: 0, Loss: 4.103469811499027\n",
      "Epoch: 339, Batch: 20, Loss: 3.6522170787572517\n",
      "Epoch: 339, Batch: 40, Loss: 2.361850247283874\n",
      "Epoch: 339, Mean Loss: 3.8274266655631464\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 340, Batch: 0, Loss: 4.603091273485163\n",
      "Epoch: 340, Batch: 20, Loss: 2.9717827340085576\n",
      "Epoch: 340, Batch: 40, Loss: 2.414209342229661\n",
      "Epoch: 340, Mean Loss: 3.72962211563571\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 341, Batch: 0, Loss: 4.14066227004203\n",
      "Epoch: 341, Batch: 20, Loss: 2.981815896914739\n",
      "Epoch: 341, Batch: 40, Loss: 2.722873661535251\n",
      "Epoch: 341, Mean Loss: 3.7234207076403805\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 342, Batch: 0, Loss: 4.037696205547648\n",
      "Epoch: 342, Batch: 20, Loss: 2.6651711960571602\n",
      "Epoch: 342, Batch: 40, Loss: 3.2788934360090005\n",
      "Epoch: 342, Mean Loss: 3.8206751789586892\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 343, Batch: 0, Loss: 3.638884417352875\n",
      "Epoch: 343, Batch: 20, Loss: 2.7654240898678535\n",
      "Epoch: 343, Batch: 40, Loss: 3.2116572656294906\n",
      "Epoch: 343, Mean Loss: 4.101915249977058\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 344, Batch: 0, Loss: 4.053246291614043\n",
      "Epoch: 344, Batch: 20, Loss: 3.101170813538656\n",
      "Epoch: 344, Batch: 40, Loss: 2.551137995409729\n",
      "Epoch: 344, Mean Loss: 4.029383016465697\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 345, Batch: 0, Loss: 3.885744321785814\n",
      "Epoch: 345, Batch: 20, Loss: 3.4111769894074424\n",
      "Epoch: 345, Batch: 40, Loss: 2.2263690142919077\n",
      "Epoch: 345, Mean Loss: 4.107358306380973\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 346, Batch: 0, Loss: 4.703613984280485\n",
      "Epoch: 346, Batch: 20, Loss: 3.175422923689062\n",
      "Epoch: 346, Batch: 40, Loss: 2.5961974072162293\n",
      "Epoch: 346, Mean Loss: 3.8812067383383897\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 347, Batch: 0, Loss: 4.212490787866564\n",
      "Epoch: 347, Batch: 20, Loss: 2.9855727833711674\n",
      "Epoch: 347, Batch: 40, Loss: 2.461503414021026\n",
      "Epoch: 347, Mean Loss: 3.76656159221643\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 348, Batch: 0, Loss: 4.230678527933503\n",
      "Epoch: 348, Batch: 20, Loss: 2.3595657588336114\n",
      "Epoch: 348, Batch: 40, Loss: 2.794411668499013\n",
      "Epoch: 348, Mean Loss: 3.629054533465367\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 349, Batch: 0, Loss: 3.6479687031377073\n",
      "Epoch: 349, Batch: 20, Loss: 2.565948143687227\n",
      "Epoch: 349, Batch: 40, Loss: 2.704708644786494\n",
      "Epoch: 349, Mean Loss: 3.6218916383441284\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 350, Batch: 0, Loss: 4.068605140879076\n",
      "Epoch: 350, Batch: 20, Loss: 2.4502604339954894\n",
      "Epoch: 350, Batch: 40, Loss: 2.7142923033200517\n",
      "Epoch: 350, Mean Loss: 3.568396087506564\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 351, Batch: 0, Loss: 3.901223429160506\n",
      "Epoch: 351, Batch: 20, Loss: 2.44984510370854\n",
      "Epoch: 351, Batch: 40, Loss: 2.728069333424379\n",
      "Epoch: 351, Mean Loss: 3.5132577488533996\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 352, Batch: 0, Loss: 3.9721774794576756\n",
      "Epoch: 352, Batch: 20, Loss: 2.6032859168976645\n",
      "Epoch: 352, Batch: 40, Loss: 2.674552044653898\n",
      "Epoch: 352, Mean Loss: 3.547887841265129\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 353, Batch: 0, Loss: 3.7159373657977617\n",
      "Epoch: 353, Batch: 20, Loss: 2.3970004533491256\n",
      "Epoch: 353, Batch: 40, Loss: 2.5922791614081055\n",
      "Epoch: 353, Mean Loss: 3.5297373762737574\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 354, Batch: 0, Loss: 3.892600881069241\n",
      "Epoch: 354, Batch: 20, Loss: 2.638457755839462\n",
      "Epoch: 354, Batch: 40, Loss: 2.4361151066235367\n",
      "Epoch: 354, Mean Loss: 3.527882451824306\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 355, Batch: 0, Loss: 4.073558242529245\n",
      "Epoch: 355, Batch: 20, Loss: 2.3324657743781634\n",
      "Epoch: 355, Batch: 40, Loss: 2.685105617115528\n",
      "Epoch: 355, Mean Loss: 3.4778561341349024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 356, Batch: 0, Loss: 3.6792366669421783\n",
      "Epoch: 356, Batch: 20, Loss: 2.3476776096505008\n",
      "Epoch: 356, Batch: 40, Loss: 2.4264186238196093\n",
      "Epoch: 356, Mean Loss: 3.5067298013095063\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 357, Batch: 0, Loss: 3.562539386089227\n",
      "Epoch: 357, Batch: 20, Loss: 2.3671105192254576\n",
      "Epoch: 357, Batch: 40, Loss: 2.535526535923625\n",
      "Epoch: 357, Mean Loss: 3.452328502973645\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 358, Batch: 0, Loss: 3.7407400063551544\n",
      "Epoch: 358, Batch: 20, Loss: 2.378890713764068\n",
      "Epoch: 358, Batch: 40, Loss: 2.4320060112286592\n",
      "Epoch: 358, Mean Loss: 3.450407321560348\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 359, Batch: 0, Loss: 3.768374819561467\n",
      "Epoch: 359, Batch: 20, Loss: 2.337507880629857\n",
      "Epoch: 359, Batch: 40, Loss: 2.4914825125912823\n",
      "Epoch: 359, Mean Loss: 3.4225185328216905\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 360, Batch: 0, Loss: 3.88603957028283\n",
      "Epoch: 360, Batch: 20, Loss: 2.4918194318552627\n",
      "Epoch: 360, Batch: 40, Loss: 2.420759180091373\n",
      "Epoch: 360, Mean Loss: 3.440856484037523\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 361, Batch: 0, Loss: 3.7261252311251956\n",
      "Epoch: 361, Batch: 20, Loss: 2.2610374374450384\n",
      "Epoch: 361, Batch: 40, Loss: 2.4113509025169018\n",
      "Epoch: 361, Mean Loss: 3.411774489658265\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 362, Batch: 0, Loss: 3.730165829289543\n",
      "Epoch: 362, Batch: 20, Loss: 2.222849848144679\n",
      "Epoch: 362, Batch: 40, Loss: 2.413079063908925\n",
      "Epoch: 362, Mean Loss: 3.413007938154745\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 363, Batch: 0, Loss: 3.8884242011981724\n",
      "Epoch: 363, Batch: 20, Loss: 3.0135461946316697\n",
      "Epoch: 363, Batch: 40, Loss: 2.2783667295334062\n",
      "Epoch: 363, Mean Loss: 3.626915911546336\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 364, Batch: 0, Loss: 4.183717678555298\n",
      "Epoch: 364, Batch: 20, Loss: 2.845706123415466\n",
      "Epoch: 364, Batch: 40, Loss: 2.337688625298064\n",
      "Epoch: 364, Mean Loss: 3.620011468835459\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 365, Batch: 0, Loss: 3.828087691542362\n",
      "Epoch: 365, Batch: 20, Loss: 2.621418234923994\n",
      "Epoch: 365, Batch: 40, Loss: 2.300443641172068\n",
      "Epoch: 365, Mean Loss: 3.583734739721091\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 366, Batch: 0, Loss: 4.300328253406167\n",
      "Epoch: 366, Batch: 20, Loss: 2.6337779631786042\n",
      "Epoch: 366, Batch: 40, Loss: 2.414745457047151\n",
      "Epoch: 366, Mean Loss: 3.6052516314377927\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 367, Batch: 0, Loss: 4.156406799785846\n",
      "Epoch: 367, Batch: 20, Loss: 2.6167294791270024\n",
      "Epoch: 367, Batch: 40, Loss: 2.463088874612455\n",
      "Epoch: 367, Mean Loss: 3.567253586039762\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 368, Batch: 0, Loss: 4.140015481358882\n",
      "Epoch: 368, Batch: 20, Loss: 2.613573272181686\n",
      "Epoch: 368, Batch: 40, Loss: 2.552251682825423\n",
      "Epoch: 368, Mean Loss: 3.5312454444305423\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 369, Batch: 0, Loss: 3.737963913467376\n",
      "Epoch: 369, Batch: 20, Loss: 2.20482688384092\n",
      "Epoch: 369, Batch: 40, Loss: 2.756347321466788\n",
      "Epoch: 369, Mean Loss: 3.4237923043984546\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 370, Batch: 0, Loss: 3.3761875758268247\n",
      "Epoch: 370, Batch: 20, Loss: 2.4090733672067346\n",
      "Epoch: 370, Batch: 40, Loss: 2.744410506138219\n",
      "Epoch: 370, Mean Loss: 3.44761392221966\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 371, Batch: 0, Loss: 3.325025864282703\n",
      "Epoch: 371, Batch: 20, Loss: 2.4224091418861957\n",
      "Epoch: 371, Batch: 40, Loss: 2.376536166693683\n",
      "Epoch: 371, Mean Loss: 3.382107292999885\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 372, Batch: 0, Loss: 3.4833019614659047\n",
      "Epoch: 372, Batch: 20, Loss: 2.274130985132676\n",
      "Epoch: 372, Batch: 40, Loss: 2.534985872224268\n",
      "Epoch: 372, Mean Loss: 3.3789499408100623\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 373, Batch: 0, Loss: 3.496804465934668\n",
      "Epoch: 373, Batch: 20, Loss: 2.124513778082351\n",
      "Epoch: 373, Batch: 40, Loss: 2.6493501357383957\n",
      "Epoch: 373, Mean Loss: 3.377591564194383\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 374, Batch: 0, Loss: 3.0544256569611203\n",
      "Epoch: 374, Batch: 20, Loss: 2.205942393373132\n",
      "Epoch: 374, Batch: 40, Loss: 2.5084647909572877\n",
      "Epoch: 374, Mean Loss: 3.3443006933913795\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 375, Batch: 0, Loss: 3.149366503489086\n",
      "Epoch: 375, Batch: 20, Loss: 1.9777379863280795\n",
      "Epoch: 375, Batch: 40, Loss: 2.6555753072069463\n",
      "Epoch: 375, Mean Loss: 3.3360509706001102\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 376, Batch: 0, Loss: 3.451462279500685\n",
      "Epoch: 376, Batch: 20, Loss: 2.387608720240749\n",
      "Epoch: 376, Batch: 40, Loss: 2.324699010959425\n",
      "Epoch: 376, Mean Loss: 3.3537670388147394\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 377, Batch: 0, Loss: 3.213775016745961\n",
      "Epoch: 377, Batch: 20, Loss: 2.1353106829642754\n",
      "Epoch: 377, Batch: 40, Loss: 2.498281711934858\n",
      "Epoch: 377, Mean Loss: 3.369653062660997\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 378, Batch: 0, Loss: 3.3480412547942895\n",
      "Epoch: 378, Batch: 20, Loss: 2.235357753252179\n",
      "Epoch: 378, Batch: 40, Loss: 2.3978224935586896\n",
      "Epoch: 378, Mean Loss: 3.4232757973546706\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 379, Batch: 0, Loss: 3.323722813824645\n",
      "Epoch: 379, Batch: 20, Loss: 2.262332548398904\n",
      "Epoch: 379, Batch: 40, Loss: 2.29837512117322\n",
      "Epoch: 379, Mean Loss: 3.402902057442453\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 380, Batch: 0, Loss: 3.123299602407887\n",
      "Epoch: 380, Batch: 20, Loss: 2.2178754817381265\n",
      "Epoch: 380, Batch: 40, Loss: 2.325891897648535\n",
      "Epoch: 380, Mean Loss: 3.369139281834908\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 381, Batch: 0, Loss: 3.4760673819431283\n",
      "Epoch: 381, Batch: 20, Loss: 2.216744133881806\n",
      "Epoch: 381, Batch: 40, Loss: 2.770993694110969\n",
      "Epoch: 381, Mean Loss: 3.464314495227703\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 382, Batch: 0, Loss: 3.058008972603063\n",
      "Epoch: 382, Batch: 20, Loss: 2.728170762408876\n",
      "Epoch: 382, Batch: 40, Loss: 2.519986566582007\n",
      "Epoch: 382, Mean Loss: 3.5897668315733653\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 383, Batch: 0, Loss: 3.290183827930351\n",
      "Epoch: 383, Batch: 20, Loss: 2.729543435233635\n",
      "Epoch: 383, Batch: 40, Loss: 2.7059807927117734\n",
      "Epoch: 383, Mean Loss: 3.5049125924534503\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 384, Batch: 0, Loss: 3.553678124521633\n",
      "Epoch: 384, Batch: 20, Loss: 2.865235667235922\n",
      "Epoch: 384, Batch: 40, Loss: 2.3816314349891994\n",
      "Epoch: 384, Mean Loss: 3.61411293377197\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 385, Batch: 0, Loss: 3.726461862072556\n",
      "Epoch: 385, Batch: 20, Loss: 2.8916769416560886\n",
      "Epoch: 385, Batch: 40, Loss: 2.5065324828318087\n",
      "Epoch: 385, Mean Loss: 3.651778714763496\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 386, Batch: 0, Loss: 3.8241257459729385\n",
      "Epoch: 386, Batch: 20, Loss: 2.737380058729681\n",
      "Epoch: 386, Batch: 40, Loss: 2.5536524672501955\n",
      "Epoch: 386, Mean Loss: 3.6059209495079854\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 387, Batch: 0, Loss: 3.7243508642715146\n",
      "Epoch: 387, Batch: 20, Loss: 2.7400718393643984\n",
      "Epoch: 387, Batch: 40, Loss: 2.1549220069120567\n",
      "Epoch: 387, Mean Loss: 3.543725238462282\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 388, Batch: 0, Loss: 3.7779460770634223\n",
      "Epoch: 388, Batch: 20, Loss: 2.336882017386076\n",
      "Epoch: 388, Batch: 40, Loss: 2.2870639232207264\n",
      "Epoch: 388, Mean Loss: 3.4150375558690973\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 389, Batch: 0, Loss: 3.850483635491735\n",
      "Epoch: 389, Batch: 20, Loss: 2.475233243695331\n",
      "Epoch: 389, Batch: 40, Loss: 2.4721839862319803\n",
      "Epoch: 389, Mean Loss: 3.3984311570367725\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 390, Batch: 0, Loss: 3.3251053794662173\n",
      "Epoch: 390, Batch: 20, Loss: 2.087610201629148\n",
      "Epoch: 390, Batch: 40, Loss: 2.714257320796234\n",
      "Epoch: 390, Mean Loss: 3.379089377567156\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 391, Batch: 0, Loss: 3.593689537397959\n",
      "Epoch: 391, Batch: 20, Loss: 2.589727188638857\n",
      "Epoch: 391, Batch: 40, Loss: 2.453971189653002\n",
      "Epoch: 391, Mean Loss: 3.382501761856495\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 392, Batch: 0, Loss: 3.5346998312743643\n",
      "Epoch: 392, Batch: 20, Loss: 2.532246487225912\n",
      "Epoch: 392, Batch: 40, Loss: 2.551596645265013\n",
      "Epoch: 392, Mean Loss: 3.3715466400827747\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 393, Batch: 0, Loss: 3.4760560687580355\n",
      "Epoch: 393, Batch: 20, Loss: 2.3106252233143065\n",
      "Epoch: 393, Batch: 40, Loss: 2.717956640619897\n",
      "Epoch: 393, Mean Loss: 3.3568998969906345\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 394, Batch: 0, Loss: 3.3475653180289124\n",
      "Epoch: 394, Batch: 20, Loss: 2.511242879483923\n",
      "Epoch: 394, Batch: 40, Loss: 2.5631344874622424\n",
      "Epoch: 394, Mean Loss: 3.3927583010130964\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 395, Batch: 0, Loss: 3.3000410873582573\n",
      "Epoch: 395, Batch: 20, Loss: 2.3365609271629766\n",
      "Epoch: 395, Batch: 40, Loss: 2.51638878258824\n",
      "Epoch: 395, Mean Loss: 3.3538454600367804\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 396, Batch: 0, Loss: 3.4867339085953217\n",
      "Epoch: 396, Batch: 20, Loss: 2.513432416223917\n",
      "Epoch: 396, Batch: 40, Loss: 2.580201625274162\n",
      "Epoch: 396, Mean Loss: 3.370906154264228\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 397, Batch: 0, Loss: 3.3818550594130983\n",
      "Epoch: 397, Batch: 20, Loss: 2.476024025254\n",
      "Epoch: 397, Batch: 40, Loss: 2.3274806043719884\n",
      "Epoch: 397, Mean Loss: 3.4228378892531066\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 398, Batch: 0, Loss: 3.557783361539787\n",
      "Epoch: 398, Batch: 20, Loss: 2.9723949156860288\n",
      "Epoch: 398, Batch: 40, Loss: 2.5157805222885536\n",
      "Epoch: 398, Mean Loss: 3.4532047806520967\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 399, Batch: 0, Loss: 3.7836367990462043\n",
      "Epoch: 399, Batch: 20, Loss: 4.5175712902251215\n",
      "Epoch: 399, Batch: 40, Loss: 3.621076964367415\n",
      "Epoch: 399, Mean Loss: 5.835921033701233\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 400, Batch: 0, Loss: 4.295981934135189\n",
      "Epoch: 400, Batch: 20, Loss: 3.3594056740819354\n",
      "Epoch: 400, Batch: 40, Loss: 4.406989159830521\n",
      "Epoch: 400, Mean Loss: 5.733649459964394\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 401, Batch: 0, Loss: 4.933957259576617\n",
      "Epoch: 401, Batch: 20, Loss: 3.012041619398984\n",
      "Epoch: 401, Batch: 40, Loss: 3.288565694168111\n",
      "Epoch: 401, Mean Loss: 4.9032680121010905\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 402, Batch: 0, Loss: 4.1053136894819255\n",
      "Epoch: 402, Batch: 20, Loss: 3.040229795122967\n",
      "Epoch: 402, Batch: 40, Loss: 2.7743330059597198\n",
      "Epoch: 402, Mean Loss: 4.3522785809334845\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 403, Batch: 0, Loss: 3.538199932427866\n",
      "Epoch: 403, Batch: 20, Loss: 2.951555299042636\n",
      "Epoch: 403, Batch: 40, Loss: 2.9893199270100217\n",
      "Epoch: 403, Mean Loss: 4.261815067236119\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 404, Batch: 0, Loss: 3.5081675378380317\n",
      "Epoch: 404, Batch: 20, Loss: 3.368994250146165\n",
      "Epoch: 404, Batch: 40, Loss: 3.505854115001709\n",
      "Epoch: 404, Mean Loss: 4.8327167706839615\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 405, Batch: 0, Loss: 3.4457887101027307\n",
      "Epoch: 405, Batch: 20, Loss: 2.511786796204488\n",
      "Epoch: 405, Batch: 40, Loss: 2.8408475558277178\n",
      "Epoch: 405, Mean Loss: 3.743502084095788\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 406, Batch: 0, Loss: 3.619947831732133\n",
      "Epoch: 406, Batch: 20, Loss: 2.2664483468565626\n",
      "Epoch: 406, Batch: 40, Loss: 2.840098720844066\n",
      "Epoch: 406, Mean Loss: 3.6265288086533563\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 407, Batch: 0, Loss: 3.4887095855685373\n",
      "Epoch: 407, Batch: 20, Loss: 2.148209179668357\n",
      "Epoch: 407, Batch: 40, Loss: 2.8986809729516474\n",
      "Epoch: 407, Mean Loss: 3.538182373092105\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 408, Batch: 0, Loss: 3.3843631328639754\n",
      "Epoch: 408, Batch: 20, Loss: 2.0949968134575063\n",
      "Epoch: 408, Batch: 40, Loss: 2.7000518568822827\n",
      "Epoch: 408, Mean Loss: 3.4604113410860373\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 409, Batch: 0, Loss: 3.4113322278941807\n",
      "Epoch: 409, Batch: 20, Loss: 2.159251128252932\n",
      "Epoch: 409, Batch: 40, Loss: 2.433086333568794\n",
      "Epoch: 409, Mean Loss: 3.422345834948452\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 410, Batch: 0, Loss: 3.5521985804234095\n",
      "Epoch: 410, Batch: 20, Loss: 2.2938155557681337\n",
      "Epoch: 410, Batch: 40, Loss: 2.337759610526027\n",
      "Epoch: 410, Mean Loss: 3.424305684985027\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 411, Batch: 0, Loss: 3.5254804147029124\n",
      "Epoch: 411, Batch: 20, Loss: 2.266972260581447\n",
      "Epoch: 411, Batch: 40, Loss: 2.329151860387439\n",
      "Epoch: 411, Mean Loss: 3.428960393776714\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 412, Batch: 0, Loss: 3.6098691623120645\n",
      "Epoch: 412, Batch: 20, Loss: 2.1377835724470873\n",
      "Epoch: 412, Batch: 40, Loss: 2.2724166985372527\n",
      "Epoch: 412, Mean Loss: 3.3886834475135106\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 413, Batch: 0, Loss: 3.510516322121045\n",
      "Epoch: 413, Batch: 20, Loss: 2.242099236524344\n",
      "Epoch: 413, Batch: 40, Loss: 2.2387759522110358\n",
      "Epoch: 413, Mean Loss: 3.3371271152549\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 414, Batch: 0, Loss: 3.4801222797311078\n",
      "Epoch: 414, Batch: 20, Loss: 2.1779302292410803\n",
      "Epoch: 414, Batch: 40, Loss: 2.310613207004558\n",
      "Epoch: 414, Mean Loss: 3.3308021219342283\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 415, Batch: 0, Loss: 3.627165650991631\n",
      "Epoch: 415, Batch: 20, Loss: 2.1739042862717985\n",
      "Epoch: 415, Batch: 40, Loss: 2.2691803527834016\n",
      "Epoch: 415, Mean Loss: 3.3109807230422525\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 416, Batch: 0, Loss: 3.746269631122491\n",
      "Epoch: 416, Batch: 20, Loss: 2.1762963486748723\n",
      "Epoch: 416, Batch: 40, Loss: 2.282721596385897\n",
      "Epoch: 416, Mean Loss: 3.3272065103873194\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 417, Batch: 0, Loss: 3.5531356226219275\n",
      "Epoch: 417, Batch: 20, Loss: 2.251949713674037\n",
      "Epoch: 417, Batch: 40, Loss: 2.1865836240261607\n",
      "Epoch: 417, Mean Loss: 3.295663141685947\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 418, Batch: 0, Loss: 3.588858906218305\n",
      "Epoch: 418, Batch: 20, Loss: 2.278574730992964\n",
      "Epoch: 418, Batch: 40, Loss: 2.2373281503768934\n",
      "Epoch: 418, Mean Loss: 3.315195127793718\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 419, Batch: 0, Loss: 3.7000686776337406\n",
      "Epoch: 419, Batch: 20, Loss: 2.143136103513568\n",
      "Epoch: 419, Batch: 40, Loss: 2.3391537483373934\n",
      "Epoch: 419, Mean Loss: 3.291992640600036\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 420, Batch: 0, Loss: 3.685901417268212\n",
      "Epoch: 420, Batch: 20, Loss: 2.264862011363178\n",
      "Epoch: 420, Batch: 40, Loss: 2.2759046105850858\n",
      "Epoch: 420, Mean Loss: 3.2970433185577788\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 421, Batch: 0, Loss: 3.588056455950685\n",
      "Epoch: 421, Batch: 20, Loss: 2.115275919689098\n",
      "Epoch: 421, Batch: 40, Loss: 2.336534314013925\n",
      "Epoch: 421, Mean Loss: 3.2705350122523154\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 422, Batch: 0, Loss: 3.235767065651863\n",
      "Epoch: 422, Batch: 20, Loss: 2.132309084717423\n",
      "Epoch: 422, Batch: 40, Loss: 2.177502570990536\n",
      "Epoch: 422, Mean Loss: 3.2402265578234575\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 423, Batch: 0, Loss: 3.3803552588988017\n",
      "Epoch: 423, Batch: 20, Loss: 2.1045916028126137\n",
      "Epoch: 423, Batch: 40, Loss: 2.3018503320578962\n",
      "Epoch: 423, Mean Loss: 3.21942942692478\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 424, Batch: 0, Loss: 3.4058675489966856\n",
      "Epoch: 424, Batch: 20, Loss: 2.170242532603962\n",
      "Epoch: 424, Batch: 40, Loss: 2.1839445011001053\n",
      "Epoch: 424, Mean Loss: 3.24295271804795\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 425, Batch: 0, Loss: 3.591569215220413\n",
      "Epoch: 425, Batch: 20, Loss: 2.125005535689764\n",
      "Epoch: 425, Batch: 40, Loss: 2.206079529113281\n",
      "Epoch: 425, Mean Loss: 3.251325355022335\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 426, Batch: 0, Loss: 3.319428946593147\n",
      "Epoch: 426, Batch: 20, Loss: 2.04312618524727\n",
      "Epoch: 426, Batch: 40, Loss: 2.2166952683698593\n",
      "Epoch: 426, Mean Loss: 3.2335705504925003\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 427, Batch: 0, Loss: 3.4803439731259993\n",
      "Epoch: 427, Batch: 20, Loss: 2.1979334304308904\n",
      "Epoch: 427, Batch: 40, Loss: 2.1755282619893905\n",
      "Epoch: 427, Mean Loss: 3.2603252120959727\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 428, Batch: 0, Loss: 3.48161237458896\n",
      "Epoch: 428, Batch: 20, Loss: 2.161505318006182\n",
      "Epoch: 428, Batch: 40, Loss: 2.160858264527025\n",
      "Epoch: 428, Mean Loss: 3.2747334430847395\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 429, Batch: 0, Loss: 3.4736426022831783\n",
      "Epoch: 429, Batch: 20, Loss: 2.229273570648951\n",
      "Epoch: 429, Batch: 40, Loss: 2.071673386667473\n",
      "Epoch: 429, Mean Loss: 3.3014601878816694\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 430, Batch: 0, Loss: 3.3629018341793397\n",
      "Epoch: 430, Batch: 20, Loss: 2.111751531426058\n",
      "Epoch: 430, Batch: 40, Loss: 2.137187759887437\n",
      "Epoch: 430, Mean Loss: 3.2780676798349915\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 431, Batch: 0, Loss: 3.4225536615547507\n",
      "Epoch: 431, Batch: 20, Loss: 2.359089509115164\n",
      "Epoch: 431, Batch: 40, Loss: 2.183880438329005\n",
      "Epoch: 431, Mean Loss: 3.4639686071405213\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 432, Batch: 0, Loss: 3.4538851928976966\n",
      "Epoch: 432, Batch: 20, Loss: 2.5334152379293573\n",
      "Epoch: 432, Batch: 40, Loss: 2.1703978194065874\n",
      "Epoch: 432, Mean Loss: 3.41643530631592\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 433, Batch: 0, Loss: 3.9434125742715245\n",
      "Epoch: 433, Batch: 20, Loss: 2.29660468115708\n",
      "Epoch: 433, Batch: 40, Loss: 2.2496224855333313\n",
      "Epoch: 433, Mean Loss: 3.3534974123717154\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 434, Batch: 0, Loss: 3.972841345438079\n",
      "Epoch: 434, Batch: 20, Loss: 2.273624738085195\n",
      "Epoch: 434, Batch: 40, Loss: 2.12698070887526\n",
      "Epoch: 434, Mean Loss: 3.3024342391035697\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 435, Batch: 0, Loss: 3.2756686165897237\n",
      "Epoch: 435, Batch: 20, Loss: 2.048936289662164\n",
      "Epoch: 435, Batch: 40, Loss: 2.3417782966636818\n",
      "Epoch: 435, Mean Loss: 3.2341260982023767\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 436, Batch: 0, Loss: 3.351015399796928\n",
      "Epoch: 436, Batch: 20, Loss: 2.078038023727292\n",
      "Epoch: 436, Batch: 40, Loss: 2.41301679719071\n",
      "Epoch: 436, Mean Loss: 3.2218495546188692\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 437, Batch: 0, Loss: 3.289189848704566\n",
      "Epoch: 437, Batch: 20, Loss: 2.2064360599012085\n",
      "Epoch: 437, Batch: 40, Loss: 2.3709818080871887\n",
      "Epoch: 437, Mean Loss: 3.2693532325049306\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 438, Batch: 0, Loss: 3.3848286403817958\n",
      "Epoch: 438, Batch: 20, Loss: 2.2331068814791366\n",
      "Epoch: 438, Batch: 40, Loss: 2.2304935181414316\n",
      "Epoch: 438, Mean Loss: 3.2420787298711424\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 439, Batch: 0, Loss: 3.30713442105034\n",
      "Epoch: 439, Batch: 20, Loss: 2.015111410981299\n",
      "Epoch: 439, Batch: 40, Loss: 2.560426838729256\n",
      "Epoch: 439, Mean Loss: 3.2128365627581186\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 440, Batch: 0, Loss: 3.1339316724882575\n",
      "Epoch: 440, Batch: 20, Loss: 2.1155995605592164\n",
      "Epoch: 440, Batch: 40, Loss: 2.456433079030732\n",
      "Epoch: 440, Mean Loss: 3.2178874645254187\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 441, Batch: 0, Loss: 3.22777552384391\n",
      "Epoch: 441, Batch: 20, Loss: 2.0947536480712596\n",
      "Epoch: 441, Batch: 40, Loss: 2.410008476251716\n",
      "Epoch: 441, Mean Loss: 3.2410301862254194\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 442, Batch: 0, Loss: 3.296417503031362\n",
      "Epoch: 442, Batch: 20, Loss: 1.985202888357469\n",
      "Epoch: 442, Batch: 40, Loss: 2.515994962485512\n",
      "Epoch: 442, Mean Loss: 3.1951623004208316\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 443, Batch: 0, Loss: 3.2281868774661406\n",
      "Epoch: 443, Batch: 20, Loss: 2.1080528452648464\n",
      "Epoch: 443, Batch: 40, Loss: 2.605763269126563\n",
      "Epoch: 443, Mean Loss: 3.220682800036364\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 444, Batch: 0, Loss: 3.2595366139362714\n",
      "Epoch: 444, Batch: 20, Loss: 2.153767397905219\n",
      "Epoch: 444, Batch: 40, Loss: 2.4854695529009128\n",
      "Epoch: 444, Mean Loss: 3.265200822558275\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 445, Batch: 0, Loss: 3.0479416497351535\n",
      "Epoch: 445, Batch: 20, Loss: 1.9929517910791115\n",
      "Epoch: 445, Batch: 40, Loss: 2.544363553456285\n",
      "Epoch: 445, Mean Loss: 3.192866208819014\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 446, Batch: 0, Loss: 3.147388740238243\n",
      "Epoch: 446, Batch: 20, Loss: 2.1643909559947705\n",
      "Epoch: 446, Batch: 40, Loss: 2.562254476938843\n",
      "Epoch: 446, Mean Loss: 3.1920982409781447\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 447, Batch: 0, Loss: 3.2640831157275265\n",
      "Epoch: 447, Batch: 20, Loss: 2.333352034138458\n",
      "Epoch: 447, Batch: 40, Loss: 2.161726512723075\n",
      "Epoch: 447, Mean Loss: 3.259168344915849\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 448, Batch: 0, Loss: 3.168837930313008\n",
      "Epoch: 448, Batch: 20, Loss: 2.076756227017078\n",
      "Epoch: 448, Batch: 40, Loss: 2.5408686133443386\n",
      "Epoch: 448, Mean Loss: 3.198890870814146\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 449, Batch: 0, Loss: 3.0513634015170368\n",
      "Epoch: 449, Batch: 20, Loss: 2.0937878260361322\n",
      "Epoch: 449, Batch: 40, Loss: 2.4330502962257667\n",
      "Epoch: 449, Mean Loss: 3.180982930426566\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 450, Batch: 0, Loss: 3.1563673821875664\n",
      "Epoch: 450, Batch: 20, Loss: 2.105056028209196\n",
      "Epoch: 450, Batch: 40, Loss: 2.56615902665495\n",
      "Epoch: 450, Mean Loss: 3.192889102578444\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 451, Batch: 0, Loss: 3.0784171915998666\n",
      "Epoch: 451, Batch: 20, Loss: 2.126317363829408\n",
      "Epoch: 451, Batch: 40, Loss: 2.4632521968707057\n",
      "Epoch: 451, Mean Loss: 3.205515162348663\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 452, Batch: 0, Loss: 2.8804304522390574\n",
      "Epoch: 452, Batch: 20, Loss: 2.019832992210018\n",
      "Epoch: 452, Batch: 40, Loss: 2.8618870813979562\n",
      "Epoch: 452, Mean Loss: 3.1882970657252288\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 453, Batch: 0, Loss: 3.0941264266545114\n",
      "Epoch: 453, Batch: 20, Loss: 2.2259894406262215\n",
      "Epoch: 453, Batch: 40, Loss: 2.348476403810972\n",
      "Epoch: 453, Mean Loss: 3.212419581314049\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 454, Batch: 0, Loss: 2.9872656613214947\n",
      "Epoch: 454, Batch: 20, Loss: 2.103934681482958\n",
      "Epoch: 454, Batch: 40, Loss: 2.476786578287569\n",
      "Epoch: 454, Mean Loss: 3.20570452100454\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 455, Batch: 0, Loss: 3.136497873559643\n",
      "Epoch: 455, Batch: 20, Loss: 2.5404817269648836\n",
      "Epoch: 455, Batch: 40, Loss: 2.6550213815256423\n",
      "Epoch: 455, Mean Loss: 3.2329838124707253\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 456, Batch: 0, Loss: 3.095288767317116\n",
      "Epoch: 456, Batch: 20, Loss: 2.1170303362253446\n",
      "Epoch: 456, Batch: 40, Loss: 2.7579843852920267\n",
      "Epoch: 456, Mean Loss: 3.2204968633027526\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 457, Batch: 0, Loss: 3.0907397879071303\n",
      "Epoch: 457, Batch: 20, Loss: 2.2558039542551125\n",
      "Epoch: 457, Batch: 40, Loss: 2.613838029487369\n",
      "Epoch: 457, Mean Loss: 3.2413545363801917\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 458, Batch: 0, Loss: 3.0466311544157416\n",
      "Epoch: 458, Batch: 20, Loss: 2.3271083459572894\n",
      "Epoch: 458, Batch: 40, Loss: 2.772094629429384\n",
      "Epoch: 458, Mean Loss: 3.293712223703005\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 459, Batch: 0, Loss: 3.141840824687452\n",
      "Epoch: 459, Batch: 20, Loss: 2.4301391048332652\n",
      "Epoch: 459, Batch: 40, Loss: 2.9938877882297814\n",
      "Epoch: 459, Mean Loss: 3.5224994428719607\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 460, Batch: 0, Loss: 3.199645534806834\n",
      "Epoch: 460, Batch: 20, Loss: 2.667773329183761\n",
      "Epoch: 460, Batch: 40, Loss: 2.8242655862906387\n",
      "Epoch: 460, Mean Loss: 3.564316434708502\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 461, Batch: 0, Loss: 2.7002931432384543\n",
      "Epoch: 461, Batch: 20, Loss: 3.1417071647274386\n",
      "Epoch: 461, Batch: 40, Loss: 2.5928153379633123\n",
      "Epoch: 461, Mean Loss: 3.561747121489534\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 462, Batch: 0, Loss: 2.9914992441913384\n",
      "Epoch: 462, Batch: 20, Loss: 2.8697825628249296\n",
      "Epoch: 462, Batch: 40, Loss: 2.7293043665354935\n",
      "Epoch: 462, Mean Loss: 3.4886376688195844\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 463, Batch: 0, Loss: 3.284502365377693\n",
      "Epoch: 463, Batch: 20, Loss: 2.2026601670736254\n",
      "Epoch: 463, Batch: 40, Loss: 4.262351956233516\n",
      "Epoch: 463, Mean Loss: 4.5613679158988925\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 464, Batch: 0, Loss: 3.6397016161848463\n",
      "Epoch: 464, Batch: 20, Loss: 2.6872115250936472\n",
      "Epoch: 464, Batch: 40, Loss: 2.7894239455379255\n",
      "Epoch: 464, Mean Loss: 3.8296143126227404\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 465, Batch: 0, Loss: 3.973771258523444\n",
      "Epoch: 465, Batch: 20, Loss: 5.430146277887016\n",
      "Epoch: 465, Batch: 40, Loss: 2.64256565206946\n",
      "Epoch: 465, Mean Loss: 4.174926905008995\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 466, Batch: 0, Loss: 3.698570745030411\n",
      "Epoch: 466, Batch: 20, Loss: 3.3011367694552307\n",
      "Epoch: 466, Batch: 40, Loss: 2.73497078880584\n",
      "Epoch: 466, Mean Loss: 3.8350549364015136\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 467, Batch: 0, Loss: 3.623661083503746\n",
      "Epoch: 467, Batch: 20, Loss: 2.5077380763533634\n",
      "Epoch: 467, Batch: 40, Loss: 3.0309008971648566\n",
      "Epoch: 467, Mean Loss: 3.7274453894836985\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 468, Batch: 0, Loss: 3.2197683278668983\n",
      "Epoch: 468, Batch: 20, Loss: 3.092498025857849\n",
      "Epoch: 468, Batch: 40, Loss: 2.2847569278637816\n",
      "Epoch: 468, Mean Loss: 3.5879641639859767\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 469, Batch: 0, Loss: 3.6552731736144612\n",
      "Epoch: 469, Batch: 20, Loss: 2.7170664951370083\n",
      "Epoch: 469, Batch: 40, Loss: 2.32984651238446\n",
      "Epoch: 469, Mean Loss: 3.449011031511705\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 470, Batch: 0, Loss: 3.742270470003058\n",
      "Epoch: 470, Batch: 20, Loss: 2.2019655527236135\n",
      "Epoch: 470, Batch: 40, Loss: 2.375509769655636\n",
      "Epoch: 470, Mean Loss: 3.3357674592248\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 471, Batch: 0, Loss: 3.6349452529794934\n",
      "Epoch: 471, Batch: 20, Loss: 2.3129087013692047\n",
      "Epoch: 471, Batch: 40, Loss: 2.2587657399577923\n",
      "Epoch: 471, Mean Loss: 3.260806514975934\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 472, Batch: 0, Loss: 3.290068875101184\n",
      "Epoch: 472, Batch: 20, Loss: 2.359784686817914\n",
      "Epoch: 472, Batch: 40, Loss: 2.1671653635338877\n",
      "Epoch: 472, Mean Loss: 3.2037118612200244\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 473, Batch: 0, Loss: 3.461661275879609\n",
      "Epoch: 473, Batch: 20, Loss: 2.3404066617575374\n",
      "Epoch: 473, Batch: 40, Loss: 2.015657703369113\n",
      "Epoch: 473, Mean Loss: 3.20421444943984\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 474, Batch: 0, Loss: 3.3315714818799025\n",
      "Epoch: 474, Batch: 20, Loss: 2.3615906396904682\n",
      "Epoch: 474, Batch: 40, Loss: 2.027527716457314\n",
      "Epoch: 474, Mean Loss: 3.1999376789662772\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 475, Batch: 0, Loss: 3.527952377624047\n",
      "Epoch: 475, Batch: 20, Loss: 2.2131216403951823\n",
      "Epoch: 475, Batch: 40, Loss: 2.02773261079251\n",
      "Epoch: 475, Mean Loss: 3.169259163326867\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 476, Batch: 0, Loss: 3.337280619764972\n",
      "Epoch: 476, Batch: 20, Loss: 2.2763176604708866\n",
      "Epoch: 476, Batch: 40, Loss: 2.026957318607013\n",
      "Epoch: 476, Mean Loss: 3.1608333464403633\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 477, Batch: 0, Loss: 3.236878755328529\n",
      "Epoch: 477, Batch: 20, Loss: 2.037406072585271\n",
      "Epoch: 477, Batch: 40, Loss: 2.1624224003480057\n",
      "Epoch: 477, Mean Loss: 3.138306526143172\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 478, Batch: 0, Loss: 3.3424406621014287\n",
      "Epoch: 478, Batch: 20, Loss: 2.0801819589997894\n",
      "Epoch: 478, Batch: 40, Loss: 2.0457164050016785\n",
      "Epoch: 478, Mean Loss: 3.1205206550509086\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 479, Batch: 0, Loss: 3.0607093472015476\n",
      "Epoch: 479, Batch: 20, Loss: 2.149002988421017\n",
      "Epoch: 479, Batch: 40, Loss: 2.0680036171292615\n",
      "Epoch: 479, Mean Loss: 3.1088493968079325\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 480, Batch: 0, Loss: 3.1771886812361685\n",
      "Epoch: 480, Batch: 20, Loss: 2.083483224542784\n",
      "Epoch: 480, Batch: 40, Loss: 2.1962790685476894\n",
      "Epoch: 480, Mean Loss: 3.1056797870078943\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 481, Batch: 0, Loss: 3.091271690670576\n",
      "Epoch: 481, Batch: 20, Loss: 2.0034156485101637\n",
      "Epoch: 481, Batch: 40, Loss: 2.145176360415954\n",
      "Epoch: 481, Mean Loss: 3.1066892124682948\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 482, Batch: 0, Loss: 3.1268542359271847\n",
      "Epoch: 482, Batch: 20, Loss: 2.1533905500128703\n",
      "Epoch: 482, Batch: 40, Loss: 2.2715178232364144\n",
      "Epoch: 482, Mean Loss: 3.1120975118738987\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 483, Batch: 0, Loss: 2.897690944804797\n",
      "Epoch: 483, Batch: 20, Loss: 2.011597506663125\n",
      "Epoch: 483, Batch: 40, Loss: 2.098566941797435\n",
      "Epoch: 483, Mean Loss: 3.097169601499857\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 484, Batch: 0, Loss: 3.0540545501474567\n",
      "Epoch: 484, Batch: 20, Loss: 2.084257496548406\n",
      "Epoch: 484, Batch: 40, Loss: 2.051640721450519\n",
      "Epoch: 484, Mean Loss: 3.1089310932622887\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 485, Batch: 0, Loss: 2.992714288494898\n",
      "Epoch: 485, Batch: 20, Loss: 2.105207553449278\n",
      "Epoch: 485, Batch: 40, Loss: 2.0621214952474833\n",
      "Epoch: 485, Mean Loss: 3.1090162379796196\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 486, Batch: 0, Loss: 3.143903957093981\n",
      "Epoch: 486, Batch: 20, Loss: 2.1139150821202044\n",
      "Epoch: 486, Batch: 40, Loss: 2.0391356891490333\n",
      "Epoch: 486, Mean Loss: 3.128829583002753\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 487, Batch: 0, Loss: 3.1845638157050304\n",
      "Epoch: 487, Batch: 20, Loss: 2.0205252767624704\n",
      "Epoch: 487, Batch: 40, Loss: 2.09752073589158\n",
      "Epoch: 487, Mean Loss: 3.1276906092003327\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 488, Batch: 0, Loss: 3.17044785981578\n",
      "Epoch: 488, Batch: 20, Loss: 2.1412565168438906\n",
      "Epoch: 488, Batch: 40, Loss: 2.0182535786747677\n",
      "Epoch: 488, Mean Loss: 3.110416886308841\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 489, Batch: 0, Loss: 3.049236842238187\n",
      "Epoch: 489, Batch: 20, Loss: 1.9841277584139343\n",
      "Epoch: 489, Batch: 40, Loss: 2.102031464701991\n",
      "Epoch: 489, Mean Loss: 3.123109099428354\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 490, Batch: 0, Loss: 3.243874463560156\n",
      "Epoch: 490, Batch: 20, Loss: 2.180181288331228\n",
      "Epoch: 490, Batch: 40, Loss: 2.080009051541787\n",
      "Epoch: 490, Mean Loss: 3.17353680605817\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 491, Batch: 0, Loss: 2.9921103009761594\n",
      "Epoch: 491, Batch: 20, Loss: 2.0898124141774947\n",
      "Epoch: 491, Batch: 40, Loss: 2.1802627525804734\n",
      "Epoch: 491, Mean Loss: 3.189618751727431\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 492, Batch: 0, Loss: 2.979548188133447\n",
      "Epoch: 492, Batch: 20, Loss: 2.231588042635791\n",
      "Epoch: 492, Batch: 40, Loss: 2.1376079369206265\n",
      "Epoch: 492, Mean Loss: 3.1566947198040554\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 493, Batch: 0, Loss: 3.0691460370812944\n",
      "Epoch: 493, Batch: 20, Loss: 2.2200573323541906\n",
      "Epoch: 493, Batch: 40, Loss: 2.0714309898649006\n",
      "Epoch: 493, Mean Loss: 3.1389085864543027\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 494, Batch: 0, Loss: 3.1737681282250825\n",
      "Epoch: 494, Batch: 20, Loss: 2.1206506049297853\n",
      "Epoch: 494, Batch: 40, Loss: 2.175469073291085\n",
      "Epoch: 494, Mean Loss: 3.1611556357739654\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 495, Batch: 0, Loss: 3.243052026008952\n",
      "Epoch: 495, Batch: 20, Loss: 2.0276088101368996\n",
      "Epoch: 495, Batch: 40, Loss: 2.029922552129296\n",
      "Epoch: 495, Mean Loss: 3.2087595420066988\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 496, Batch: 0, Loss: 3.006105123758705\n",
      "Epoch: 496, Batch: 20, Loss: 2.274441402090237\n",
      "Epoch: 496, Batch: 40, Loss: 2.0156400748461514\n",
      "Epoch: 496, Mean Loss: 3.2438079143033844\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 497, Batch: 0, Loss: 3.393380773169846\n",
      "Epoch: 497, Batch: 20, Loss: 2.518638307327495\n",
      "Epoch: 497, Batch: 40, Loss: 1.894078715039218\n",
      "Epoch: 497, Mean Loss: 3.379534779830769\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 498, Batch: 0, Loss: 3.190866580262463\n",
      "Epoch: 498, Batch: 20, Loss: 2.7451276446732256\n",
      "Epoch: 498, Batch: 40, Loss: 2.2343875672608484\n",
      "Epoch: 498, Mean Loss: 3.382784860339575\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 499, Batch: 0, Loss: 2.75947793607138\n",
      "Epoch: 499, Batch: 20, Loss: 2.300608100544269\n",
      "Epoch: 499, Batch: 40, Loss: 2.0912977659853857\n",
      "Epoch: 499, Mean Loss: 3.2748870226959266\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 500, Batch: 0, Loss: 3.1831268040664784\n",
      "Epoch: 500, Batch: 20, Loss: 2.868261581849827\n",
      "Epoch: 500, Batch: 40, Loss: 2.2680624595809196\n",
      "Epoch: 500, Mean Loss: 3.537725957914563\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 501, Batch: 0, Loss: 3.1565349752573586\n",
      "Epoch: 501, Batch: 20, Loss: 2.7951182412223052\n",
      "Epoch: 501, Batch: 40, Loss: 1.9375425197268072\n",
      "Epoch: 501, Mean Loss: 3.3408400633134625\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 502, Batch: 0, Loss: 2.9795053129143816\n",
      "Epoch: 502, Batch: 20, Loss: 2.2940534045731154\n",
      "Epoch: 502, Batch: 40, Loss: 1.8367073981616906\n",
      "Epoch: 502, Mean Loss: 3.2516144564868363\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 503, Batch: 0, Loss: 2.8151579931190684\n",
      "Epoch: 503, Batch: 20, Loss: 2.3218265386496144\n",
      "Epoch: 503, Batch: 40, Loss: 2.0757559810184474\n",
      "Epoch: 503, Mean Loss: 3.2526969120994123\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 504, Batch: 0, Loss: 3.1439698494087516\n",
      "Epoch: 504, Batch: 20, Loss: 2.6629423471006453\n",
      "Epoch: 504, Batch: 40, Loss: 1.9392379804883002\n",
      "Epoch: 504, Mean Loss: 3.2743294668473935\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 505, Batch: 0, Loss: 2.8459003171899617\n",
      "Epoch: 505, Batch: 20, Loss: 2.5725449886624197\n",
      "Epoch: 505, Batch: 40, Loss: 1.8681404647267195\n",
      "Epoch: 505, Mean Loss: 3.2139998262111438\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 506, Batch: 0, Loss: 2.9999336861135606\n",
      "Epoch: 506, Batch: 20, Loss: 2.731935518252418\n",
      "Epoch: 506, Batch: 40, Loss: 1.9238909912585136\n",
      "Epoch: 506, Mean Loss: 3.1841635270671884\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 507, Batch: 0, Loss: 2.7858982056630732\n",
      "Epoch: 507, Batch: 20, Loss: 2.164115461692146\n",
      "Epoch: 507, Batch: 40, Loss: 2.0777440249361843\n",
      "Epoch: 507, Mean Loss: 3.125721031874018\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 508, Batch: 0, Loss: 2.824256265346087\n",
      "Epoch: 508, Batch: 20, Loss: 2.7569705425002757\n",
      "Epoch: 508, Batch: 40, Loss: 1.9084751886160243\n",
      "Epoch: 508, Mean Loss: 3.25856733190114\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 509, Batch: 0, Loss: 2.9490115135996344\n",
      "Epoch: 509, Batch: 20, Loss: 2.5307688399822634\n",
      "Epoch: 509, Batch: 40, Loss: 2.452196483213129\n",
      "Epoch: 509, Mean Loss: 3.191062483070935\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 510, Batch: 0, Loss: 2.8469532697953555\n",
      "Epoch: 510, Batch: 20, Loss: 2.1508961350846487\n",
      "Epoch: 510, Batch: 40, Loss: 1.8188179180855832\n",
      "Epoch: 510, Mean Loss: 3.20759046854914\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 511, Batch: 0, Loss: 2.899747738387779\n",
      "Epoch: 511, Batch: 20, Loss: 2.7899218932434575\n",
      "Epoch: 511, Batch: 40, Loss: 1.9286701743864139\n",
      "Epoch: 511, Mean Loss: 3.2512943371590426\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 512, Batch: 0, Loss: 2.679031970696194\n",
      "Epoch: 512, Batch: 20, Loss: 2.4462388600037053\n",
      "Epoch: 512, Batch: 40, Loss: 1.9451234491682825\n",
      "Epoch: 512, Mean Loss: 3.125940874226997\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 513, Batch: 0, Loss: 2.9469569421824193\n",
      "Epoch: 513, Batch: 20, Loss: 2.4541665598361897\n",
      "Epoch: 513, Batch: 40, Loss: 1.863321578099581\n",
      "Epoch: 513, Mean Loss: 3.1350641491020217\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 514, Batch: 0, Loss: 2.7907990557530615\n",
      "Epoch: 514, Batch: 20, Loss: 2.6169552482296585\n",
      "Epoch: 514, Batch: 40, Loss: 1.9629395516750303\n",
      "Epoch: 514, Mean Loss: 3.1495667255950917\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 515, Batch: 0, Loss: 2.8046485101018312\n",
      "Epoch: 515, Batch: 20, Loss: 2.3750297820295927\n",
      "Epoch: 515, Batch: 40, Loss: 1.9279255082925837\n",
      "Epoch: 515, Mean Loss: 3.138795684677969\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 516, Batch: 0, Loss: 3.0164477660548137\n",
      "Epoch: 516, Batch: 20, Loss: 2.6852027155022915\n",
      "Epoch: 516, Batch: 40, Loss: 2.1666531272742313\n",
      "Epoch: 516, Mean Loss: 3.2437299239969035\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 517, Batch: 0, Loss: 2.86538090830025\n",
      "Epoch: 517, Batch: 20, Loss: 2.395246135597201\n",
      "Epoch: 517, Batch: 40, Loss: 2.297198023129952\n",
      "Epoch: 517, Mean Loss: 3.2101304182368358\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 518, Batch: 0, Loss: 3.02859256711668\n",
      "Epoch: 518, Batch: 20, Loss: 3.0774142200429737\n",
      "Epoch: 518, Batch: 40, Loss: 2.030336618351066\n",
      "Epoch: 518, Mean Loss: 3.3974298056614414\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 519, Batch: 0, Loss: 3.3777461601547785\n",
      "Epoch: 519, Batch: 20, Loss: 2.844423201661127\n",
      "Epoch: 519, Batch: 40, Loss: 3.270740943501599\n",
      "Epoch: 519, Mean Loss: 3.7889502911173434\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 520, Batch: 0, Loss: 3.7555596722907607\n",
      "Epoch: 520, Batch: 20, Loss: 2.642956099629555\n",
      "Epoch: 520, Batch: 40, Loss: 2.562768082848529\n",
      "Epoch: 520, Mean Loss: 3.6514044909214833\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 521, Batch: 0, Loss: 2.802789329495138\n",
      "Epoch: 521, Batch: 20, Loss: 2.539909146087828\n",
      "Epoch: 521, Batch: 40, Loss: 2.1994674622570463\n",
      "Epoch: 521, Mean Loss: 3.4244573389927506\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 522, Batch: 0, Loss: 3.0812656356275725\n",
      "Epoch: 522, Batch: 20, Loss: 2.807384342455758\n",
      "Epoch: 522, Batch: 40, Loss: 2.210254202316014\n",
      "Epoch: 522, Mean Loss: 3.6510231130214503\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 523, Batch: 0, Loss: 3.2087856930414618\n",
      "Epoch: 523, Batch: 20, Loss: 2.2691539179869444\n",
      "Epoch: 523, Batch: 40, Loss: 1.891219573150683\n",
      "Epoch: 523, Mean Loss: 3.2882749471540387\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 524, Batch: 0, Loss: 3.3622103564136987\n",
      "Epoch: 524, Batch: 20, Loss: 2.4554111691842415\n",
      "Epoch: 524, Batch: 40, Loss: 1.84978106860269\n",
      "Epoch: 524, Mean Loss: 3.2654174066026753\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 525, Batch: 0, Loss: 3.207546590117893\n",
      "Epoch: 525, Batch: 20, Loss: 2.557289306756505\n",
      "Epoch: 525, Batch: 40, Loss: 2.3293876494465784\n",
      "Epoch: 525, Mean Loss: 3.24436059903843\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 526, Batch: 0, Loss: 3.0790198142917182\n",
      "Epoch: 526, Batch: 20, Loss: 2.8166541052927347\n",
      "Epoch: 526, Batch: 40, Loss: 1.723134497933982\n",
      "Epoch: 526, Mean Loss: 3.280822462216412\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 527, Batch: 0, Loss: 3.700343174691793\n",
      "Epoch: 527, Batch: 20, Loss: 2.8590121326153017\n",
      "Epoch: 527, Batch: 40, Loss: 1.5678193214679648\n",
      "Epoch: 527, Mean Loss: 3.3551029102914485\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 528, Batch: 0, Loss: 3.6995045208911677\n",
      "Epoch: 528, Batch: 20, Loss: 2.9472607279132657\n",
      "Epoch: 528, Batch: 40, Loss: 1.5886142510959662\n",
      "Epoch: 528, Mean Loss: 3.406768108852969\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 529, Batch: 0, Loss: 3.23280743745523\n",
      "Epoch: 529, Batch: 20, Loss: 2.107509395066443\n",
      "Epoch: 529, Batch: 40, Loss: 1.7529525924167435\n",
      "Epoch: 529, Mean Loss: 3.1757946842811817\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 530, Batch: 0, Loss: 3.212829270239631\n",
      "Epoch: 530, Batch: 20, Loss: 2.1095509681799784\n",
      "Epoch: 530, Batch: 40, Loss: 1.670503036990925\n",
      "Epoch: 530, Mean Loss: 3.2266107453295536\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 531, Batch: 0, Loss: 3.717219629985554\n",
      "Epoch: 531, Batch: 20, Loss: 2.0294758472021246\n",
      "Epoch: 531, Batch: 40, Loss: 2.1052182213561097\n",
      "Epoch: 531, Mean Loss: 3.27833792609499\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 532, Batch: 0, Loss: 2.972812206576013\n",
      "Epoch: 532, Batch: 20, Loss: 2.199677675659085\n",
      "Epoch: 532, Batch: 40, Loss: 1.9421629693649702\n",
      "Epoch: 532, Mean Loss: 3.2591173135820926\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 533, Batch: 0, Loss: 3.16322327070451\n",
      "Epoch: 533, Batch: 20, Loss: 2.0579059123711567\n",
      "Epoch: 533, Batch: 40, Loss: 1.7771560224030396\n",
      "Epoch: 533, Mean Loss: 3.139869196325392\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 534, Batch: 0, Loss: 3.4084912264019214\n",
      "Epoch: 534, Batch: 20, Loss: 2.2137066529932223\n",
      "Epoch: 534, Batch: 40, Loss: 1.579259510127067\n",
      "Epoch: 534, Mean Loss: 3.1782531867396795\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 535, Batch: 0, Loss: 3.4345586495876996\n",
      "Epoch: 535, Batch: 20, Loss: 2.2389973948784103\n",
      "Epoch: 535, Batch: 40, Loss: 1.5300236343281726\n",
      "Epoch: 535, Mean Loss: 3.1830205174082318\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 536, Batch: 0, Loss: 3.304909955370706\n",
      "Epoch: 536, Batch: 20, Loss: 2.070484404803845\n",
      "Epoch: 536, Batch: 40, Loss: 1.57209016433147\n",
      "Epoch: 536, Mean Loss: 3.163143012046914\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 537, Batch: 0, Loss: 3.284063891092833\n",
      "Epoch: 537, Batch: 20, Loss: 2.107164586025252\n",
      "Epoch: 537, Batch: 40, Loss: 1.6235091036087566\n",
      "Epoch: 537, Mean Loss: 3.186413454520062\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 538, Batch: 0, Loss: 3.6979715914406737\n",
      "Epoch: 538, Batch: 20, Loss: 2.1791282237801446\n",
      "Epoch: 538, Batch: 40, Loss: 1.6576867093230516\n",
      "Epoch: 538, Mean Loss: 3.194040475282586\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 539, Batch: 0, Loss: 3.002031311295642\n",
      "Epoch: 539, Batch: 20, Loss: 1.9881377058925387\n",
      "Epoch: 539, Batch: 40, Loss: 1.7853871681461655\n",
      "Epoch: 539, Mean Loss: 3.1472640532529845\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 540, Batch: 0, Loss: 3.355548844251173\n",
      "Epoch: 540, Batch: 20, Loss: 2.091294884260996\n",
      "Epoch: 540, Batch: 40, Loss: 1.553224831371797\n",
      "Epoch: 540, Mean Loss: 3.1794585067116468\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 541, Batch: 0, Loss: 3.000684081465171\n",
      "Epoch: 541, Batch: 20, Loss: 2.217132206530427\n",
      "Epoch: 541, Batch: 40, Loss: 1.70595206719256\n",
      "Epoch: 541, Mean Loss: 3.1673182169062937\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 542, Batch: 0, Loss: 2.9929928112835857\n",
      "Epoch: 542, Batch: 20, Loss: 2.236234416902103\n",
      "Epoch: 542, Batch: 40, Loss: 1.6999350393171941\n",
      "Epoch: 542, Mean Loss: 3.1089623100138426\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 543, Batch: 0, Loss: 2.9528195223183755\n",
      "Epoch: 543, Batch: 20, Loss: 2.05210404558944\n",
      "Epoch: 543, Batch: 40, Loss: 1.8560275895386789\n",
      "Epoch: 543, Mean Loss: 3.101395343877802\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 544, Batch: 0, Loss: 3.022484892244535\n",
      "Epoch: 544, Batch: 20, Loss: 2.102975665897796\n",
      "Epoch: 544, Batch: 40, Loss: 1.930271491276085\n",
      "Epoch: 544, Mean Loss: 3.080670174645772\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 545, Batch: 0, Loss: 3.1017054629559224\n",
      "Epoch: 545, Batch: 20, Loss: 2.0542848299005363\n",
      "Epoch: 545, Batch: 40, Loss: 1.8563726634308921\n",
      "Epoch: 545, Mean Loss: 3.076238426206598\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 546, Batch: 0, Loss: 2.900114219181691\n",
      "Epoch: 546, Batch: 20, Loss: 2.0827968588021895\n",
      "Epoch: 546, Batch: 40, Loss: 1.8768370299956194\n",
      "Epoch: 546, Mean Loss: 3.070576212011103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 547, Batch: 0, Loss: 2.9472113659499\n",
      "Epoch: 547, Batch: 20, Loss: 2.237249845834753\n",
      "Epoch: 547, Batch: 40, Loss: 1.9060297211047943\n",
      "Epoch: 547, Mean Loss: 3.143423715188634\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 548, Batch: 0, Loss: 3.086897882043956\n",
      "Epoch: 548, Batch: 20, Loss: 2.01403416043142\n",
      "Epoch: 548, Batch: 40, Loss: 2.0413253610418196\n",
      "Epoch: 548, Mean Loss: 3.174798690536464\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 549, Batch: 0, Loss: 3.044481494270276\n",
      "Epoch: 549, Batch: 20, Loss: 2.187181900682958\n",
      "Epoch: 549, Batch: 40, Loss: 1.9965559351381732\n",
      "Epoch: 549, Mean Loss: 3.144997199428525\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 550, Batch: 0, Loss: 3.1700857983041764\n",
      "Epoch: 550, Batch: 20, Loss: 2.1713636015291913\n",
      "Epoch: 550, Batch: 40, Loss: 1.8474658408161957\n",
      "Epoch: 550, Mean Loss: 3.128962742632277\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 551, Batch: 0, Loss: 3.2399762181941405\n",
      "Epoch: 551, Batch: 20, Loss: 2.1027390640459975\n",
      "Epoch: 551, Batch: 40, Loss: 1.9257591818034054\n",
      "Epoch: 551, Mean Loss: 3.1245621998333126\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 552, Batch: 0, Loss: 2.9074853818502295\n",
      "Epoch: 552, Batch: 20, Loss: 2.0161812778440407\n",
      "Epoch: 552, Batch: 40, Loss: 1.863370300374235\n",
      "Epoch: 552, Mean Loss: 3.1270805276121485\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 553, Batch: 0, Loss: 3.111912752514086\n",
      "Epoch: 553, Batch: 20, Loss: 1.9221295938271186\n",
      "Epoch: 553, Batch: 40, Loss: 1.7964042460304894\n",
      "Epoch: 553, Mean Loss: 3.1372171548387575\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 554, Batch: 0, Loss: 3.0499855714765056\n",
      "Epoch: 554, Batch: 20, Loss: 2.3667188563511736\n",
      "Epoch: 554, Batch: 40, Loss: 1.9393992174994894\n",
      "Epoch: 554, Mean Loss: 3.2398820728249484\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 555, Batch: 0, Loss: 3.0261858517397373\n",
      "Epoch: 555, Batch: 20, Loss: 2.0718366844120957\n",
      "Epoch: 555, Batch: 40, Loss: 1.8904728778948805\n",
      "Epoch: 555, Mean Loss: 3.170913105699292\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 556, Batch: 0, Loss: 3.2210176692674137\n",
      "Epoch: 556, Batch: 20, Loss: 2.137649373234145\n",
      "Epoch: 556, Batch: 40, Loss: 1.95252643303197\n",
      "Epoch: 556, Mean Loss: 3.1255742464365737\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 557, Batch: 0, Loss: 2.970115347838537\n",
      "Epoch: 557, Batch: 20, Loss: 1.9607182593515193\n",
      "Epoch: 557, Batch: 40, Loss: 2.1684122315731096\n",
      "Epoch: 557, Mean Loss: 3.1339136214364576\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 558, Batch: 0, Loss: 3.2610947230858653\n",
      "Epoch: 558, Batch: 20, Loss: 2.046250034974522\n",
      "Epoch: 558, Batch: 40, Loss: 2.263903305669223\n",
      "Epoch: 558, Mean Loss: 3.1705713126824224\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 559, Batch: 0, Loss: 3.1119194925315736\n",
      "Epoch: 559, Batch: 20, Loss: 1.8294096219950189\n",
      "Epoch: 559, Batch: 40, Loss: 2.241993188910985\n",
      "Epoch: 559, Mean Loss: 3.130677160031164\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 560, Batch: 0, Loss: 3.224111583628939\n",
      "Epoch: 560, Batch: 20, Loss: 2.0898697007803646\n",
      "Epoch: 560, Batch: 40, Loss: 2.2768752116829383\n",
      "Epoch: 560, Mean Loss: 3.106559285496163\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 561, Batch: 0, Loss: 3.0578531658988877\n",
      "Epoch: 561, Batch: 20, Loss: 2.0818209291540644\n",
      "Epoch: 561, Batch: 40, Loss: 2.0731618342074762\n",
      "Epoch: 561, Mean Loss: 3.150269327413956\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 562, Batch: 0, Loss: 3.198629763246981\n",
      "Epoch: 562, Batch: 20, Loss: 2.0626304568176854\n",
      "Epoch: 562, Batch: 40, Loss: 2.2760794412692276\n",
      "Epoch: 562, Mean Loss: 3.1619493934764744\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 563, Batch: 0, Loss: 3.116993981741166\n",
      "Epoch: 563, Batch: 20, Loss: 2.064029196038075\n",
      "Epoch: 563, Batch: 40, Loss: 2.4993025688092927\n",
      "Epoch: 563, Mean Loss: 3.385600689505527\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 564, Batch: 0, Loss: 3.068892451053583\n",
      "Epoch: 564, Batch: 20, Loss: 2.7243548152737938\n",
      "Epoch: 564, Batch: 40, Loss: 1.7431695393546447\n",
      "Epoch: 564, Mean Loss: 3.81080027336729\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 565, Batch: 0, Loss: 4.7106011957195335\n",
      "Epoch: 565, Batch: 20, Loss: 3.025150780448217\n",
      "Epoch: 565, Batch: 40, Loss: 1.7659371277963234\n",
      "Epoch: 565, Mean Loss: 3.6388422811059002\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 566, Batch: 0, Loss: 4.050060055582082\n",
      "Epoch: 566, Batch: 20, Loss: 2.322748536933216\n",
      "Epoch: 566, Batch: 40, Loss: 1.727331416515197\n",
      "Epoch: 566, Mean Loss: 3.5235084607525664\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 567, Batch: 0, Loss: 3.7318657646714635\n",
      "Epoch: 567, Batch: 20, Loss: 2.2737992525665405\n",
      "Epoch: 567, Batch: 40, Loss: 1.690996723599955\n",
      "Epoch: 567, Mean Loss: 3.3315472957252665\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 568, Batch: 0, Loss: 3.9002381144190883\n",
      "Epoch: 568, Batch: 20, Loss: 2.8461345554678905\n",
      "Epoch: 568, Batch: 40, Loss: 1.7610835566787968\n",
      "Epoch: 568, Mean Loss: 3.4751842255393335\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 569, Batch: 0, Loss: 3.9116895696221485\n",
      "Epoch: 569, Batch: 20, Loss: 2.12323726740969\n",
      "Epoch: 569, Batch: 40, Loss: 2.006131119761673\n",
      "Epoch: 569, Mean Loss: 3.1890414188635585\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 570, Batch: 0, Loss: 3.267726585545851\n",
      "Epoch: 570, Batch: 20, Loss: 1.913254780890139\n",
      "Epoch: 570, Batch: 40, Loss: 2.1756315006289486\n",
      "Epoch: 570, Mean Loss: 3.106970001646589\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 571, Batch: 0, Loss: 3.512457176113889\n",
      "Epoch: 571, Batch: 20, Loss: 1.9661330041963834\n",
      "Epoch: 571, Batch: 40, Loss: 2.159316996417407\n",
      "Epoch: 571, Mean Loss: 3.1355203032296863\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 572, Batch: 0, Loss: 3.41526444210922\n",
      "Epoch: 572, Batch: 20, Loss: 2.0479451311653127\n",
      "Epoch: 572, Batch: 40, Loss: 2.3055226390624117\n",
      "Epoch: 572, Mean Loss: 3.160823794831723\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 573, Batch: 0, Loss: 3.2945257793926896\n",
      "Epoch: 573, Batch: 20, Loss: 2.067175935875032\n",
      "Epoch: 573, Batch: 40, Loss: 2.5460429451238156\n",
      "Epoch: 573, Mean Loss: 3.2736461089083253\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 574, Batch: 0, Loss: 3.448255544350494\n",
      "Epoch: 574, Batch: 20, Loss: 1.9820421449941088\n",
      "Epoch: 574, Batch: 40, Loss: 2.187541576357812\n",
      "Epoch: 574, Mean Loss: 3.244949398963597\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 575, Batch: 0, Loss: 2.970011005520644\n",
      "Epoch: 575, Batch: 20, Loss: 1.998958934178332\n",
      "Epoch: 575, Batch: 40, Loss: 2.265108986774028\n",
      "Epoch: 575, Mean Loss: 3.1561599159639013\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 576, Batch: 0, Loss: 3.3654648654454475\n",
      "Epoch: 576, Batch: 20, Loss: 2.129142157151685\n",
      "Epoch: 576, Batch: 40, Loss: 2.076846973981388\n",
      "Epoch: 576, Mean Loss: 3.1754299237618513\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 577, Batch: 0, Loss: 3.0456015038519544\n",
      "Epoch: 577, Batch: 20, Loss: 1.969430702273708\n",
      "Epoch: 577, Batch: 40, Loss: 2.15616002068946\n",
      "Epoch: 577, Mean Loss: 3.118044433012721\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 578, Batch: 0, Loss: 3.2240364016821537\n",
      "Epoch: 578, Batch: 20, Loss: 2.0407377766809818\n",
      "Epoch: 578, Batch: 40, Loss: 2.1111671385814836\n",
      "Epoch: 578, Mean Loss: 3.1300102910333694\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 579, Batch: 0, Loss: 3.086076299043003\n",
      "Epoch: 579, Batch: 20, Loss: 2.1026565860280066\n",
      "Epoch: 579, Batch: 40, Loss: 1.9424817704838935\n",
      "Epoch: 579, Mean Loss: 3.091379324133616\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 580, Batch: 0, Loss: 3.3301961750671114\n",
      "Epoch: 580, Batch: 20, Loss: 2.252577080701118\n",
      "Epoch: 580, Batch: 40, Loss: 1.6477135734752248\n",
      "Epoch: 580, Mean Loss: 3.148914886081245\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 581, Batch: 0, Loss: 3.152176228794572\n",
      "Epoch: 581, Batch: 20, Loss: 2.256767394266057\n",
      "Epoch: 581, Batch: 40, Loss: 1.550312583058551\n",
      "Epoch: 581, Mean Loss: 3.1449451589263417\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 582, Batch: 0, Loss: 3.3451329568906902\n",
      "Epoch: 582, Batch: 20, Loss: 2.3572676998437974\n",
      "Epoch: 582, Batch: 40, Loss: 1.7572952390170051\n",
      "Epoch: 582, Mean Loss: 3.2242211707436383\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 583, Batch: 0, Loss: 3.249414225102286\n",
      "Epoch: 583, Batch: 20, Loss: 2.1123375258707857\n",
      "Epoch: 583, Batch: 40, Loss: 1.636974515952255\n",
      "Epoch: 583, Mean Loss: 3.122024344028662\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 584, Batch: 0, Loss: 3.5038757370272737\n",
      "Epoch: 584, Batch: 20, Loss: 2.0078092543059127\n",
      "Epoch: 584, Batch: 40, Loss: 2.0344726840632013\n",
      "Epoch: 584, Mean Loss: 3.1712407024229567\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 585, Batch: 0, Loss: 2.969918005954601\n",
      "Epoch: 585, Batch: 20, Loss: 2.20048137708063\n",
      "Epoch: 585, Batch: 40, Loss: 1.7248821599732507\n",
      "Epoch: 585, Mean Loss: 3.0990677772300006\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 586, Batch: 0, Loss: 3.1755726283233248\n",
      "Epoch: 586, Batch: 20, Loss: 1.9009538575682388\n",
      "Epoch: 586, Batch: 40, Loss: 1.6146787325833005\n",
      "Epoch: 586, Mean Loss: 3.1038111369653945\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 587, Batch: 0, Loss: 3.593678774916307\n",
      "Epoch: 587, Batch: 20, Loss: 2.068484262941298\n",
      "Epoch: 587, Batch: 40, Loss: 1.7089892492036811\n",
      "Epoch: 587, Mean Loss: 3.1006510349144647\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 588, Batch: 0, Loss: 3.372051706581586\n",
      "Epoch: 588, Batch: 20, Loss: 2.0098292648876064\n",
      "Epoch: 588, Batch: 40, Loss: 1.8034261099707207\n",
      "Epoch: 588, Mean Loss: 3.105011024171067\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 589, Batch: 0, Loss: 2.960309305832358\n",
      "Epoch: 589, Batch: 20, Loss: 1.9905508759985264\n",
      "Epoch: 589, Batch: 40, Loss: 1.5926586234221127\n",
      "Epoch: 589, Mean Loss: 3.0442490070079127\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 590, Batch: 0, Loss: 3.1374247778262\n",
      "Epoch: 590, Batch: 20, Loss: 2.09393472671746\n",
      "Epoch: 590, Batch: 40, Loss: 1.7247346186768093\n",
      "Epoch: 590, Mean Loss: 3.028644581388552\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 591, Batch: 0, Loss: 3.283346836966862\n",
      "Epoch: 591, Batch: 20, Loss: 2.0137123531869423\n",
      "Epoch: 591, Batch: 40, Loss: 1.6149953863956368\n",
      "Epoch: 591, Mean Loss: 3.0590363483241383\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 592, Batch: 0, Loss: 3.0536612587816765\n",
      "Epoch: 592, Batch: 20, Loss: 1.8933446382315113\n",
      "Epoch: 592, Batch: 40, Loss: 1.864789736676227\n",
      "Epoch: 592, Mean Loss: 3.017387133695788\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 593, Batch: 0, Loss: 3.092310074821887\n",
      "Epoch: 593, Batch: 20, Loss: 1.8684561067156356\n",
      "Epoch: 593, Batch: 40, Loss: 1.7968619404413646\n",
      "Epoch: 593, Mean Loss: 3.0261771841347427\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 594, Batch: 0, Loss: 3.0762907482829025\n",
      "Epoch: 594, Batch: 20, Loss: 1.9145034555532532\n",
      "Epoch: 594, Batch: 40, Loss: 2.14086710149604\n",
      "Epoch: 594, Mean Loss: 2.9957898252431083\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 595, Batch: 0, Loss: 2.7805000564523676\n",
      "Epoch: 595, Batch: 20, Loss: 1.9800048987759686\n",
      "Epoch: 595, Batch: 40, Loss: 2.0638302255310434\n",
      "Epoch: 595, Mean Loss: 3.057757092465438\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 596, Batch: 0, Loss: 3.0590838874119863\n",
      "Epoch: 596, Batch: 20, Loss: 2.2411519733705965\n",
      "Epoch: 596, Batch: 40, Loss: 2.1209637070876277\n",
      "Epoch: 596, Mean Loss: 3.044466958754606\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 597, Batch: 0, Loss: 3.060648140244241\n",
      "Epoch: 597, Batch: 20, Loss: 2.042169318444727\n",
      "Epoch: 597, Batch: 40, Loss: 1.8251166210321148\n",
      "Epoch: 597, Mean Loss: 3.0427582628384404\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 598, Batch: 0, Loss: 3.15327365064432\n",
      "Epoch: 598, Batch: 20, Loss: 2.0169528053596206\n",
      "Epoch: 598, Batch: 40, Loss: 1.9785465704212986\n",
      "Epoch: 598, Mean Loss: 3.107277209765133\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 599, Batch: 0, Loss: 2.8400186229677256\n",
      "Epoch: 599, Batch: 20, Loss: 1.904146351504018\n",
      "Epoch: 599, Batch: 40, Loss: 1.823014901424095\n",
      "Epoch: 599, Mean Loss: 3.0545930770559244\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "epoch_train_loss = []\n",
    "num_epochs = 600\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    # mean_ade = []\n",
    "    # mean_fde = []    \n",
    "    for batch_num, data in enumerate(train_loader):\n",
    "        traj_inp, traj_out, fixed_params, var_inp = data\n",
    "        traj_inp = traj_inp.to(device)\n",
    "        traj_out = traj_out.to(device)\n",
    "        fixed_params = fixed_params.to(device)\n",
    "        var_inp = var_inp.to(device)\n",
    "\n",
    "        # ade = []\n",
    "        # fde = []       \n",
    "        #print(traj_inp.shape, traj_out.shape, fixed_params.shape, var_inp.shape)\n",
    "        out = model(traj_inp, fixed_params, var_inp)\n",
    "        loss = criterion(out, traj_out)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "#         for ii in range(traj_inp.size()[0]):\n",
    "#             gt = [[out[ii][j],out[ii][j + num]] for j in range(len(out[ii])//2)]\n",
    "# #             print(out[ii][0])\n",
    "#             pred = [[traj_out[ii][j],traj_out[ii][j + num]] for j in range(len(out[ii])//2)]\n",
    "#             ade.append(get_ade(np.array(pred), np.array(gt)))\n",
    "#             fde.append(get_fde(np.array(pred), np.array(gt)))                        \n",
    "#             #plot_traj(ii, traj_inp[ii], traj_out[ii], out[ii], {\"x\": [], \"y\": []}, offsets=offsets_train, cities = [], avm=None, center=include_centerline, inp_len=t_obs * 2, c_len = t_obs * 2 + num_elems * 2, num=num, mode=\"test\", batch_num=batch_num)\n",
    "        if batch_num % 20 == 0:\n",
    "            print(\"Epoch: {}, Batch: {}, Loss: {}\".format(epoch, batch_num, loss.item()))\n",
    "            #print(\"ADE: {}\".format(np.mean(ade)), \"FDE: {}\".format(np.mean(fde)))\n",
    "    \n",
    "        # mean_ade.append(np.mean(ade))\n",
    "        # mean_fde.append(np.mean(fde))\n",
    "    \n",
    "    mean_loss = np.mean(train_loss)\n",
    "    epoch_train_loss.append(mean_loss)\n",
    "    #torch.save(model.state_dict(), \"./checkpoints/{}.ckpt\".format(name))\n",
    "    print(\"Epoch: {}, Mean Loss: {}\".format(epoch, mean_loss))\n",
    "    #print(\"Mean ADE: {}\".format(np.mean(mean_ade)), \"Mean FDE: {}\".format(np.mean(mean_fde)))\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce1b9f92-8052-4461-b9b5-30d4241939bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'lstmv2_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
