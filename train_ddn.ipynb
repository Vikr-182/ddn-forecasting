{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"./ddn/\")\n",
    "sys.path.append(\"./\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from bernstein import bernstesin_coeff_order10_new\n",
    "from utils import *\n",
    "from ddn.pytorch.node import AbstractDeclarativeNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/vikrant/.local/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/vikrant/.local/lib/python3.8/site-packages (from pandas) (1.19.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas) (2.7.3)\n"
     ]
    }
   ],
   "source": [
    "!TMPDIR=/home/vikrant pip install --cache-dir=/home/vikrant/ --build /home/vikrant/ pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernstein_coeff_order10_new(n, tmin, tmax, t_actual):\n",
    "    l = tmax - tmin\n",
    "    t = (t_actual - tmin) / l\n",
    "\n",
    "    P0 = scipy.special.binom(n, 0) * ((1 - t) ** (n - 0)) * t ** 0\n",
    "    P1 = scipy.special.binom(n, 1) * ((1 - t) ** (n - 1)) * t ** 1\n",
    "    P2 = scipy.special.binom(n, 2) * ((1 - t) ** (n - 2)) * t ** 2\n",
    "    P3 = scipy.special.binom(n, 3) * ((1 - t) ** (n - 3)) * t ** 3\n",
    "    P4 = scipy.special.binom(n, 4) * ((1 - t) ** (n - 4)) * t ** 4\n",
    "    P5 = scipy.special.binom(n, 5) * ((1 - t) ** (n - 5)) * t ** 5\n",
    "    P6 = scipy.special.binom(n, 6) * ((1 - t) ** (n - 6)) * t ** 6\n",
    "    P7 = scipy.special.binom(n, 7) * ((1 - t) ** (n - 7)) * t ** 7\n",
    "    P8 = scipy.special.binom(n, 8) * ((1 - t) ** (n - 8)) * t ** 8\n",
    "    P9 = scipy.special.binom(n, 9) * ((1 - t) ** (n - 9)) * t ** 9\n",
    "    P10 = scipy.special.binom(n, 10) * ((1 - t) ** (n - 10)) * t ** 10\n",
    "\n",
    "    P0dot = -10.0 * (-t + 1) ** 9\n",
    "    P1dot = -90.0 * t * (-t + 1) ** 8 + 10.0 * (-t + 1) ** 9\n",
    "    P2dot = -360.0 * t ** 2 * (-t + 1) ** 7 + 90.0 * t * (-t + 1) ** 8\n",
    "    P3dot = -840.0 * t ** 3 * (-t + 1) ** 6 + 360.0 * t ** 2 * (-t + 1) ** 7\n",
    "    P4dot = -1260.0 * t ** 4 * (-t + 1) ** 5 + 840.0 * t ** 3 * (-t + 1) ** 6\n",
    "    P5dot = -1260.0 * t ** 5 * (-t + 1) ** 4 + 1260.0 * t ** 4 * (-t + 1) ** 5\n",
    "    P6dot = -840.0 * t ** 6 * (-t + 1) ** 3 + 1260.0 * t ** 5 * (-t + 1) ** 4\n",
    "    P7dot = -360.0 * t ** 7 * (-t + 1) ** 2 + 840.0 * t ** 6 * (-t + 1) ** 3\n",
    "    P8dot = 45.0 * t ** 8 * (2 * t - 2) + 360.0 * t ** 7 * (-t + 1) ** 2\n",
    "    P9dot = -10.0 * t ** 9 + 9 * t ** 8 * (-10.0 * t + 10.0)\n",
    "    P10dot = 10.0 * t ** 9\n",
    "\n",
    "    P0ddot = 90.0 * (-t + 1) ** 8\n",
    "    P1ddot = 720.0 * t * (-t + 1) ** 7 - 180.0 * (-t + 1) ** 8\n",
    "    P2ddot = 2520.0 * t ** 2 * (-t + 1) ** 6 - 1440.0 * t * (-t + 1) ** 7 + 90.0 * (-t + 1) ** 8\n",
    "    P3ddot = 5040.0 * t ** 3 * (-t + 1) ** 5 - 5040.0 * t ** 2 * (-t + 1) ** 6 + 720.0 * t * (-t + 1) ** 7\n",
    "    P4ddot = 6300.0 * t ** 4 * (-t + 1) ** 4 - 10080.0 * t ** 3 * (-t + 1) ** 5 + 2520.0 * t ** 2 * (-t + 1) ** 6\n",
    "    P5ddot = 5040.0 * t ** 5 * (-t + 1) ** 3 - 12600.0 * t ** 4 * (-t + 1) ** 4 + 5040.0 * t ** 3 * (-t + 1) ** 5\n",
    "    P6ddot = 2520.0 * t ** 6 * (-t + 1) ** 2 - 10080.0 * t ** 5 * (-t + 1) ** 3 + 6300.0 * t ** 4 * (-t + 1) ** 4\n",
    "    P7ddot = -360.0 * t ** 7 * (2 * t - 2) - 5040.0 * t ** 6 * (-t + 1) ** 2 + 5040.0 * t ** 5 * (-t + 1) ** 3\n",
    "    P8ddot = 90.0 * t ** 8 + 720.0 * t ** 7 * (2 * t - 2) + 2520.0 * t ** 6 * (-t + 1) ** 2\n",
    "    P9ddot = -180.0 * t ** 8 + 72 * t ** 7 * (-10.0 * t + 10.0)\n",
    "    P10ddot = 90.0 * t ** 8\n",
    "    90.0 * t ** 8\n",
    "\n",
    "    P = np.hstack((P0, P1, P2, P3, P4, P5, P6, P7, P8, P9, P10))\n",
    "    Pdot = np.hstack((P0dot, P1dot, P2dot, P3dot, P4dot, P5dot, P6dot, P7dot, P8dot, P9dot, P10dot)) / l\n",
    "    Pddot = np.hstack((P0ddot, P1ddot, P2ddot, P3ddot, P4ddot, P5ddot, P6ddot, P7ddot, P8ddot, P9ddot, P10ddot)) / (l ** 2)\n",
    "    return P, Pdot, Pddot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traj(cnt, traj_inp, traj_out, traj_pred, obs):\n",
    "    traj_inp = traj_inp.numpy()[:2*num]\n",
    "    traj_out = traj_out.numpy()\n",
    "    traj_pred = traj_pred.detach().numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax.scatter(traj_inp[::2], traj_inp[1::2], label='Inp traj')\n",
    "    ax.scatter(traj_out[:num], traj_out[num:], label='GT')\n",
    "    ax.scatter(traj_pred[:num], traj_pred[num:], label='Pred')\n",
    "#     ax.scatter(obs[\"x\"], obs[\"y\"], s=20, label=\"Obstacles\")\n",
    "    \n",
    "    x_obs_temp = obs[\"x\"]\n",
    "    y_obs_temp = obs[\"y\"]\n",
    "    \n",
    "    th = np.linspace(0, 2 * np.pi, 100)\n",
    "    for i in range(0, num_obs):\n",
    "        x_circ = x_obs_temp[i] + a_obs * np.cos(th)\n",
    "        y_circ = y_obs_temp[i] + b_obs * np.sin(th)\n",
    "        ax.plot(x_circ, y_circ, '-k')\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_xlim([-7, 7])\n",
    "    ax.set_ylim([-7, 7])\n",
    "    plt.savefig('./results/{}.png'.format(cnt))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rho_obs = 0.3\n",
    "rho_obs = 1.2\n",
    "rho_eq = 10.0\n",
    "weight_smoothness = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 10\n",
    "t_fin = 2\n",
    "a_obs = 1.0\n",
    "b_obs = 1.0\n",
    "\n",
    "tot_time = np.linspace(0.0, t_fin, num)\n",
    "tot_time_copy = tot_time.reshape(num, 1)\n",
    "P, Pdot, Pddot = bernstein_coeff_order10_new(10, tot_time_copy[0], tot_time_copy[-1], tot_time_copy)\n",
    "nvar = np.shape(P)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2. , -2. , -2. , -2. , -2. , -2. , -2. , -2. , -2. , -2. ],\n",
       "       [ 2. ,  2. ,  2. ,  2. ,  2. ,  2. ,  2. ,  2. ,  2. ,  2. ],\n",
       "       [-0.8, -0.8, -0.8, -0.8, -0.8, -0.8, -0.8, -0.8, -0.8, -0.8],\n",
       "       [ 2. ,  2. ,  2. ,  2. ,  2. ,  2. ,  2. ,  2. ,  2. ,  2. ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_obs_temp = np.hstack((-10.0, -10.79, -10, -10))\n",
    "# y_obs_temp = np.hstack((10.0, 15.0, 15, 15))\n",
    "x_obs_temp = np.hstack((-2.0, -5.79, 3.0, 4.0))\n",
    "y_obs_temp = np.hstack((-2.0, 2.0, -0.80, 2.0))\n",
    "num_obs = np.shape(x_obs_temp)[0]\n",
    "\n",
    "x_obs = np.ones((num_obs, num)) * x_obs_temp[:, np.newaxis]\n",
    "y_obs = np.ones((num_obs, num)) * y_obs_temp[:, np.newaxis]\n",
    "x_obs\n",
    "y_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.  , -2.  ,  1.  , -5.79,  2.  ,  1.  ,  3.  , -0.8 ,  1.  ,\n",
       "        4.  ,  2.  ,  1.  ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_inp_list = []\n",
    "for i in range(num_obs):\n",
    "    obs_inp_list.extend([x_obs_temp[i], y_obs_temp[i], a_obs])\n",
    "\n",
    "obs_inp = np.array(obs_inp_list)\n",
    "obs_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_obs = np.tile(P, (num_obs, 1))\n",
    "A_eq = np.vstack((P[0], Pdot[0], Pddot[0], P[-1], Pdot[-1], Pddot[-1]))\n",
    "Q_smoothness = np.dot(Pddot.T, Pddot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot({\"x\":x, \"y\":y}, {\"x\":x_obs_temp, \"y\":y_obs_temp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPTNode(AbstractDeclarativeNode):\n",
    "    def __init__(self, rho_eq=1.0, rho_goal=1.0, rho_lane=1.0, rho_nonhol=1.0, rho_w_psi=1.0, rho_psi=1.0, maxiter=500, weight_smoothness=1.0, weight_smoothness_psi=1.0, t_fin=8.0, num=100, rho_mid=0.01):\n",
    "        super().__init__()\n",
    "        self.rho_eq = rho_eq\n",
    "        self.rho_goal = rho_goal\n",
    "        self.rho_lane = rho_lane\n",
    "        self.rho_nonhol = rho_nonhol\n",
    "        self.rho_w_psi = rho_w_psi\n",
    "        self.rho_psi = rho_psi\n",
    "        self.maxiter = maxiter\n",
    "        self.weight_smoothness = weight_smoothness\n",
    "        self.weight_smoothness_psi = weight_smoothness_psi\n",
    "\n",
    "        self.t_fin = t_fin\n",
    "        self.num = num\n",
    "        self.t = self.t_fin / self.num\n",
    "\n",
    "        tot_time = torch.linspace(0.0, self.t_fin, self.num).to(device)\n",
    "        tot_time_copy = tot_time.reshape(self.num, 1)\n",
    "        self.P, self.Pdot, self.Pddot = bernstein_coeff_order10_new(10, tot_time_copy[0], tot_time_copy[-1], tot_time_copy)\n",
    "\n",
    "        self.nvar = np.shape(self.P)[1]\n",
    "        self.A_eq_psi = np.vstack((self.P[0], self.Pdot[0], self.P[-1], self.Pdot[-1]))\n",
    "\n",
    "        self.cost_smoothness = self.weight_smoothness * np.dot(self.Pddot.T, self.Pddot)\n",
    "        self.cost_smoothness_v = self.weight_smoothness * np.dot(self.Pddot.T, self.Pddot)\n",
    "        self.cost_smoothness_psi = self.weight_smoothness_psi * np.dot(self.Pddot.T, self.Pddot)\n",
    "        self.lincost_smoothness_psi = np.zeros(self.nvar)\n",
    "\n",
    "        self.P = torch.tensor(self.P, dtype=torch.double).to(device)\n",
    "        self.Pdot = torch.tensor(self.Pdot, dtype=torch.double).to(device)\n",
    "        self.Pddot = torch.tensor(self.Pddot, dtype=torch.double).to(device)\n",
    "        self.A_eq_psi = torch.tensor(self.A_eq_psi, dtype=torch.double).to(device)\n",
    "        self.cost_smoothness = torch.tensor(self.cost_smoothness, dtype=torch.double).to(device)\n",
    "        self.cost_smoothness_v = torch.tensor(self.cost_smoothness_v, dtype=torch.double).to(device)\n",
    "        self.cost_smoothness_psi = torch.tensor(self.cost_smoothness_psi, dtype=torch.double).to(device)\n",
    "        self.lincost_smoothness_psi = torch.tensor(self.lincost_smoothness_psi, dtype=torch.double).to(device)\n",
    "\n",
    "        self.rho_mid = rho_mid\n",
    "        self.mid_idx = torch.tensor([int(self.num/4), int(self.num/2), int(3*self.num/4)]).to(device)\n",
    "        \n",
    "        self.A_w = self.P\n",
    "        self.A_w_psi = self.P\n",
    "        self.A_psi = self.P\n",
    "\n",
    "    def compute_w_psi(self, x_init, y_init, x_fin, y_fin, x_mid, y_mid, psi, v, lamda_wc, lamda_ws):\n",
    "        b_wc_psi = torch.cos(psi).double()\n",
    "        b_ws_psi = torch.sin(psi).double()\n",
    "\n",
    "        temp_x = torch.cumsum(self.P * (v * self.t)[:, np.newaxis], axis=0)\n",
    "        temp_y = torch.cumsum(self.P * (v * self.t)[:, np.newaxis], axis=0)\n",
    "\n",
    "        A_x = temp_x[0:self.num - 1]\n",
    "        A_y = temp_y[0:self.num - 1]\n",
    "\n",
    "        A_x_goal = A_x[-1].reshape(1, self.nvar)\n",
    "        b_x_goal = torch.tensor([x_fin - x_init], dtype=torch.double)\n",
    "\n",
    "        A_y_goal = A_y[-1].reshape(1, self.nvar)\n",
    "        b_y_goal = torch.tensor([y_fin - y_init], dtype=torch.double)\n",
    "\n",
    "        A_x_mid = A_x[self.mid_idx]\n",
    "        A_y_mid = A_y[self.mid_idx]\n",
    "\n",
    "        b_x_mid = x_mid - x_init\n",
    "        b_y_mid = y_mid - y_init\n",
    "\n",
    "        obj_x_goal = self.rho_goal * torch.matmul(A_x_goal.T, A_x_goal)\n",
    "        linterm_augment_x_goal = -self.rho_goal * torch.matmul(A_x_goal.T, b_x_goal)\n",
    "\n",
    "        obj_y_goal = self.rho_goal * torch.matmul(A_y_goal.T, A_y_goal)\n",
    "        linterm_augment_y_goal = -self.rho_goal * torch.matmul(A_y_goal.T, b_y_goal)\n",
    "\n",
    "        obj_x_mid = self.rho_mid * torch.matmul(A_x_mid.T, A_x_mid)\n",
    "        linterm_augment_x_mid = -self.rho_mid * torch.matmul(A_x_mid.T, b_x_mid)\n",
    "\n",
    "        obj_y_mid = self.rho_mid * torch.matmul(A_y_mid.T, A_y_mid)\n",
    "        linterm_augment_y_mid = -self.rho_mid * torch.matmul(A_y_mid.T, b_y_mid)\n",
    "\n",
    "        obj_wc_psi = self.rho_w_psi * torch.matmul(self.A_w_psi.T, self.A_w_psi)\n",
    "        linterm_augment_wc_psi = -self.rho_w_psi * torch.matmul(self.A_w_psi.T, b_wc_psi)\n",
    "\n",
    "        obj_ws_psi = self.rho_w_psi * torch.matmul(self.A_w_psi.T, self.A_w_psi)\n",
    "        linterm_augment_ws_psi = -self.rho_w_psi * torch.matmul(self.A_w_psi.T, b_ws_psi)\n",
    "\n",
    "        cost_wc = obj_wc_psi + obj_x_goal + obj_x_mid\n",
    "        lincost_wc = -lamda_wc + linterm_augment_x_goal + linterm_augment_wc_psi + linterm_augment_x_mid\n",
    "\n",
    "        cost_ws = obj_y_goal + obj_ws_psi + obj_y_mid\n",
    "        lincost_ws = -lamda_ws + linterm_augment_y_goal + linterm_augment_ws_psi + linterm_augment_y_mid\n",
    "\n",
    "        c_wc_psi = torch.linalg.solve(-cost_wc, lincost_wc)\n",
    "        c_ws_psi = torch.linalg.solve(-cost_ws, lincost_ws)\n",
    "\n",
    "        wc = torch.matmul(self.P, c_wc_psi)\n",
    "        ws = torch.matmul(self.P, c_ws_psi)\n",
    "\n",
    "        return wc, ws, c_wc_psi, c_ws_psi\n",
    "\n",
    "    def compute_psi(self, wc, ws, b_eq_psi, lamda_psi):\n",
    "        b_psi = torch.atan2(ws, wc).double()\n",
    "        obj_psi = self.rho_psi * torch.matmul(self.A_psi.T, self.A_psi)\n",
    "        linterm_augment_psi = -self.rho_psi * torch.matmul(self.A_psi.T, b_psi)\n",
    "\n",
    "        cost_psi = self.cost_smoothness_psi + obj_psi + self.rho_eq * torch.matmul(self.A_eq_psi.T, self.A_eq_psi)\n",
    "        lincost_psi = -lamda_psi + linterm_augment_psi - self.rho_eq * torch.matmul(self.A_eq_psi.T, b_eq_psi)\n",
    "        sol = torch.linalg.solve(-cost_psi, lincost_psi)\n",
    "\n",
    "        c_psi = sol[0:self.nvar]\n",
    "        psi = torch.matmul(self.P, c_psi)\n",
    "\n",
    "        res_psi = torch.matmul(self.A_psi, c_psi) - b_psi\n",
    "        res_eq_psi = torch.matmul(self.A_eq_psi, c_psi) - b_eq_psi\n",
    "        lamda_psi = lamda_psi - self.rho_psi * torch.matmul(self.A_psi.T, res_psi) - self.rho_eq * torch.matmul(self.A_eq_psi.T, res_eq_psi)\n",
    "        return psi, c_psi, torch.linalg.norm(res_psi), torch.linalg.norm(res_eq_psi), lamda_psi\n",
    "    \n",
    "    def compute_v(self, v_init, x_init, x_fin, x_mid, y_init, y_fin, y_mid, psi, lamda_v):\n",
    "        temp_x = torch.cumsum(self.P * (torch.cos(psi) * self.t)[:, np.newaxis], axis=0)\n",
    "        temp_y = torch.cumsum(self.P * (torch.sin(psi) * self.t)[:, np.newaxis], axis=0)\n",
    "\n",
    "        A_x = temp_x[0:self.num - 1]\n",
    "        A_y = temp_y[0:self.num - 1]\n",
    "\n",
    "        A_x_goal = A_x[-1].resize(1, self.nvar)\n",
    "        b_x_goal = (x_fin - x_init).unsqueeze(0)\n",
    "\n",
    "        A_y_goal = A_y[-1].resize(1, self.nvar)\n",
    "        b_y_goal = (y_fin - y_init).unsqueeze(0)\n",
    "\n",
    "        A_x_mid = A_x[self.mid_idx]\n",
    "        A_y_mid = A_y[self.mid_idx]\n",
    "\n",
    "        b_x_mid = x_mid - x_init\n",
    "        b_y_mid = y_mid - y_init\n",
    "\n",
    "        A_vel_init = self.P[0].reshape(1, self.nvar)\n",
    "        b_vel_init = torch.tensor([v_init], dtype=torch.double)\n",
    "\n",
    "        obj_x_goal = self.rho_goal * torch.matmul(A_x_goal.T, A_x_goal)\n",
    "        linterm_augment_x_goal = -self.rho_goal * torch.matmul(A_x_goal.T, b_x_goal)\n",
    "\n",
    "        obj_y_goal = self.rho_goal * torch.matmul(A_y_goal.T, A_y_goal)\n",
    "        linterm_augment_y_goal = -self.rho_goal * torch.matmul(A_y_goal.T, b_y_goal)\n",
    "\n",
    "        obj_x_mid = self.rho_mid * torch.matmul(A_x_mid.T, A_x_mid)\n",
    "        linterm_augment_x_mid = -self.rho_mid * torch.matmul(A_x_mid.T, b_x_mid)\n",
    "\n",
    "        obj_y_mid = self.rho_mid * torch.matmul(A_y_mid.T, A_y_mid)\n",
    "        linterm_augment_y_mid = -self.rho_mid * torch.matmul(A_y_mid.T, b_y_mid)\n",
    "\n",
    "        obj_v_init = self.rho_eq * torch.matmul(A_vel_init.T, A_vel_init)\n",
    "        linterm_augment_v_init = -self.rho_eq * torch.matmul(A_vel_init.T, b_vel_init)\n",
    "\n",
    "        cost = obj_x_goal + obj_y_goal + self.cost_smoothness_v + obj_x_mid + obj_y_mid + obj_v_init\n",
    "        lincost = -lamda_v + linterm_augment_x_goal + linterm_augment_y_goal + linterm_augment_x_mid + linterm_augment_y_mid + linterm_augment_v_init\n",
    "\n",
    "        sol = torch.linalg.solve(-cost, lincost)\n",
    "\n",
    "        v = torch.matmul(self.P, sol)\n",
    "\n",
    "        res_v_init = torch.matmul(A_vel_init, sol) - b_vel_init\n",
    "        lamda_v = lamda_v - self.rho_eq * torch.matmul(A_vel_init.T, res_v_init)\n",
    "\n",
    "        return sol, lamda_v, v\n",
    "    \n",
    "    def optimize(self, fixed_params, variable_params):\n",
    "        x_init, y_init, v_init, psi_init, psidot_init = fixed_params\n",
    "        x_fin, y_fin, v_fin, psi_fin, psidot_fin = variable_params[:5]\n",
    "        x_mid = variable_params[5::2]\n",
    "        y_mid = variable_params[6::2]\n",
    "\n",
    "        v = v_init * torch.ones(self.num, dtype=torch.double).to(device)\n",
    "        psi = psi_init * torch.ones(self.num, dtype=torch.double).to(device)\n",
    "\n",
    "        res_psi = torch.ones(self.maxiter, dtype=torch.double).to(device)\n",
    "        res_w_psi = torch.ones(self.maxiter, dtype=torch.double).to(device)\n",
    "        res_w = torch.ones(self.maxiter, dtype=torch.double).to(device)\n",
    "        res_eq_psi = torch.ones(self.maxiter, dtype=torch.double).to(device)\n",
    "        res_eq = torch.ones(self.maxiter, dtype=torch.double).to(device)\n",
    "        b_eq_psi = torch.hstack((psi_init, psidot_init, psi_fin, psidot_fin))\n",
    "\n",
    "        lamda_wc = torch.zeros(self.nvar, dtype=torch.double).to(device)\n",
    "        lamda_ws = torch.zeros(self.nvar, dtype=torch.double).to(device)\n",
    "        lamda_psi = torch.zeros(self.nvar, dtype=torch.double).to(device)\n",
    "        lamda_v = torch.zeros(self.nvar, dtype=torch.double).to(device)\n",
    "        \n",
    "        for i in range(0, self.maxiter):\n",
    "            wc, ws, c_wc_psi, c_ws_psi = self.compute_w_psi(x_init, y_init, x_fin, y_fin, x_mid, y_mid, psi, v, lamda_wc, lamda_ws)\n",
    "            psi, c_psi, res_psi[i], res_eq_psi[i], lamda_psi = self.compute_psi(wc, ws, b_eq_psi, lamda_psi)\n",
    "            c_v, lamda_v, v = self.compute_v(v_init, x_init, x_fin, x_mid, y_init, y_fin, y_mid, psi, lamda_v)\n",
    "\n",
    "            res_wc = wc - torch.cos(psi)\n",
    "            res_ws = ws - torch.sin(psi)\n",
    "            \n",
    "            lamda_wc = lamda_wc - self.rho_w_psi * torch.matmul(self.A_w.T, res_wc)\n",
    "            lamda_ws = lamda_ws - self.rho_w_psi * torch.matmul(self.A_w.T, res_ws)\n",
    "\n",
    "            res_w[i] = torch.linalg.norm(torch.hstack((res_wc, res_ws)))\n",
    "            \n",
    "        primal_sol = torch.hstack((c_psi, c_v))\n",
    "        return primal_sol\n",
    "    \n",
    "    def solve(self, fixed_params, variable_params):\n",
    "        batch_size, _ = fixed_params.size()\n",
    "        y = torch.zeros(batch_size, 2 * self.nvar, dtype=torch.double).to(device)\n",
    "        mid_size = self.mid_idx.size()[0]\n",
    "        for i in range(batch_size):\n",
    "            fixed_params_cur = fixed_params[i]\n",
    "            variable_params_cur = variable_params[i]\n",
    "            primal_sol = self.optimize(fixed_params_cur, variable_params_cur)\n",
    "            y[i, :] = primal_sol\n",
    "        return y, None\n",
    "    \n",
    "    def objective(self, fixed_params, variable_params, y):\n",
    "        x_init, y_init, v_init, psi_init, psidot_init = torch.chunk(fixed_params, 5, dim=1)\n",
    "        x_fin, y_fin, v_fin, psi_fin, psidot_fin = torch.chunk(variable_params[:, :5], 5, dim=1)\n",
    "        x_mid = variable_params[:, 5::2]\n",
    "        y_mid = variable_params[:, 6::2]\n",
    "        \n",
    "        c_psi = y[:, :self.nvar]\n",
    "        c_v = y[:, self.nvar:]\n",
    "        \n",
    "        v_new = torch.matmul(self.P, c_v.T)\n",
    "        psi_new = torch.matmul(self.P, c_psi.T)\n",
    "        psidot_new = torch.matmul(self.Pdot, c_psi.T)\n",
    "        \n",
    "        x_temp = x_init + torch.cumsum(v_new * torch.cos(psi_new) * self.t, dim=0).T\n",
    "        y_temp = y_init + torch.cumsum(v_new * torch.sin(psi_new) * self.t, dim=0).T\n",
    "        \n",
    "        x = torch.hstack((x_init, x_temp[:, :-1]))\n",
    "        y = torch.hstack((y_init, y_temp[:, :-1]))\n",
    "        \n",
    "        cost_final_pos = 0.5 * self.rho_goal * ((x[:, -1:] - x_fin) ** 2 + (y[:, -1:] - y_fin) ** 2)\n",
    "        cost_psi_term = 0.5 * self.rho_eq * ((psi_new[:1, :].T - psi_init) ** 2 + (psidot_new[:1, :].T - psidot_init) ** 2 + (psi_new[-1:, :].T - psi_fin) ** 2 + (psidot_new[-1:, :].T - psidot_fin) ** 2)\n",
    "        cost_v_term = 0.5 * self.rho_eq * (v_new[:1, :].T - v_init) ** 2\n",
    "        cost_mid_term = 0.5 * self.rho_mid * (torch.sum((x[:, self.mid_idx] - x_mid) ** 2, dim=1, keepdim=True) + torch.sum((y[:, self.mid_idx] - y_mid) ** 2, dim=1, keepdim=True))\n",
    "        \n",
    "        cost = cost_final_pos + cost_psi_term + cost_v_term + cost_mid_term\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Declarative Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptFunction(torch.autograd.Function):\n",
    "    \"\"\"Generic declarative autograd function.\n",
    "    Defines the forward and backward functions. Saves all inputs and outputs,\n",
    "    which may be memory-inefficient for the specific problem.\n",
    "    \n",
    "    Assumptions:\n",
    "    * All inputs are PyTorch tensors\n",
    "    * All inputs have a single batch dimension (b, ...)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, problem, *inputs):\n",
    "        output, solve_ctx = torch.no_grad()(problem.solve)(*inputs)\n",
    "        ctx.save_for_backward(output, *inputs)\n",
    "        ctx.problem = problem\n",
    "        ctx.solve_ctx = solve_ctx\n",
    "        return output.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output, *inputs = ctx.saved_tensors\n",
    "        problem = ctx.problem\n",
    "        solve_ctx = ctx.solve_ctx\n",
    "        output.requires_grad = True\n",
    "        inputs = tuple(inputs)\n",
    "        grad_inputs = problem.gradient(*inputs, y=output, v=grad_output,\n",
    "            ctx=solve_ctx)\n",
    "        return (None, *grad_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Declarative Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptLayer(torch.nn.Module):\n",
    "    \"\"\"Generic declarative layer.\n",
    "    \n",
    "    Assumptions:\n",
    "    * All inputs are PyTorch tensors\n",
    "    * All inputs have a single batch dimension (b, ...)\n",
    "    Usage:\n",
    "        problem = <derived class of *DeclarativeNode>\n",
    "        declarative_layer = DeclarativeLayer(problem)\n",
    "        y = declarative_layer(x1, x2, ...)\n",
    "    \"\"\"\n",
    "    def __init__(self, problem):\n",
    "        super(OptLayer, self).__init__()\n",
    "        self.problem = problem\n",
    "        \n",
    "    def forward(self, *inputs):\n",
    "        return OptFunction.apply(self.problem, *inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TrajNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajNet(nn.Module):\n",
    "    def __init__(self, opt_layer, P, input_size=16, hidden_size=64, output_size=12, nvar=11, t_obs=8):\n",
    "        super(TrajNet, self).__init__()\n",
    "        self.P = torch.tensor(P, dtype=torch.double).to(device)\n",
    "        self.nvar = nvar\n",
    "        self.t_obs = t_obs\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        self.opt_layer = opt_layer\n",
    "        self.activation = nn.ReLU()\n",
    "        self.mask = torch.tensor([[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]], dtype=torch.double)\n",
    "    \n",
    "    def forward(self, x, b):\n",
    "        batch_size, _ = x.size()\n",
    "        out = self.activation(self.linear1(x))\n",
    "        b_pred = self.linear2(out)\n",
    "        b_gen = self.mask * b + (1 - self.mask) * b_pred\n",
    "        \n",
    "        # Run optimization\n",
    "        lamda_x = torch.zeros(batch_size, self.nvar, dtype=torch.double).to(device)\n",
    "        lamda_y = torch.zeros(batch_size, self.nvar, dtype=torch.double).to(device)\n",
    "        sol = self.opt_layer(b_gen, lamda_x, lamda_y)\n",
    "\n",
    "        # Compute final trajectory\n",
    "        x_pred = torch.matmul(self.P, sol[:, :self.nvar].transpose(0, 1))\n",
    "        y_pred = torch.matmul(self.P, sol[:, self.nvar:2*self.nvar].transpose(0, 1))\n",
    "        \n",
    "        x_pred = x_pred.transpose(0, 1)\n",
    "        y_pred = y_pred.transpose(0, 1)\n",
    "        out = torch.cat([x_pred, y_pred], dim=1)\n",
    "        #return out, sol, b_gen, lamda_x, lamda_y\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_node = OPTNode(\n",
    "    P, \n",
    "    Pddot, \n",
    "    A_eq,\n",
    "    A_obs, \n",
    "    Q_smoothness, \n",
    "    x_obs,\n",
    "    y_obs,\n",
    "    num=num, \n",
    "#     num_obs=num_obs, \n",
    "#     nvar=nvar, \n",
    "#     a_obs=a_obs, \n",
    "#     b_obs=b_obs, \n",
    "#     rho_obs=rho_obs, \n",
    "#     rho_eq=rho_eq,\n",
    "    weight_smoothness=weight_smoothness, \n",
    "#     maxiter=300, \n",
    "#     eps=1e-7, \n",
    "#     num_tot=num*num_obs\n",
    ")\n",
    "opt_layer = OptLayer(opt_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrajNet(opt_layer, P, 36)\n",
    "model = model.double()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inp = torch.randn(10, 36, dtype=torch.double)\n",
    "y_out = torch.randn(10, 20, dtype=torch.double)\n",
    "b_inp = torch.randn(10, 12, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 36])\n"
     ]
    }
   ],
   "source": [
    "print(x_inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "solve() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8e50fd8d6863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-fc4370bb7e5f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, b)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlamda_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlamda_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamda_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamda_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Compute final trajectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-28e49ce9af72>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mOptFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-81e65742b1e3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, problem, *inputs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolve_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: solve() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "out = model(x_inp, b_inp)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, _ = x_inp.size()\n",
    "lamda_x = torch.zeros(batch_size, nvar, dtype=torch.double).to(device)\n",
    "lamda_y = torch.zeros(batch_size, nvar, dtype=torch.double).to(device)\n",
    "opt_node.gradient(b_inp, lamda_x, lamda_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trajectory DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class PedDataLoader(Dataset):    \n",
    "    def __init__(self, file_path, threshold=20):        \n",
    "        \"\"\"        \n",
    "            Takes the filepath of input zara dataset, and trajectory length threshold.        \n",
    "        \"\"\"        \n",
    "        self.df = pd.read_csv(file_path, sep='\\t', header=None, names=[\"Frame\", \"Agent\", \"x-pos\", \"y-pos\"]) \n",
    "        print(self.df)\n",
    "        self.data = []    \n",
    "        self.dt = 0.2\n",
    "        cnt = 0\n",
    "        for i in range(int(self.df[\"Agent\"].max())):              \n",
    "            self.data.append([])\n",
    "            cnt = cnt + 1\n",
    "            for row in self.df.itertuples():\n",
    "                if int(row[2] - 1) == i and isinstance(self.data[int(row[2]) - 1],list) == False:\n",
    "                    self.data[int(row[2] - 1)].append([])\n",
    "                if int(row[2] - 1) == i and 1 and len(self.data[int(row[2] - 1)]) <= threshold:\n",
    "                    self.data[int(row[2] - 1)].append([row[3], row[4]])\n",
    "        self.data = list(filter(lambda x: len(x)==threshold, self.data))    \n",
    "        print(np.array(self.data).shape)\n",
    "#         print(np.array(self.data))\n",
    "    def __len__(self):        \n",
    "        return len(self.data)                \n",
    "    \n",
    "    def __getitem__(self, ind):       \n",
    "        traj = np.array(self.data[ind])\n",
    "        print(traj.shape)\n",
    "        x_traj = traj[:, 0]\n",
    "        y_traj = traj[:, 1]        \n",
    "#         print(x_traj, y_traj)       \n",
    "        x_inp = x_traj[:10]        \n",
    "        x_fut = x_traj[10:]        \n",
    "        y_inp = y_traj[:10]        \n",
    "        y_fut = y_traj[10:]               \n",
    "        vx_beg = x_inp[-1] - x_inp[-2]        \n",
    "        vy_beg = y_inp[-1] - y_inp[-2]        \n",
    "        vel_acc = np.array([vx_beg, vy_beg, 0, 0])        \n",
    "        traj_inp = np.dstack((x_inp, y_inp)).flatten()        \n",
    "        traj_inp = np.hstack((traj_inp, vel_acc, obs_inp))        \n",
    "        traj_out = np.hstack((x_fut, y_fut)).flatten()        \n",
    "        b_inp = np.array([x_fut[0], vx_beg, 0, x_fut[-1], 0, 0, y_fut[0], vy_beg, 0, y_fut[-1], 0, 0])    \n",
    "        return torch.tensor(traj_inp), torch.tensor(traj_out), torch.tensor(b_inp)\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, root_dir, t_obs=10, dt=0.4):\n",
    "        self.root_dir = root_dir\n",
    "        self.t_obs = t_obs\n",
    "        self.dt = dt\n",
    "        self.data = []\n",
    "        data = np.load(self.root_dir + \"/test.npy\")\n",
    "        print(data.shape)\n",
    "        self.data = data    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_vel(self, pos):\n",
    "        return (pos[-1] - pos[-2]) / self.dt\n",
    "    \n",
    "    def get_acc(self, vel):\n",
    "        return (vel[-1] - vel[-2]) / self.dt\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         file_name = \"tests/test.npy\"\n",
    "#         file_path = os.path.join(self.root_dir, file_name)\n",
    "        \n",
    "        data = np.load(self.root_dir + \"/test.npy\")\n",
    "        self.data = data\n",
    "#         print(data[idx].shape)\n",
    "#         print(data[idx][0])\n",
    "        x_traj = data[idx][:, 0]\n",
    "        y_traj = data[idx][:, 1]\n",
    "        \n",
    "        x_inp = x_traj[:self.t_obs]\n",
    "        y_inp = y_traj[:self.t_obs]\n",
    "        x_fut = x_traj[self.t_obs:]\n",
    "        y_fut = y_traj[self.t_obs:]\n",
    "        \n",
    "        vx_beg = x_inp[-1] - x_inp[-2]        \n",
    "        vy_beg = y_inp[-1] - y_inp[-2]        \n",
    "        vel_acc = np.array([vx_beg, vy_beg, 0, 0])        \n",
    "        traj_inp = np.dstack((x_inp, y_inp)).flatten()        \n",
    "        traj_inp = np.hstack((traj_inp, vel_acc, obs_inp))        \n",
    "        traj_out = np.hstack((x_fut, y_fut)).flatten()\n",
    "#         print(x_fut, y_fut)\n",
    "        b_inp = np.array([x_fut[0], vx_beg, 0, x_fut[-1], 0, 0, y_fut[0], vy_beg, 0, y_fut[-1], 0, 0])    \n",
    "        return torch.tensor(traj_inp), torch.tensor(traj_out), torch.tensor(b_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = PedDataLoader(\"./datasets/zara1/train/students003_train.txt\")\n",
    "# train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20, 2)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TrajectoryDataset(\"./tests\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = TrajectoryDataset(\"./test2/data/\", t_obs=25, dt=0.08)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=20, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 36]) torch.Size([20, 20]) torch.Size([20, 12])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    traj_inp, traj_out, b_inp = data\n",
    "    print(traj_inp.size(), traj_out.size(), b_inp.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(traj_inp, b_inp)\n",
    "# out.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 36]), torch.Size([20, 20]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_inp.size(), traj_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 3.0481331363347306\n",
      "Epoch: 0, Mean Loss: 1.7811489985744289\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 1, Batch: 0, Loss: 1.217869182545357\n",
      "Epoch: 1, Mean Loss: 1.0416578574414712\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 2, Batch: 0, Loss: 0.7797071260774974\n",
      "Epoch: 2, Mean Loss: 0.9119967196240559\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 3, Batch: 0, Loss: 0.9364042018108267\n",
      "Epoch: 3, Mean Loss: 0.8978284588835728\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 4, Batch: 0, Loss: 0.9738340806820664\n",
      "Epoch: 4, Mean Loss: 0.8886460951635582\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 5, Batch: 0, Loss: 0.9683644807741638\n",
      "Epoch: 5, Mean Loss: 0.8851426532122332\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 6, Batch: 0, Loss: 0.9694596880267311\n",
      "Epoch: 6, Mean Loss: 0.8740660292589204\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 7, Batch: 0, Loss: 0.8833974821326904\n",
      "Epoch: 7, Mean Loss: 0.875389142232757\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 8, Batch: 0, Loss: 0.7370304618259498\n",
      "Epoch: 8, Mean Loss: 0.8700164392413499\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 9, Batch: 0, Loss: 0.9320811620897664\n",
      "Epoch: 9, Mean Loss: 0.8682743385782612\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 10, Batch: 0, Loss: 0.8775107424377521\n",
      "Epoch: 10, Mean Loss: 0.8703974512969189\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 11, Batch: 0, Loss: 0.885068912105951\n",
      "Epoch: 11, Mean Loss: 0.8645061887883598\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 12, Batch: 0, Loss: 0.7055444605586567\n",
      "Epoch: 12, Mean Loss: 0.8645672021937478\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 13, Batch: 0, Loss: 0.728825985795386\n",
      "Epoch: 13, Mean Loss: 0.8532374516070547\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 14, Batch: 0, Loss: 0.7914425194827653\n",
      "Epoch: 14, Mean Loss: 0.8549532229812293\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 15, Batch: 0, Loss: 0.7228205117109794\n",
      "Epoch: 15, Mean Loss: 0.8457926614865491\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 16, Batch: 0, Loss: 0.8246084850417755\n",
      "Epoch: 16, Mean Loss: 0.8558020099326701\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 17, Batch: 0, Loss: 0.6540313501160914\n",
      "Epoch: 17, Mean Loss: 0.8470228403936663\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 18, Batch: 0, Loss: 0.8933804967752124\n",
      "Epoch: 18, Mean Loss: 0.8457191089559833\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 19, Batch: 0, Loss: 0.6882981720078493\n",
      "Epoch: 19, Mean Loss: 0.8445736559183071\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 20, Batch: 0, Loss: 0.837690174402544\n",
      "Epoch: 20, Mean Loss: 0.8346862416835179\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 21, Batch: 0, Loss: 0.8301306127859988\n",
      "Epoch: 21, Mean Loss: 0.8377039042868153\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 22, Batch: 0, Loss: 0.9003770249012945\n",
      "Epoch: 22, Mean Loss: 0.8347382341344469\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 23, Batch: 0, Loss: 0.7055819140486753\n",
      "Epoch: 23, Mean Loss: 0.8358621045669758\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch: 24, Batch: 0, Loss: 0.8776295806095266\n",
      "Epoch: 24, Mean Loss: 0.8318679230480184\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "epoch_train_loss = []\n",
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for batch_num, data in enumerate(train_loader):\n",
    "        traj_inp, traj_out, b_inp = data\n",
    "        traj_inp = traj_inp.to(device)\n",
    "        traj_out = traj_out.to(device)\n",
    "        b_inp = b_inp.to(device)\n",
    "\n",
    "        out = model(traj_inp, b_inp)\n",
    "        loss = criterion(out, traj_out)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        cnt = 0\n",
    "        obs = np.load(\"train/obstacle.npy\")\n",
    "        for i in range(traj_inp.size()[0]):\n",
    "            plot_traj(cnt, traj_inp[i], traj_out[i], out[i], {\"x\": x_obs_temp, \"y\": y_obs_temp})\n",
    "            cnt += 1\n",
    "        \n",
    "        if batch_num % 10 == 0:\n",
    "            print(\"Epoch: {}, Batch: {}, Loss: {}\".format(epoch, batch_num, loss.item()))\n",
    "    \n",
    "    mean_loss = np.mean(train_loss)\n",
    "    epoch_train_loss.append(mean_loss)\n",
    "    print(\"Epoch: {}, Mean Loss: {}\".format(epoch, mean_loss))\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traj(1, traj_inp, traj_out, traj_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    cnt = 0\n",
    "    test_loss = []\n",
    "    for batch_num, data in enumerate(test_loader):\n",
    "        traj_inp, traj_out, b_inp = data\n",
    "        traj_inp = traj_inp.to(device)\n",
    "        traj_out = traj_out.to(device)\n",
    "        b_inp = b_inp.to(device)\n",
    "\n",
    "        out = model(traj_inp, b_inp)\n",
    "        loss = criterion(out, traj_out)\n",
    "        \n",
    "        test_loss.append(loss.item())\n",
    "        obs = np.load(\"train/obstacle.npy\")\n",
    "        print(\"Batch: {}, Loss: {}\".format(batch_num, loss.item()))\n",
    "        for i in range(traj_inp.size()[0]):\n",
    "            plot_traj(cnt, traj_inp[i], traj_out[i], out[i], {\"x\": obs[i][0], \"y\": obs[i][1]})\n",
    "            cnt += 1\n",
    "mean_loss = np.mean(test_loss)\n",
    "print(\"Epoch Mean Test Loss: {}\".format(mean_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_inp.numpy()[0][:2*num].shape, traj_out.numpy()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_inp[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_out[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sqrt(torch.sum((traj_out[6] - out[6]) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = torch.tensor([traj_out[6][1], traj_out[6][23]])\n",
    "fin_pred = torch.tensor([out[6][11], out[6][23]])\n",
    "fin, fin_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sqrt(torch.sum((fin_pred - fin) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sqrt(torch.sum((traj_out[6][-1] - out[6][-1]) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(traj_out[6], out[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_traj(cnt, traj_inp, traj_out, traj_pred):\n",
    "    traj_inp = traj_inp.numpy()\n",
    "    traj_out = traj_out.numpy()\n",
    "    traj_pred = traj_pred.numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax.scatter(traj_inp[::2], traj_inp[1::2], label='Inp traj')\n",
    "    ax.scatter(traj_out[:12], traj_out[12:], label='GT')\n",
    "    ax.scatter(traj_out[11], traj_out[23], label='Fin GT')\n",
    "    ax.scatter(traj_pred[:12], traj_pred[12:], label='Pred')\n",
    "    ax.scatter(traj_pred[11], traj_pred[23], label='Fin Pred')\n",
    "    \n",
    "    th = np.linspace(0, 2 * np.pi, 100)\n",
    "    for i in range(0, num_obs):\n",
    "        x_circ = x_obs_temp[i] + a_obs * np.cos(th)\n",
    "        y_circ = y_obs_temp[i] + b_obs * np.sin(th)\n",
    "        ax.plot(x_circ, y_circ, '-k')\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set_xlim([-7, 7])\n",
    "    ax.set_ylim([-7, 7])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_traj(0, traj_inp[8], traj_out[8], out[8].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "class OPTNodee(AbstractDeclarativeNode):\n",
    "    def __init__(self, P, Pddot, A_eq, A_obs, Q_smoothness, x_obs, y_obs, num=12, num_obs=4, nvar=11, a_obs=1.0, b_obs=1.0, rho_obs=0.3, rho_eq=10.0, weight_smoothness=10, maxiter=300, eps=1e-7, num_tot=48):\n",
    "        super().__init__()\n",
    "        self.P = torch.tensor(P, dtype=torch.double).to(device)\n",
    "        self.Pddot = torch.tensor(Pddot, dtype=torch.double).to(device)\n",
    "        self.A_eq = torch.tensor(A_eq, dtype=torch.double).to(device)\n",
    "        self.A_obs = torch.tensor(A_obs, dtype=torch.double).to(device)\n",
    "        self.Q_smoothness = torch.tensor(Q_smoothness, dtype=torch.double).to(device)\n",
    "        self.x_obs = torch.tensor(x_obs, dtype=torch.double).to(device)\n",
    "        self.y_obs = torch.tensor(y_obs, dtype=torch.double).to(device)\n",
    "        \n",
    "        self.num = num\n",
    "        self.num_obs = num_obs\n",
    "        self.eps = eps\n",
    "        self.nvar = nvar        \n",
    "        self.a_obs = a_obs\n",
    "        self.b_obs = b_obs        \n",
    "        self.rho_eq = rho_eq\n",
    "        self.num_obs = num_obs\n",
    "        self.maxiter = maxiter\n",
    "        self.num_tot = num_tot\n",
    "        self.rho_obs = rho_obs\n",
    "        self.weight_smoothness = weight_smoothness\n",
    "        \n",
    "    def objective(self, b, lamda_x, lamda_y, y):  \n",
    "        batch_size, _ = b.size()\n",
    "        b = b.transpose(0, 1)\n",
    "        y = y.transpose(0, 1)\n",
    "        lamda_x = lamda_x.transpose(0, 1)\n",
    "        lamda_y = lamda_y.transpose(0, 1)\n",
    "        bx_eq_tensor, by_eq_tensor = torch.split(b, 6, dim=0)\n",
    "        ones_tensor = torch.ones(self.num_tot, batch_size, dtype=torch.double).to(device)\n",
    "\n",
    "        c_x = y[0:self.nvar]\n",
    "        c_y = y[self.nvar:2 * self.nvar]\n",
    "        alpha_obs = y[2 * self.nvar: 2 * self.nvar + self.num_tot]\n",
    "        d_obs = y[2 * self.nvar + self.num_tot:]\n",
    "\n",
    "        cost_smoothness_x = 0.5 * self.weight_smoothness * torch.diag(torch.matmul(c_x.T, torch.matmul(self.Q_smoothness, c_x)))\n",
    "        cost_smoothness_y = 0.5 * self.weight_smoothness * torch.diag(torch.matmul(c_y.T, torch.matmul(self.Q_smoothness, c_y)))\n",
    "\n",
    "        temp_x_obs = d_obs * torch.cos(alpha_obs) * self.a_obs\n",
    "        b_obs_x = self.x_obs.view(-1, 1) + temp_x_obs\n",
    "\n",
    "        temp_y_obs = d_obs * torch.sin(alpha_obs) * self.b_obs\n",
    "        b_obs_y = self.y_obs.view(-1, 1) + temp_y_obs\n",
    "\n",
    "        cost_obs_x = 0.5 * self.rho_obs * (torch.sum((torch.matmul(self.A_obs, c_x) - b_obs_x) ** 2, axis=0))\n",
    "        cost_obs_y = 0.5 * self.rho_obs * (torch.sum((torch.matmul(self.A_obs, c_y) - b_obs_y) ** 2, axis=0))\n",
    "        cost_slack = self.rho_obs * torch.sum(torch.max(1 - d_obs, ones_tensor), axis=0)\n",
    "\n",
    "        cost_eq_x = 0.5 * self.rho_eq * torch.sum((torch.matmul(self.A_eq, c_x) - bx_eq_tensor) ** 2, axis=0)\n",
    "        cost_eq_y = 0.5 * self.rho_eq * torch.sum((torch.matmul(self. A_eq, c_y) - by_eq_tensor) ** 2, axis=0)\n",
    "\n",
    "        cost_x = cost_smoothness_x + cost_obs_x + cost_eq_x\n",
    "        cost_y = cost_smoothness_y + cost_obs_y + cost_eq_y\n",
    "        cost = cost_x + cost_y + self.eps * torch.sum(c_x ** 2, axis=0) + self.eps * torch.sum(c_y ** 2, axis=0) + self.eps * torch.sum(d_obs ** 2, axis=0) + self.eps * torch.sum(alpha_obs ** 2, axis=0) + cost_slack\n",
    "        return cost\n",
    "\n",
    "    def optimize(self, b, lamda_x, lamda_y):\n",
    "        bx_eq_tensor, by_eq_tensor = torch.split(b, 6, dim=0)\n",
    "        \n",
    "        d_obs = torch.ones(self.num_obs, self.num, dtype=torch.double).to(device)\n",
    "        alpha_obs = torch.zeros(self.num_obs, self.num, dtype=torch.double).to(device)\n",
    "        ones_tensor = torch.ones((self.num_obs, self.num), dtype=torch.double).to(device)\n",
    "        cost_smoothness = self.weight_smoothness * torch.matmul(self.Pddot.T, self.Pddot)\n",
    "        cost = cost_smoothness + self.rho_obs * torch.matmul(self.A_obs.T, self.A_obs) + self.rho_eq * torch.matmul(self.A_eq.T, self.A_eq)\n",
    "\n",
    "        temp_x_obs = d_obs * torch.cos(alpha_obs) * self.a_obs\n",
    "        temp_y_obs = d_obs * torch.sin(alpha_obs) * self.b_obs\n",
    "\n",
    "        b_obs_x = self.x_obs.view(self.num * self.num_obs) + temp_x_obs.view(self.num * self.num_obs)\n",
    "        b_obs_y = self.y_obs.view(self.num * self.num_obs) + temp_y_obs.view(self.num * self.num_obs)        \n",
    "        \n",
    "        bx_eq_tensor, by_eq_tensor = torch.split(b, 6, dim=0)\n",
    "        \n",
    "#         + torch.matmul(self.A_obs.T, b_obs_x) + self.rho_eq * torch.matmul(self.A_eq.T, bx_eq_tensor)\n",
    "#          self.rho_obs * torch.matmul(self.A_obs.T, b_obs_y) - self.rho_eq * torch.matmul(self.A_eq.T, by_eq_tensor)        \n",
    "        \n",
    "#       A_eq = np.vstack((P[0], Pdot[0], Pddot[0], P[-1], Pdot[-1], Pddot[-1]))\n",
    "#       A_eq = np.vstack((A_eq, A_eq))\n",
    "#        A_obs = np.tile(P, (num_obs, 1))\n",
    "        b_eq = np.hstack((bx_eq_tensor, by_eq_tensor))\n",
    "        print(A_obs.shape)\n",
    "#         A_eq = np.vstack((P[0], Pdot[0], Pddot[0], P[-1], Pdot[-1], Pddot[-1]))\n",
    "        Q_obs = torch.matmul(self.A_obs.T, self.A_obs)\n",
    "        Q_eq = torch.matmul(self.A_eq.T, self.A_eq)\n",
    "        bx_obs = x_obs.reshape(num_obs * num)\n",
    "        by_obs = y_obs.reshape(num_obs * num)\n",
    "        qx_obs = torch.matmul(b_obs_x.T, self.A_obs) * 2\n",
    "        qy_obs = torch.matmul(b_obs_y.T, self.A_obs) * 2\n",
    "        qx_eq = torch.matmul(bx_eq_tensor.T, self.A_eq) * 2\n",
    "        qy_eq = torch.matmul(by_eq_tensor.T, self.A_eq) * 2\n",
    "        q = np.hstack((qx_eq, qy_eq))\n",
    "        print(Q_obs.shape)\n",
    "        AA_eq = cvxopt.matrix(self.A_eq, tc='d')\n",
    "        bx_eq = cvxopt.matrix(bx_eq_tensor, tc='d')\n",
    "        by_eq = cvxopt.matrix(by_eq_tensor, tc='d')\n",
    "        b_eq = cvxopt.matrix(b_eq, tc='d')\n",
    "        Q = cvxopt.matrix(cost, tc='d')\n",
    "        q_x = cvxopt.matrix(rho_obs * qx_obs + rho_eq * qx_eq)\n",
    "        q_y = cvxopt.matrix(rho_obs * qy_obs + rho_eq * qy_eq)\n",
    "        sol_x = solvers.qp(Q, q_x, None, None, AA_eq, bx_eq)\n",
    "        sol_x = torch.tensor(sol_x['x'])\n",
    "        sol_y = solvers.qp(Q, q_y, None, None, AA_eq, by_eq)\n",
    "        sol_y = torch.tensor(sol_y['x'])\n",
    "        sol = torch.cat([sol_x, sol_y, alpha_obs.view(-1), d_obs.view(-1)])\n",
    "        return sol\n",
    "\n",
    "    def solve(self, b, lamda_x, lamda_y):\n",
    "        batch_size, _ = b.size()\n",
    "        b = b.transpose(0, 1)\n",
    "        lamda_x = lamda_x.transpose(0, 1)\n",
    "        lamda_y = lamda_y.transpose(0, 1)\n",
    "        y = torch.zeros(batch_size, 2 * self.nvar + 2 * self.num_tot, dtype=torch.double).to(device)\n",
    "        for i in range(batch_size):\n",
    "            b_cur = b[:, i]\n",
    "            lamda_x_cur = lamda_x[:, i]\n",
    "            lamda_y_cur = lamda_y[:, i]\n",
    "            sol = self.optimize(b_cur, lamda_x_cur, lamda_y_cur)\n",
    "            y[i, :] = sol\n",
    "        return y, None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
